---
title: "Attention-Adjusted Discounted Utility"
author: "Zijian Zark Wang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: reference.bib
biblio-style: apalike
header-includes: 
  \usepackage{setspace}
  \usepackage{amsmath}
  \usepackage{array}
  \usepackage{caption}
  \usepackage{longtable}
  \usepackage{booktabs}
  \renewcommand{\arraystretch}{1}
  \captionsetup[table]{skip=5pt}
  \setstretch{1.5} 
fontsize: 12pt
geometry: margin=1in
editor_options: 
  markdown: 
    wrap: 72
output:
  #word_document:
    #number_sections: true
  pdf_document:
    number_sections: true
    citation_package: natbib
    keep_tex: true
  html_document:
    toc: true
    number_sections: true
---

# Introduction

The discounted utility framework has been widely employed to model
intertemporal choices. According to this framework, decision makers
evaluate a sequence of rewards by assigning a weight to each time period
and summing up the weighted utilities of the rewards across these
periods. Typically, the weights are referred to as the discounting
factors and are assumed to decline over time, indicating a preference
for receiving a sooner reward over a later reward. However, recent
research suggests that, due to limited information processing capacity,
prior to making a decision, people tend to allocate more attention to
the information most relevant to the decision. We propose that the
determination of weights in discounted utility framework is also
influenced by such an attentional mechanism. For instance, when an
investor consider whether to invest \$100 now and get \$110 in 100 days,
she may focus more on the amount of money she can obtain on the 100th
day (i.e. £110). Consequently, when calculating the utility of
"receiving £110 in 100 days", the investor may want to assign a higher
weight to the 100th day. And doing that will reduce weight assigned to
the days prior to the 100th day. Based on this perspective, we develop a
novel model of intertemporal choice, which we term "*attention-adjusted
discounted utility*". We demonstrate how this model can better
accommodate certain empirical findings. To distinguish it from the
discounting factor, we refer to the weights in our model as "attention
weights."

Let $\textbf{x} = [x_0, x_1, …, x_T]$ denote a sequence of rewards and
$u(.)$ denote the utility function. The overall utility of this sequence
is calculated by $\sum_{t=0}^T w_t u(x_t)$, where $w_t$ is the attention
weight assigned to period $t$. In our model, the attention weight is
calculated by

$$
w_t = \frac{d_t e^{u(x_t)}}{\sum_{\tau=0}^T d_\tau e^{u(x_t)}}
$$

where $d_t$ is the initial weight (discounting factor) allocated to
period $t$. The weight for each period, i.e. $w_t$, is increasing with
$u(x_t)$, indicating that the decision maker is motivated to shift
attention to the periods with larger rewards. $w_t$ is "anchored" in the
initial weight $d_t$, because the attention adjustment process is
costly. The sum of weights is fixed at 1, indicating the decision
maker's capacity of focusing is limited.

We draw inspirations on three fields of research. The first is motivated
beliefs. The theory of motivated beliefs states that people will adjust
beliefs to subjectively maximize their utility. When the choice made by
a decision maker does not deliver the maximum utility, she may tend to
adjust the belief to safeguard the choice, rather than shift to another
option. This is usually applied to cognitive dissonance and
overconfidence. The similar mechanism may exist in the allocation of
weights to each time period.

The second is rational inattention. The model of rational inattention
states that, prior to making choices between options, the decision maker
has to learn the information about each option. Under the assumption
that she wants to maximum the expected utility minus the cost of
learning, which is linear to the information gains, we can derive that
the probability of each option being chosen follows a logistic-like
distribution. In our model, we assume that when allocating weights
across time periods, the decision maker has a similar objective
function, thus the attention weights also follows a logistic-like
distribution.

The third is the attention mechanisms which have been widely applied in
deep learning. In such models, there is often an input sequence and a
query vector. Attention weights are assigned to each period of the
sequence to determine their relevance to the current context. The most
common approach to obtain attention weights involves computing
similarity scores between the query vector and each period in the input
sequence, normalizing these scores using a softmax (i.e. multinominal
logistic) function.

In the attention-adjusted discounted utility model (hereafter referred
to as "ADU"), the underlying cognitive process is efficient sampling. We
assume that the decision maker initially has no information about which
period in a reward sequence has a larger reward. She implements a costly
sampling strategy to draw some rewards from the sequence to learn the
information, then choose the attention weight to each period
accordingly. Therefore, when evaluating the given reward sequence, she
tends to assign more weights (pay more attention) to the time periods
with larger rewards, in order to subjectively maximize her overall
utility. This attention adjustment process incurs a cognitive cost; and
the more the weight allocation deviates from the initial allocation, the
greater the cost is. The decision maker optimally re-allocates the
weights across time periods.

Our contribution is three-fold. First, the model can accommodate a lot
of empirical evidence with just one to two parameters.

In this paper, I show that a set of intertemporal choice anomalies can
be attributed to such attention adjustment processes (that is, can be
explained by ADU to some extent), including common difference effect and
magnitude effect [@loewenstein_anomalies_1992], risk aversion over time
lotteries [@onay_intertemporal_2007; @dejarnette_time_2020],
non-additive time intervals [@read_is_2001; @scholten_discounting_2006],
intertemporal correlation aversion [@andersen_multiattribute_2018], and
dynamic inconsistency. The model can also offer insights on the
preferences for sequences of outcomes [@loewenstein_preferences_1993]
and the formation of reference-dependent preferences
[@koszegi_model_2006]. In an empirical test, I find ADU outperforms a
set of time discounting models in predicting human intertemporal
choices. Therefore, I think there is a need to rethink the foundation of
many behavioral phenomena.

Second, we link the literature of attention to time preferences.

Third, we make some novel predictions based on the model.

The remaining part of this document is organized as follows. Section
\ref{model} outlines the model of attention-adjusted discounted utility
(ADU). Section \ref{behavioral} explains how the model can help explain
some empirical findings in intertemporal choice.

# The Model \label{model}

## Rational Inattention and Time Discounting

Consider a reward sequence $x = [x_0,x_1,...,x_T]$ that yields reward
$x_t$ in time period $t$. The time length of this sequence, denoted by
$T$, is finite. For any $t \in \{0,1,...,T\}$, the reward level $x_t$ is
a random variable defined on $R_{+}$. The support of $x$ is $X$, which
is a subset of $R_{+}^T$.

Suppose a decision maker evaluates reward sequence $x$ by three steps:
At first, she randomly draws some potential realizations of $x$ from
$X$. Then, from each drawn realization of $x$, she draws some time
periods at random, taking the rewards of these periods into a sample.
Finally, she uses the mean utility of sampled rewards as a value
representation of $x$. Let $s=[s_0,s_1,...,s_T]$ be a potentially
realized outcome of $x$ and $p(s)$ be the probability that $s$ is drawn.
I use $w(.)$ and $u(.)$ to denote the decision maker's weight function
and utility function, where $w(s_t)$ is the probability that the reward
of the $t$-th period in a potentially realized sequence $s$ is sampled,
$u(s_t)$ is the utility obtained by reward $s_t$
($t \in \{0,1,...,T\}$), $u'>0$, $u''<0$.

The sampling process is sequential, and the decision maker wants to find
a sampling strategy, denoted by function $w(.)$, that maximizes her
overall utility. In a given potentially realized sequence $s$, the
periods with larger reward levels should be sampled more frequently.
However, at the very beginning, the decision maker has no information
about which period in $s$ has a larger reward -- she learns such
information gradually in the process of sampling. This learning process
triggers a cognitive cost. Hence, her overall utility is the mean
utility of sampled rewards minus the cognitive cost of learning.

Suppose when having no information, the weight on period $t$ across each
potentially realized sequence is equal ($\equiv w^0_t$). Let $W$ and $P$
be the minimal sets that contain all available function $w$ and $p$
respectively. We can use an optimization problem to represent the
described evaluation procedure: $$ 
\begin{aligned}
\max_{w\in W}  \quad & \sum_{s\in X}\sum_{t=0}^T w(s_t)u(s_t) - C(w;\theta) \\
s.t. \quad &  \sum_{s\in X}\sum_{t=0}^T w(s_t)=1 \\
& w(s_t)>0, \forall s\in X,t=0,1,…,T \\
\end{aligned}
$$where $C(.)$ is a cognitive cost function with $\theta$ as its
parameters. To solve this optimization problem, I add two additional
assumptions. The first is that the weight updating process is consistent
with Bayes rule, that is, $w^0_t=\sum_{s\in X} w(s_t)$. The second is
that the cognitive cost function takes a form similar to Shannon mutual
information, that is$$
C(\textbf{w};\theta)= \lambda \sum_{s\in X}\sum_{t=0}^T w(s_t) \log\left(\frac{w(s_t)}{p(s)w_t^0}\right)
$$where $p(s)w^0_t$ is the probability of $s_t$ being sampled when no
information is learned, $w(s_t)$ is the probability of that after
learning the information about $x$. Shannon mutual information
quantifies the amount of information gain when learning about which time
period has a larger reward in any initially unknown $s$. Consistent with
@matejka_rational_2015, I set $C(\textbf{w};\theta)$ linear to that.
Parameter $\lambda$ denotes unit cost of information ($\lambda>0$).

Define $w(s_t|s) = \frac{w(s_t)}{p(s)}$. As is shown in
@matejka_rational_2015, the optimization problem can be easily solved by
Lagrangain method. The solution is$$ \tag{1}
w(s_t|s) =\frac{w_t^0e^{u(s_t)/\lambda}}{\sum_{t=0}^T w_\tau^0 e^{u(s_t)/\lambda}}
$$Note $w(s_t|s)$ reveals how the decision maker weights the utility of
time period $t$ in a drawn sequence $s$. It can naturally represent the
discounting factor. $w(s_t|s)$ is increasing in $s_t$, which implies the
decision maker exhibit more patience for a larger reward.

While building the model, I was mainly inspired by the theories of
rational inattention [@matejka_rational_2015; @jung_discrete_2019;
@mackowiak_rational_2023]. In @matejka_rational_2015's theory of
rational inattention, the decision maker makes choices between discrete
alternatives; she evaluates each alternative via a costly information
acquisition process, then decides the optimal choice strategy. The
theory deduces the probability of each alternative being chosen should
follow a logistic-like distribution. In ADU, I assume the discounting
factors are generated by a similar process; hence, she subjectively
weights each time period according to a logistic-like distribution -- as
Equation (1) does -- as well.

The reason why I use Shannon mutual information as the cognitive cost
function is twofold. First, note that
$w(s_t|s) \propto w^0_t e^{u(s_t)/\lambda}$. Given a certain stream $s$
and two time periods $t_1$ and $t_2$ ($t_2>t_1$), the relative weight
between them $\frac{w(s_{t_1}|s)}{w(s_{t_2}|s)}$ is only relevant to
$s_{t_1}$ and $s_{t_2}$. Therefore, changing the reward of a third
period has no impact on how the reward in $t_2$ should be discounted
relative to that in $t_1$. Second, under such settings, the objective
function can be rewritten as$$
\sum_{s\in X} p(s)[w(s_t|s)u(s_t) - \lambda D_{KL}]
$$

where $D_{KL}$ is the KL divergence between the initial weights over
time periods and the weights updated given the stream $s$ is drawn.
Clearly, the determination of $w(s_t|s)$ in each $s$ can be separated
from each other. In other words, given two potentially realized streams
$s$ and $s'$, the changes in $s'$ has no impact on the determination of
discounting factors in $s$. This property is consistent with many forms
of optimal sequential learning (for example, @zhong_optimal_2022 ).
@matejka_rational_2015 and @caplin_rationally_2022 show that the two
properties are jointly satisfied if and only if the solution of
$w(s_t|s)$ follows Equation (1).

## Comparison With Existing Models

In addition to ADU, there are other models that attempt to incorporate
attention mechanism into the formation of time preferences. For example,
@steiner_rational_2017 consider a decision maker adjusting the belief
$p(s)$ over time but holding the discounting factor $w(s_t|s)$ constant.
In each time period, given that her ability to learn new information is
limited, the updated belief cannot deviate from that in a previous
period by too much, which causes behavioral inertia. Instead, ADU
assumes the decision maker re-allocates $w(s_t|s)$ each time period.
Thus, the process of attention adjustment not only affects dynamic
decision-making but also affects the choices in "Money Earlier or Later"
(MEL) tasks. Besides, @gabaix_myopia_2017 assume the perception of
future rewards is noisy and the decision maker infers the value of them
by sampling from normal distributions; @gershman_rationally_2020 allow
the decision maker optimally chooses sample variance to minimize the
mean sample squared error. Such theories, together with a certain
specification on rate-distortion function, can lead to
magnitude-increasing patience and hyperbolic-like discounting.
Discounting factors in this style can be viewed as a special case of
those in ADU. @noor_optimal_2022, @noor_constrained_2023 construct an
optimization problem similar to ADU. However, they use a different
cognitive cost function. I compare the performance of ADU with models of
@gershman_rationally_2020, @noor_optimal_2022 and some other papers in
predicting human choices in MEL tasks.

# Implications in Time Preferences \label{behavioral}

## Evaluating A Delayed Reward

Suppose a decision maker receives a positive detereminstic reward in
time period $j$ (and no reward in other periods), that is, for any
$t\in[0,1,…,T]$ and $t \neq j$, $x_t = 0$. The decision maker evaluates
the reward sequence by implementing the ADU evaluation procedure. For
simplicity, I set $v(0)=0$. I also set the decision maker initially
holds stationary time preferences, i.e. $w^0_t=\delta^t$, where
$\delta\in(0,1]$. When $\delta=1$, we say that the initial attention is
uniformly distributed across time periods. Given the reward is
detereminsic, one can omit $s$ in $w(s_t|s)$ and directly represent the
weight on each time period $t$ by $w_t$.

$$ 
w_j = \left\{ \begin{aligned}
& \delta^j \cdot\frac{1}{1+\frac{\delta}{1-\delta}(1-\delta^T)e^{-v(x_j)}}\;, & 0<\delta<1 \\
& \frac{1}{1+T\cdot e^{-v(x_j)}}\; , & \delta=1
\end{aligned}
\right.
$$

## Hidden Zero Effect

One way to validate the model is that, when we frame of the length of
sequence in different ways, the decision maker's overall utility may
change.

One evidence is @magen_hidden-zero_2008, which find that people perform
greater patience when both SS and LL are framed as sequences, rather
than being framed single-period rewards. They term this finding as
"hidden zero effect". For instance, suppose SS is "receive £100 today"
and LL is "receive £120 in 6 months", and we have

SS~0~: "receive £100 today and £0 in 6 months"

LL~0~: "receive £0 today and £120 in 6 months"

Then people will be more likely to prefer LL~0~ over SS~0~ than
preferring LL over SS. The subsequent studies (e.g. @read_value_2017)
show that the hidden zero effect is asymmetric. That is, shifting SS to
SS~0~ and keeping LL unchanged leads to an increase in patience, whereas
shifting LL to LL~0~ and keeping SS unchanged cannot increase patience.
The attention-based explanation is that, in SS, the decision maker may
conceive the length of sequence as \"today"; in SS~0~, she may conceive
the length as "6 months". In the latter case, she allocates some
attention weights to some future periods with zero reward, which
decreases her overall utility.

The existence of hidden zero effect also provides some hints on the
selection of time length $T$. As is shown in Equation \ref{eq:w_delay},
when evaluating a delayed reward delivered in period $j$, the range of
$T$ is $[j,+\infty)$. An increase in $T$ will reduce the overall
utility. Thus, when comparing SS and LL, the decision maker may tend to
set $T=j$ (the minimum length she can set), in order to optimize the
overall utility. That is, without mentioning the periods after $j$, she
does not necessarily sample from the periods later than $j$ to evaluate
the given delayed reward. In this case, i.e. $T=j$ we have

$$
w_T = \frac{1}{1+G(T)e^{-v(x_T)}}
$$

where

$$
G(T) = \left\{ \begin{aligned}
& \frac{1}{1-\delta}(\delta^{-T}-1) \; ,& 0<\delta<1\\
& T\; ,& \delta=1\
\end{aligned}
\right.
$$

Clearly, when $\delta=1$, the attention weight $w_j$ takes a form
similar with hyperbolic discounting.

## Common Difference Effect

Suppose there are a large later reward $x_l$ arriving at period $t_l$
(denoted by LL) and a small sooner reward $x_s$ arriving at period $t_s$
(denoted by SS), where $x_l>x_s>0$, $t_l>t_s>0$. Assuming
$w_{t_l}(x_l)v(x_l)=w_{t_s}(x_s)v(x_s)$, common difference effect
implies $w_{t_l+\Delta t}(x_l)v(x_l)>w_{t_s+\Delta t}(x_s)v(x_s)$ for
any positive integer $\Delta t$[@loewenstein_anomalies_1992].

**Proposition 1**: If the initial weights are uniformly distributed,
then the common difference effect always holds; if the initial weights
exponentially declines over time, the common difference effect holds
when $v(x_l)-v(x_s)+\ln\frac{v(x_l)}{v(x_s)}>-(t_l-t_s)\ln\delta$.

When $\delta = 1$, ADU predicts that the decision maker always performs
common difference effect. This is obvious because the discounting factor
$w_T$ takes a hyperbolic-like form. When $\delta<1$, the decision maker
performs common difference effect only when the difference between $x_l$
and $x_s$ are much larger than the difference between $t_l$ and $t_s$.

The ADU's prediction on common difference effect can be understood as
follows. Note that $w_t \propto \delta^t e^{u(x_t)/\lambda}$. If we omit
the constraint that the sum of weights on each time period is fixed
(i.e. attention is limited), then
$w_{t_l+\Delta t}(x_l) = \delta^{\Delta t} \cdot w_{t_l}$ and the same
can be applied to $w_{t_s+\Delta t}$. Thus,
$w_{t_l+\Delta t} / w_{t_s+\Delta t}$ keeps constant for any $\Delta t$.
However, given the decision maker's attention is limited, the change
from $w_{t_l}$ to $w_{t_l+\Delta t}$ is not only driven by
$\delta^{\Delta t}$, but also driven by the effect that the final
period, with a positive reward, can naturally grab attention from the
previous periods which has no reward. Since $x_l > x_s$, this
attention-grabbing effect is greater for LL than for SS. Meanwhile, when
extending the time length, the average attention that can be allocated
to each period should shrink. The decision makers performs common
difference effect only when the former effect exceeds the latter effect.

## Magnitude Effect

Assuming we have $t_l$, $t_s$, $x_l$ fixed, and want to find a $x_s$
such that $w_{t_l}(x_l)v(x_l) = w_{t_s}(x_s)v(x_s)$, the magnitude
effect says that, if we increase $x_l$, then the $x_s/x_l$ that makes
the equality valid will also increase.

**Proposition 2**: The magnitude effect always holds when the utility
function satisfies

$$
RRA_v(x)\leq 1
$$

where $RRA_v(x)$ is the relative risk aversion coefficient of $v(x)$.

**Corollary 1**: Suppose $v(x)=x^\gamma/\lambda$, where $\gamma>0$ and
$\lambda>0$. Then magnitude effect always holds if and only if
$0<\gamma<1$.

## Non-Additive Time Intervals

Both exponential and hyperbolic discounting models predict the decision
maker is risk seeking over time lotteries. That is, suppose a
deterministic reward of level $c$ ($c>0$) is delivered in period $t_s$
with probability $\pi$ and is delivered in period $t_l$ with probability
$1-\pi$ ($0<\pi<1$); another deterministic reward, of the same level, is
delivered in a certain period $\pi t_s +(1-\pi) t_l$. The decision maker
should prefer the former case to the latter case. However,
@onay_intertemporal_2007 find in experiments that people are only risk
seeking over time lotteries when $\pi$ is small and are risk averse over
time lotteries when $\pi$ is large. This finding can be explained by the
convexity of $w_T$.

Let $t_m = \pi t_s +(1-\pi) t_l$. By definition, the decision makers are
risk seeking over time lotteries when
$\pi w_{t_s}(c)+(1-\pi)w_{t_l}(c)>w_{t_m}(c)$. First, note the LHS
equals to the RHS when $\pi=0$ or $\pi=1$. Fixing $t_s$ and $t_l$, the
inequality implies $w_{t_m}(c)$ is convex in $t_m$. Second, it can be
proved that $w_T(c)$ is convex in $T$ if and only if $T$ is above a
certain threshold. This is also consistent with
@takeuchi_non-parametric_2011 that suggests the discount function should
be inverse S-shaped with respect to time. By contrast, in many models
such as exponential and hyperbolic discounting, discounting factors are
typically decided by a convex function of $T$. Third, note $t_m$ is
linearly decreasing with $\pi$, thus the decision maker is more likely
to be risk seeking over time lotteries when $\pi$ is small. The same can
be applied to the risk aversion case.

Now consider $T$ is small enough to make $w_T$ concave in $T$. In this
case, adding an extension to $T$ will increase the rate at which $w_T$
declines with $T$ -- this property is termed "super-additive time
intervals" by @read_is_2001. Moreover, ADU predicts intervals are
sub-additive when the total time length $T$ is large, and are
super-additive when $T$ is small, which is consistent with
@scholten_discounting_2006.

## Non-Separable Sequences

Let $x$ and $y$ denote two 2-period risky reward sequences. For $x$, the
realized sequence is [£100,£100] with probability 1/2, and is [£3,£3]
with probability 1/2. For $y$, the realized sequence is [£3,£100] with
probability 1/2, and is [£100,£3] with probability 1/2. Classical models
of intertemporal choice, such as @fishburn_time_1982, typically assume
the separability of potentially realized sequences. This implies that
the decision maker is indifferent between $x$ and $y$. However,
@andersen_multiattribute_2018 find evidence of intertemporal correlation
aversion, that is, people often prefer $y$ to $x$. Such a property is
also termed "weak separability" in @noor_constrained_2023.

ADU can naturally yield intertemporal correlation aversion. For
simplicity, suppose the initial attention is uniformly distributed
across the two periods. For $x$, under each potentially realized
sequence, the decision maker equally weights each period. For $y$,
decision maker tends to assign more weight to the period with a reward
of £100 (suppose that weight is $w$). Then the value of $x$ is
$\frac{1}{2} u(100) + \frac{1}{2} u(3)$ and the value of $y$ is
$w\cdot u(100) +(1-w) \cdot u(3)$. Given that $x>\frac{1}{2}$, the
decision makers should strictly prefer $y$ to $x$.

## Reference-Dependent Preferences

ADU predicts that the larger the unit cost of information $\lambda$ or
the smaller the magnitude of $x_l$ and $x_s$ is, the more likely it is
that the decision maker performs magnitude effect.

First, note that the magnitude effect requires the decision maker's
overall utility $w_T(x_T)u(x_T)$ to be a convex function of $x_T$. Given
that $u(.)$ is concave, whether the magnitude effect holds should depend
on $w_T$. Then, set $z = u(x_T)-\lambda\log G(T)$. We can rewrite
Equation (2) as a logistic function of $z$, i.e.
$w_T = 1/(1+e^{-z/\lambda})$. By the shape of logistic function, $w_T$
is convex in $u(x_T)$ if and only if $u(x_T)<\lambda \log G(T)$ (that
is, when $x_T$ is small relative to $T$ or when $\lambda$ is large).
Finally, it is notable that the given condition is necessary but not
sufficient to yield magnitude effect.

In summary, holding the others equal, the decision makers' overall
utility can be convex in a future reward when the level of it is under a
certain threshold, and be concave when it is above the threshold. This
is also consistent to the theories about reference-dependent preferences
[@koszegi_model_2006].

# Attentional Account of Dynamic Inconsisteny

Suppose a decision maker has budget $m$ ($m>0$) and is considering how
to spend it over different time periods. We can use a reward sequence
$x$ to represent this decision problem, where the decision maker's
spending in period $t$ is $x_t$. In period 0, she wants to find a $x$
such that$$ \tag{3}
\max_{x}\;\sum_{t=0}^T w_t u(x_t)\quad s.t. \;\sum_{t=0}^T x_t = m  
$$

where $w_t$ is the attention-adjusted discounting factor in period $t$.
I assume
$w_t=\delta^t e^{u(x_t)/\lambda}/\sum_{t=\tau}^T \delta^{\tau} e^{u(x_\tau)/\lambda}$
and there is no risk under this setting.

In models like exponential and hyperbolic discounting, the discounting
factor of a future period is consistently smaller than that of the
current period. Thus, the decision maker should spend more at the
present than in the future. By contrast, in ADU, when increasing the
spending in a certain period, the discounting factor corresponding to
that period should also increase. So it is possible that the decision
maker spends more in the future and that a future period has a greater
discounting factor than the current period. This is consistent with
@loewenstein_preferences_1993 that find people sometimes prefer
improving sequences to declining sequences.

ADU suggests there are two mechanisms that can help explain why people
may perform dynamically inconsistent behavior. The first is
*attention-grabbing effect*, that is, keeping the others equal, when we
increase $x_t$ (which lead to an increase in $w_t$), the discounting
factor in any other period should decrease due to limited attention.
After omitting a previous period from the decision problem in Equation
(3), the decision maker can assign more weights to remaining periods;
thus, the attention-grabbing effect is enhanced. The increased
attention-grabbing effect will offset some benefit of increasing
spending toward a certain period. Therefore, when the decision maker
prefers improving sequences, the attention-grabbing effect will make her
perform a present bias-like behavior (always feeling that she should
spend more at the present than the original plan); when the decision
maker prefers declining sequences, this effect will maker her perform a
future bias-like behavior (always feeling she should spend more in the
future).

The second mechanism is *initial attention updating*. As is assumed
above, in period 0, prior to evaluating each reward sequence, the
decision maker's initial weight on period $t$ is proportional to
$\delta^t$; after evaluation, the weight becomes being proportional to
$\delta^t e^{u(x_t)/\lambda}$. In period 1, if she implements the
evaluation based on the information attained in period 0, the initial
weight should be updated to being proportional
$\delta^t e^{u(x_t)/\lambda}$; thus, the weight after evaluation should
become being proportional to $\delta e^{2u(x_t)/\lambda}$. As a result,
the benefit of increasing spending toward a certain period gets
strengthened. The updated initial attention can make those who prefer
improving sequences perform present bias and those who prefer declining
sequences perform future bias.

Both the attention-grabbing effect and initial attention updating are
affected by the curvature of utility function. They jointly decide which
behavior pattern that people should perform in dynamics.

# Consumption Oversmoothness and Oversensitivity

# Discussion

# Reference
