---
output: latex_fragment
---

\newpage

# Appendix {.unnumbered}

## A. Proof of Proposition 1 {.unnumbered}

We present the proof of sufficiency here. That is, if $\succsim$ has an optimal discounting representation and satisfies Axiom 1-4, then it has an AAD representation.

\noindent \textbf{Lemma 1}: *If Axiom 1 and 3 hold, for any* $s_{0\rightarrow T}$*, there exist* $w_0, w_1, …, w_T > 0$ *such that* $s_{0\rightarrow T} \sim w_0 \cdot s_0 + ...+w_T\cdot s_T$*, where* $\sum_{t=0}^T w_t=1$*.*

\noindent \emph{Proof}: If $T=1$, Lemma 1 is a direct application of Axiom 3. If $T\geq 2$, for any $2\leq t\leq T$, there should exist $\alpha_t\in(0,1)$ such that $s_{0\rightarrow t}\sim \alpha_t\cdot s_{0\rightarrow t-1}+(1-\alpha_t)\cdot s_{t}$. By state-independence and reduction of compound alternatives, we can recursively apply such equivalence relations as follows:$$
\begin{aligned}
s_{0\rightarrow T} &\sim \alpha_{T-1}\cdot s_{0\rightarrow T-1} + (1-\alpha_{T-1})\cdot s_T \\
&\sim  \alpha_{T-1}\alpha_{T-2}\cdot s_{0\rightarrow T-2} + \alpha_{T-1}(1-\alpha_{T-2})\cdot s_{T-1} + (1-\alpha_{T-1})\cdot s_T \\
& \sim ...\\
& \sim w_0 \cdot s_0 + w_1\cdot s_1 +... +w_T\cdot s_T
\end{aligned}
$$where $w_0=\prod_{t=0}^{T-1}\alpha_t$, $w_T = 1-\alpha_{T-1}$, and for $0<t<T$, $w_t=(1-\alpha_{t-1})\prod_{\tau=t}^{T-1}\alpha_{\tau}$. It is easy to show the sum of $w_0,…,w_T$ is equal to 1. *QED*.

Therefore, if Axiom 1 and 3 hold, for any reward sequence $s_{0\rightarrow T}$, we can always find a convex combination of all its elements, such that the DM is indifferent between the reward sequence and this convex combination. If $s_{0\rightarrow T}$ is a constant sequence, i.e. all its elements are constant, then we can directly assume $\mathcal{W}$ is AAD-style. So henceforth, we discuss whether AAD can apply to non-constant sequences.

By Lemma 2, we show adding a new reward to the end of $s_{0\rightarrow T}$ has no impact on the relative decision weights of rewards in the original reward sequence.

\noindent \textbf{Lemma 2}: *For any* $s_{0\rightarrow T+1}$*, if* $s_{0\rightarrow T}\sim \sum_{t=0}^T w_t \cdot s_t$ *and* $s_{0\rightarrow T+1} \sim \sum_{t=0}^{T+1} w'_t\cdot s_t$*, where* $w_t, w'_t>0$ and $\sum_{t=0}^Tw_t=1$*,* $\sum_{t=0}^{T+1}w'_t=1$*, then when Axiom 1-4 hold, we can obtain* $\frac{w'_0}{w_0}=\frac{w'_1}{w_1}=…=\frac{w'_T}{w_T}$*.*

\noindent \emph{Proof}: According to Axiom 3, for any $s_{0\rightarrow T+1}$, there exist $\alpha,\zeta \in (0,1)$ such that$$\tag{A1}
\begin{aligned}
s_{0 \rightarrow T}\sim\alpha\cdot s_{0 \rightarrow T-1} + (1-\alpha)\cdot s_T \\
s_{0\rightarrow T+1} \sim \zeta\cdot s_{0\rightarrow T} + (1-\zeta)\cdot s_{T+1}
\end{aligned}
$$On the other hand, we drawn on Lemma 1 and set$$\tag{A2}
s_{0\rightarrow T+1} \sim \beta_0\cdot s_{0 \rightarrow T-1} + \beta_1\cdot s_T + (1-\beta_0-\beta_1)\cdot s_{T+1}
$$where $\beta_0, \beta_1 > 0$. According to Axiom 4, $1-\zeta=1-\beta_0-\beta_1$. So, $\beta_1=\zeta-\beta_0$. This also implies $\zeta > \beta_0$.

According to Axiom 2, we suppose there exists a reward sequence $s$ such that $s \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_T$. By Equation (A2) and reduction of compound alternatives, we have $s_{0\rightarrow T+1}\sim \zeta \cdot s + (1-\zeta)\cdot s_{T+1}$. Combining Equation (A2) with the second line of Equation (A1) and applying transitivity and state-independence, we obtain $s_{0\rightarrow T} \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1$.

We aim to prove that for any $s_{0\rightarrow T+1}$, we can obtain $\alpha=\frac{\beta_0}{\zeta}$. We show this by contradiction.

Given the symmetry of $\alpha$ and $\frac{\beta_0}{\zeta}$, we can assume that $\alpha > \frac{\beta_0}{\zeta}$. Consider the case that $s_{0 \rightarrow T-1} \succ s_T$. By state-independence, for any $c\in \mathbb{R}_{\geq 0}$, we have $(\alpha - \frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c \succ (\alpha - \frac{\beta_0}{\zeta})\cdot s_T + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c$. By Axiom 2, there exists $z\in \mathbb{R}_{\geq 0}$ such that $(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}\sim z$. Given $c$ is arbitrary, we can set $(1-\alpha+\frac{\beta_0}{\zeta})\cdot c \sim z$. By reduction of compound alternatives, we can derive that$$
(\alpha-\frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1} \succ (\alpha-\frac{\beta_0}{\zeta})\cdot s_T +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}
$$where the LHS can be rearranged to $\alpha\cdot s_{0\rightarrow T-1} + (1-\alpha)\cdot s_T$, and the RHS can be rearranged to $\frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1$. They both should be indifferent from $s_{0\rightarrow T}$. This results in a contradiction.

Similarly, in the case that $s_T \succ s_{0 \rightarrow T-1}$, we can also derive such a contradiction. Meanwhile, when $s_{0\rightarrow T}\sim s_T$, $\alpha$ and $\frac{\beta_0}{\zeta}$ can be any number within $(0,1)$. In that case, we can directly set $\alpha = \frac{\beta_0}{\zeta}$.

Thus, we have $\alpha = \frac{\beta_0}{\zeta}$ for any $s_{0\rightarrow T+1}$, which indicates $\frac{\beta_0}{\alpha}=\frac{\beta_1}{1-\alpha}=\zeta$. We can recursively apply this equality to any sub-sequence $s_{0\rightarrow t}$ ($t\leq T$) of $s_{0\rightarrow T+1}$, so that the lemma will be proved. *QED*.

Now we move on to prove Proposition 1. The proof contains six steps.

First, we add the constraints $\sum_{t=0}^T w_t=1$ and $w_t>0$ to the optimal discounting problem for $s_{0\rightarrow T}$ so that the problem can accomodate Lemma 1. According to the FOC of its solution, for all $t=0,1,….,T$, we have$$\tag{A3}
f_t'(w_t)=u(s_t)+\theta
$$where $\theta$ is the Lagrange multiplier. Given that $f'_t(w_t)$ is strictly increasing, $w_t$ is increasing with $u(s_t)+\theta$. We define the solution as $w_t =\phi_t(u(s_t)+\theta)$.

Second, we add a new reward $s_{T+1}$ to the end of $s_{0\rightarrow T}$ and apply Lemma 2 as a constraint on optimal discounting problem. Look at the optimal discounting problem for $s_{0\rightarrow T+1}$. For all $t\leq T$, its FOC should take the same form as Equation (A3). Hence, if the introduction of $s_{T+1}$ changes some $w_t$ to $w'_t$ ($w'_t \neq w_t$, where $w_t$ is the solution to optimal discounting problem for $s_{0\rightarrow T}$), the only way is through changing the multiplier $\theta$. Suppose introducing $s_{T+1}$ changes $\theta$ to $\theta-\Delta \theta$, we have $w'_t = \phi_t(u(s_t)+\theta-\Delta \theta)$.

By Lemma 2, we know $\frac{w_0}{w'_0}=\frac{w_1}{w'_1}=…=\frac{w_T}{w'_T}$. In other words, for $t=0,1,…,T$, we have $w_t \propto \phi_t(u(s_t)+\theta-\Delta \theta)$. We can rewrite $w_t$ as $$\tag{A4}
w_t = \frac{\phi_t(u(s_t)+\theta-\Delta \theta)}{\sum_{\tau=0}^{T}\phi_\tau(u(s_\tau)+\theta-\Delta \theta)}
$$

Third, we show that in $s_{0\rightarrow T}$, if we change each $s_t$ to $z_t$ such that $u(z_t)=u(s_t)+\Delta u$, the decision weights $w_0,…,w_T$ will remain the same. Note $\sum_{t=0}^T \phi_t(u(s_t)+\theta)=1$. It is clear that $\sum_{t=0}^T \phi_t(u(z_t)+\theta-\Delta u)=1$. Suppose changing every $s_t$ to $z_t$ moves $\theta$ to $\theta'$ and $\theta'<\theta-\Delta u$. Then, we must have $\phi_t(u(z_t)+\theta')<\phi_t(u(z_t)+\theta-\Delta u)$ since $\phi_t(.)$ is strictly increasing. Summing all such decision weights up will result in $\sum_{t=0}^T \phi_t(u(z_t)+\theta')<1$, which contradicts with the constraint that the sum of decision weights is 1. The same contradiction can apply to the case that $\theta'>\theta-\Delta u$. Therefore, changing every $s_t$ to $z_t$ must move $\theta$ to $\theta - \Delta u$, and each $w_t$ can only be moved to $\phi_t(u(z_t)+\theta -\Delta u)$, which is exactly the same as the original decision weight.

A natural corollary of this step is that, subtracting or adding a common number to all intantaneous utilities within a reward sequence has no effect on decision weights. What actually matters for determining the decision weights is the difference between these instantaneous utilities. This indicates, for convenience, we can subtract or add an arbitrary number to the utility function.

In other words, for a given $s_{0\rightarrow T}$ and $s_{T+1}$, we can define a new utility function $v(.)$ such that $v(s_t) = u(s_t) +\theta-\Delta \theta$. So, Equation (A4) can be rewritten as$$\tag{A5}
w_t = \frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))}
$$If $w_t$ takes the AAD form under the utility function $v(.)$, i.e. $w_t \propto d_t e^{v(s_t)/\lambda}$, then it should also take the AAD form under the original utility function $u(.)$.

Fourth, we show that in Equation (A4), $\Delta \theta$ has two properties: (i) $\Delta \theta$ is strictly increasing with $u(s_{T+1})$; (ii) suppose $\Delta \theta = \underline{\theta}$ when $u(s_{T+1})=\underline{u}$ and $\Delta\theta=\bar{\theta}$ when $u(s_{T+1})=\bar{u}$, where $\underline{u}<\bar{u}$, then for any $l \in(\underline{\theta},\bar{\theta})$, there exists $u(s_{T+1})\in(\underline{u},\bar{u})$ such that $\Delta \theta = l$.

The property (i) can be shown by contradiction. Let $\{w'_t\}_{t=0}^{T+1}$ denote a sequence of decision weights for $s_{0\rightarrow T+1}$. Suppose $u(s_{T+1})$ is increased but $\Delta \theta$ is constant. In this case, each of $w'_0,…,w'_T$ should also be constant. However, $w'_{T+1}$ should increase as it is strictly increasing with $u(s_{T+1})+\theta-\Delta \theta$ (as $\theta$ is determined only by the optimal discounting problem for $s_{0\rightarrow T}$, any operations on $s_{T+1}$ should have no effect on $\theta$). This contradicts with the constraint that $\sum_{t=0}^{T+1} w'_t =1$. The only way to avoid such contradictions is to set $\Delta \theta$ strictly increasing with $s_{T+1}$, so that $w'_0,…,w'_T$ are decreasing with $u(s_{T+1})$.

For property (ii), note that given $s_{0\rightarrow T+1}$ and $\theta$, $\Delta\theta$ is defined as the solution to $\sum_{t=0}^{T+1} \phi_t(u(s_t)+\theta-\Delta\theta)=1$. For any arbitrary number $l\in(\underline{\theta},\bar{\theta})$, the proof of property (ii) consists of two stages. First, for period $t=0,1,…,T$, we need to show $u(s_t)+\theta-l$ is in the domain of $\phi_t(.)$. Second, for period $T+1$, we need to show given any $\omega\in(0,1)$, there exists $u(s_{T+1})\in \mathbb{R}$ such that $\phi_{T+1}(u(s_{T+1})+\theta-l)=\omega$.

For the first stage, note $\phi_t(.)$ is the inverse function of $f'_t(.)$. Suppose when $\Delta\theta=\bar{\theta}$, we have $f'_t(w^{a}_t)=u(s_t)+\theta-\bar{\theta}$, and when $\Delta\theta=\underline{\theta}$, we have $f'_t(w^{b}_t)=u(s_t)+\theta-\underline{\theta}$. For any $l\in(\underline{\theta},\bar{\theta})$, we have $u(s_t)+\theta-l \in (f'_t(w^a_t),f'_t(w^b_t))$. Given that $f'_t(.)$ is continuous and strictly increasing, there must be $w_t\in(w^a_t,w^b_t)$ such that $f'_t(w_t)=u(s_t)+\theta-l$. So, $u(s_t)+\theta-l$ is in the domain of $\theta_t(.)$. For the second stage, given an arbitrary $\omega\in(0,1)$, we can directly set $u(s_{T+1})=f'(\omega)-\theta+l$, so that the target condition is satisfied.

A corollary of this step is that we can manipulate $\Delta \theta$ in Equation (A4) at any level between $[\underline{\theta},\bar{\theta}]$ by changing a hypothetical $s_{T+1}$.

Fifth, we show $\ln \phi_t(.)$ is linear under some condition. To do this, let us add a hypothetical $s_{T+1}$ to the end of $s_T$ and let $w'_t=\phi_t(v(s_t))$ denote the decision weights for $s_{0\rightarrow T+1}$. We can change the hypothetical $s_{T+1}$ within the set $\{s_{T+1}|v(s_{T+1})\in[\underline{v},\bar{v}]\}$ and see what will happen to the decision weights from period 0 to period $T$. Suppose this changes each $w'_t$ to $\phi_t(v(s_t)-\eta)$. Set $\eta=\underline{\eta}$ when $u(s_{T+1})=\underline{v}$ and $\eta=\bar{\eta}$ when $u(s_{T+1})=\bar{v}$. By Equation (A5), we have$$\tag{A6}
\frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))} = \frac{\phi_t(v(s_t)-\eta)}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau)-\eta)}
$$For each $t=0,1,...,T$, we can rewrite $\phi_t(v(s_t))$ as $e^{\ln \phi_t(v(s_t))}$. For the LHS of Equation (A6), multiplying both the numerator and the denominator by a same number will not affect the value. Therefore, Equation (A6) can be rewritten as $$
\frac{e^{\ln\phi_t(v(s_t))-\kappa\eta}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau))-\kappa\eta}} = \frac{e^{\ln\phi_t(v(s_t)-\eta)}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau)-\eta)}}
$$where $\kappa$ can be any constant number. By properly selecting $\kappa$, for all $t=0,1,...,T$, we can obtain$$\tag{A7}
\ln \phi_t(v(s_t))-\kappa\eta=\ln \phi_t(v(s_t)-\eta)
$$as long as $\eta \in [\underline{\eta},\bar{\eta}]$. Since $\ln\phi_t(.)$ is strictly increasing, for any $\eta\neq 0$, we have $\kappa>0$.

Finally, we denote the maximum and minimum of $\{v(s_t)\}_{t=0}^T$ by $v_{\max}$ and $v_{\min}$, and show that Equation (A7) can hold if $\eta = v_{\max} - v_{\min}$. That implies $v_{\max}-v_{\min}\in [\underline{\eta},\bar{\eta}]$, where $\underline{\eta}, \bar{\eta}$ are the realizations of $\eta$ at the points of $v(s_{T+1})=\underline{v}$ and $v(s_{T+1})=\bar{v}$. Obviously, $\underline{\eta}$ can take the value $\underline{\eta}=0$. Thus, we focus on whether $\bar{\eta}$ can take a value $\bar{\eta}\geq v_{\max}-v_{\min}$.

The proof is similar with the fourth step and consists of two stages. First, for $t=0,1,…,T$, we show $v(s_t)-v_{\max}+v_{\min}$ is in the domain of $\phi_t(.)$. That is, under some $w_t$, we have $f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}$. Note in a non-constant reward sequence, $v_{\max}-v_{\min}\in(0,+\infty)$. On the one hand, Equation (A5) indicates that the equation $f'_t(\omega)=v(s_t)$ has a solution $\omega$. On the other hand, by Definition 2, we know $\lim_{w_t\rightarrow 0}f'_t(w_t)=-\infty$. Given $f'_t(w_t)$ is continuous and strictly increasing, there must be a solution $w_t$ lying in $(0,\omega)$ for equation $f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}$. Second, we show that for any $\omega'\in(0,1)$, there exists some $v(s_{T+1})$ such that $\phi_{T+1}(v(s_{T+1})-v_{\max}+v_{\min})=\omega'$. This can be achieved by setting $v(s_{T+1})=f'_{T+1}(\omega')+v_{\max}-v_{\min}$.

As a result, for any period $t$ in $s_{0\rightarrow T}$, by Equation (A7), we have $\ln \phi_t(v(s_t))=\ln\phi_t(v(s_t)-\eta)+\kappa\eta$ so long as $\eta\in[0,v_{\max}-v_{\min}]$, where $\kappa>0$. We can rewrite each $\ln \phi_t(v(s_t))$ as $\ln \phi_t(v_{\min})+\kappa[v(s_t)-v_{\min}]$. Therefore, we have$$\tag{A8}
w_t \propto \phi_t(v_{\min})\cdot e^{\kappa[v(s_t)-v_{\min}]}
$$and $\sum_{t=0}^T w_t=1$. In Equation (A8), setting $\phi_t(v_{\min})=d_t$, $\lambda = 1/\kappa$, and apply the corollary of the third step, we can conclude that $w_t\propto d_t e^{u(s_t)/\lambda}$, which is of the AAD form.

## B. Proof of Proposition 2 {.unnumbered}

Note the instantaneous utilities of LL and SS are $v_l$ and $v_s$, and the delays for LL and SS are $t_l$ and $t_s$. According to Equation (4), the common difference effect implies that, if$$ \tag{B1}
\frac{v_s}{1+G(t_s)e^{-v_s}} = \frac{v_l}{1+G(t_l)e^{-v_l}}
$$then for any $\Delta t \geq 0$, we have $$ \tag{B2}
\frac{v_s}{1+G(t_s+\Delta t)e^{-v_s}} < \frac{v_l}{1+G(t_l+\Delta t)e^{-v_l}}
$$If $G(T)=T$, we have $G(t+\Delta t) = G(t) + \Delta t$. In this case, combining Equation (B1) and (B2), we can obtain$$ \tag{B3}
\frac{\Delta t e^{-v_s}}{v_s} > \frac{\Delta t e^{-v_l}}{v_l}
$$Given that function $\psi(v) = e^{-v}/v$ is decreasing with $v$ so long as $v>0$, Equation (B3) is valid.

If $G(T) = \frac{1}{1-\delta}(\delta^{-T}-1)$, we have$$
1+G(t+\Delta t)e^{-v} = \delta^{-\Delta t}[1+G(t)e^{-v}]+(\delta^{-\Delta t}-1)(\frac{e^{-v}}{1-\delta}-1)
$$ Thus, combining Equation (B1) and (B2), we can obtain$$\tag{B4}
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_s}}{1-\delta}-1}{v_s} >
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_l}}{1-\delta}-1}{v_l}
$$ Given that $0<\delta<1$, we have $\delta^{-\Delta t}>1$. So, Equation (B4) is valid if and only if$$\tag{B5}
\frac{1}{v_s}-\frac{1}{v_l}<\frac{1}{1-\delta}(\frac{e^{-v_s}}{v_s}-\frac{e^{-v_l}}{v_l})
$$ By Equation (B1), we know that$$\tag{B6}
\frac{1}{v_s}-\frac{1}{v_l}=\frac{1}{1-\delta}\left[\frac{(\delta^{-t_l}-1)e^{-v_l}}{v_l} -\frac{(\delta^{-t_s}-1)e^{-v_s}}{v_s}\right]
$$ Combining Equation (B5) and (B6), we have$$
\delta^{-t_l}\frac{e^{-v_l}}{v_l}<\delta^{-t_s}\frac{e^{-v_s}}{v_s} \Longleftrightarrow v_l - v_s + \ln \left(\frac{v_l}{v_s}\right)>-(t_l-t_s)\ln\delta
$$

## C. Proof of Proposition 3 {.unnumbered}

Suppose a positve reward $x$ is delivered at period $T$. By Equation (4), if $w_T$ is convex in $T$, we should have $\frac{\partial^2 w_T}{\partial T^2}\geq 0$. This implies$$\tag{C1} 2G'(T)^2\geq(G(T)+e^{v(x)})G''(T) $$

If $\delta=1$, then $G(T)=T$. We have $G'(T)=1$, $G''(T)=0$. Thus, Equation (C1) is always valid.

If $0<\delta<1$, then $G(T)=(1-\delta)^{-1}(\delta^{-T}-1)$. We have $G'(T)=(1-\delta)^{-1}(-\ln\delta)\delta^{-T}$, $G''(t)=(-\ln\delta)G'(T)$. Thus, Equation (C1) is valid when $$\tag{C2} \delta^{-T}\geq(1-\delta)e^{v(x)}-1 $$Given $T>0$, Equation (C2) holds true in two cases. The first case is $1\geq (1-\delta)e^{v(x)}-1$, which implies that $v(x)$ is no greater than a certain threshold $v(\underline{x})$, where $v(\underline{x})=\ln(\frac{2}{1-\delta})$. The second case is that $v(x)$ is above $v(\underline{x})$ and $T$ is above a threshold $\underline{t}$. In the second case, we can take the logarithm on both sides of Equation (C2). It yields $\underline{t}=\frac{\ln[(1-\delta)\exp\{v(x)\}-1]}{\ln(1/\delta)}$.

## D. Proof of Proposition 4 {.unnumbered}

For convenience, we use $v$ to represent $v(x)\equiv u(x)/\lambda$, and use $U$ to represent $U(x,T)$. Set $g= G(T)$. The first-order derivative of $U$ with respect to $x$ can be written as$$\tag{D1}
\frac{\partial U}{\partial x}=v'\frac{e^v+U}{e^v+g}
$$If $U$ is strictly concave in $x$, we should have $\frac{\partial^2 U}{\partial x^2}<0$. By Equation (D1), we calculate the second-order derivative of $U$ with respect to $x$, and rearrange this second-order condition to$$\tag{D2}
2\zeta(v)+\frac{1}{1+v\zeta(v)}-1<\frac{-v''}{(v')^2}\equiv\frac{d}{dx}\left(\frac{1}{v'}\right)
$$

where $\zeta(v)=g/(g+e^v)$. Since $v''<0$, the RHS of Equation (D2) is clearly positive.

To prove the first part of Proposition 4, we can show that when $x$ is large enough, the LHS of Equation (D2) will be non-positive. To make the LHS non-positive, we require$$\tag{D3}
\zeta(v)+\frac{1}{v}\leq\frac{1}{2}
$$hold true. Note that $\zeta(v)$ is decreasing in $v$, and $v$ is increasing in $x$. Hence, $\zeta(v)+\frac{1}{v}$ is decreasing in $x$. Besides, it approaches $+\infty$ when $x\rightarrow0$ and approaches 0 when $x\rightarrow +\infty$. When $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ is continuous, there must be a unique realization of $x$ in $(0,+\infty)$, say $\bar{x}$, making the equality in Equation (D3) valid. Moreover, when $x\geq\bar{x}$, Equation (D3) is always valid. In such cases, $U(x,T)$ is concave in $x$.

To prove the second part, first note that when $x=0$, the LHS of Equation (D2) will become $\frac{2g}{g+1}$. If $\frac{d}{dx}\left(\frac{1}{v'(0)}\right)$ is smaller than this number, then the LHS of Equation (D2) should be greater than the RHS at the point of $x=0$. Meanwhile, from the first part of the current proposition, we know the LHS is smaller than the RHS at the point of $x=\bar{x}$. Thus, given $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ is continuous in $[0,\bar{x}]$, there must also be a point within $[0,\bar{x}]$, such that the LHS equals the RHS. Let $x^*$ denote the minimum of $x$ that makes the equality valid. Then, for any $x\in(0,x^*)$, we must have that the LHS of Equation (D2) is greater than the RHS, which implies $U(x,T)$ is convex in $x$. Given that $T\geq1$, we have $g\geq1$ and thus $\frac{2g}{g+1}\geq 1$. Therefore, when $\frac{d}{dx}\left(\frac{1}{v'(0)}\right)<1$, $U(x,t)$ can be convex in $x$ for any $x\in(0,x^{*})$, regardless of $g$.

The prove the third part, note $v(x)=u(x)/\lambda$. So, $$
\frac{d}{dx}\left(\frac{1}{v'}\right)=\lambda\frac{d}{dx}\left(\frac{1}{u'}\right)
$$We arbitrarily draw a point from $(0,\bar{x})$ and derive the range $\lambda$ relative to this point. For simplicity, we choose $x=\ln g$. In this case, the LHS of Equation (D2) becomes $\frac{2}{2+\ln g}$. Define a function $\xi(x)$, where $\xi$ is the value of the LHS of Equation (D2) minus its RHS. Note $\xi(x)$ is continuous at $x=\ln g$. Therefore, for any positive real number $b$, there must exist a positive real number $c$ such that, when $x\in(\ln g-c,\ln g+c)$, we have$$\tag{D4}
\xi(\ln g)-b<\xi(x)<\xi(\ln g)+b
$$If $\xi(\ln g)-b\geq 0$, then $\xi(x)$ will keep positive for all $x\in(\ln g-c,\ln g+c)$, which implies the LHS of Equation (D2) is always greater than its RHS.

Now we derive the condition for $\xi(\ln g)-b\geq 0$. Suppose when $x=\ln g$, $\frac{d}{dx}\left(\frac{1}{u'}\right)=a$ (note at this point we have $\frac{d}{dx}\left(\frac{1}{u'}\right)<+\infty$). Combining with Equation (D3), we know that $\xi(\ln g)-b =\frac{2}{2+\ln g}-\lambda a-b$. Letting this value be non-negative, we obtain$$\tag{D5}
\lambda \leq \frac{2}{a(2+\ln g)}-\frac{b}{a}
$$Given that $T\geq1$, we have $g\geq 1$ and thus $\frac{2}{2+\ln g}$ should be positive. Meanwhile, given that $u'>0$ and $u''<0$, $a$ should also be positive. Since $b$ can be any positive number, Equation (D5) holds if $\lambda <\frac{2}{a(2+\ln g)}$. That is, when $\lambda$ is positive but smaller than a certain threshold, there must be an interval $(\ln g-c,\ln g+c)$ such that the LHS of Equation (D2) is greater than the RHS. Set $x_1 = \max\{0,\ln g-c\}$, $x_2=\min\{\bar{x}, \ln g +c\}$. When $x\in (x_1,x_2)$, function $U(x,T)$ must be convex in $x$.

## E. Proof of Proposition 5 {.unnumbered}

The proof consists of four steps. First, we write the expressions for $U(L1)$ and $U(L2)$. Suppose the time length of each lottery result is $T$. For a period $\tau$ at which no reward is delivered, the instantaneous utility is zero. Let $\Omega$ denote the set of all such period $\tau$, then $\Omega=\{\tau\,|\,0\leq\tau\leq T,\;\tau \neq t_1,t_2\}$. For any $j,k\in\{s,l\}$, we define $\phi_j=d_{t_1}e^{v(x_j)}$ and $\eta_k=d_{t_2}e^{v(y_k)}$, where $v(s)=u(s)/\lambda$, and $d_t$ represents the reference factor for reward delivered at period $t$.

For a given lottery result $(s_1,s_2)$, we denote the decision weight of each positive reward by $w_{t_1}$ and $w_{t_2}$. By the definition of AAD, we have$$
w_{t_1} = \frac{\phi_j}{\phi_j + \eta_k +D} \quad ,\quad
w_{t_2} = \frac{\eta_k}{\phi_j + \eta_k +D}
$$where $j,k\in\{s,l\}$, $D=\sum_{\tau\in\Omega} d_{\tau}\geq 0$. The value of a lottery $L$ can be written as $U(L)=w_{t_1}u(s_1)+w_{t_2}u(s_2)$. Hence, $$\tag{E1}
\begin{aligned}
U(L1)=0.5\frac{\phi_s u(x_s)+\eta_s u(y_s)}{\phi_s+\eta_s+D} + 0.5\frac{\phi_l u(x_l)+\eta_l u(y_l)}{\phi_l+\eta_l+D} \\
U(L2)=0.5\frac{\phi_s u(x_s)+\eta_l u(y_l)}{\phi_s+\eta_l+D} + 0.5\frac{\phi_l u(x_l)+\eta_s u(y_s)}{\phi_l+\eta_s+D}
\end{aligned}
$$We observe that, when $x_l=x_s$, we have $U(L1)=U(L2)$.

Second, suppose we increase $x_l$ from $x_s$ by an increment. This increases both $U(L1)$ and $U(L2)$ (either by a positive or a negative number). To make $U(L1)<U(L2)$, this increment should increase $U(L2)$ by a greater number than $U(L1)$. Specifically, we assume $U(L2)$ is increasing faster than $U(L1)$ at any level of $x_l$. That is, the partial derivative of $U(L2)$ in terms of $x_l$ is always greater than that of $U(L1)$. Given $\phi_l$ is increasing in $x_l$, to see this, we can take partial derivatives in terms of $\phi_l$.

In each line of Equation (E1), note only the second term contains $x_l$. Thus, we focus on the difference between the second terms. The second term of the $U(L1)$ is influenced by $y_l$, while that of the $U(L2)$ is influenced by $y_s$, where $y_l>y_s$. Thus, we can construct a function $\xi$ such that$$
\xi(\phi_l,\eta) = \frac{\phi_l \cdot v(x_l)+\eta\cdot v(y)}{\phi_l+\eta+D}
$$where $\eta=d_{t_2}e^{v(y)}$. In reverse, we can define $v(x_l)=\ln(\phi_l/d_{t_1})$ and $v(y)=\ln(\eta/d_{t_2})$. The function $\xi$ is similar to the second term of each line, but note we replace $u(.)$ by $v(.)$. When $y=y_l$, $\xi$ is proportional to the second term of $U(L1)$. When $y=y_s$, $\xi$ is proportional to the second term of $U(L2)$ (by the same proportion). Thus, to show that the partial derivative of $U(L2)$ in terms of $x_l$ is greater than that of $U(L1)$, we just need to show $\partial \xi/\partial \phi_l$ is decreasing with $y$ (or $\eta$).

Third, we take the first- and second-order partial derivatives of $\xi(\phi_l,\eta)$. The partial derivative of $\xi$ in terms of $\phi_l$ is$$
\frac{\partial \xi}{\partial \phi_l}=\frac{(v(x_l)+1)\eta-v(y)\eta+\phi_l+D(v(x_l)+1)}{(\phi_l+\eta+D)^2}
$$We need to show that for $y\in[y_s,y_l]$, we can obtain $\partial^2 \xi/\partial \phi_l\partial \eta<0$. This implies$$\tag{E2}
(v(x_l)+v(y)+2)D-(\phi_l-\eta)(v(x_l)-v(y))+2(\phi_l+\eta)>0
$$We want Equation (E2) to hold for any $D\geq$. Given the LHS is increasing with $D$, this can only be achieved when$$\tag{E3}
2(\phi_l+\eta)>(\phi_l-\eta)(v(x_l)-v(y))
$$Define $\kappa=d_{t_2}/d_{t_1}$, $\alpha=v(x_l)-v(y)$. Note $\kappa\in \mathbb{R}_{>0}$, $\alpha\in\mathbb{R}$. Equation (E3) can be rewritten as$$\tag{E4}
(\alpha-2)\kappa^{-1} e^{\alpha}-\alpha-2<0
$$

Fourth, based on Equation (E4), we construct a function $h(\alpha)=(\alpha-2)\kappa^{-1} e^\alpha-\alpha-2$. We aim to examine whether there exists some $\alpha\in\mathbb{R}$ that makes $h(a)<0$. Obviously, $\alpha=-2$ and $\alpha=2$ satisfy this condition. Moreover, note $h(\alpha)$ is decreasing in $\alpha$ when $(\alpha-1)e^{\alpha}\leq \kappa$ and is increasing in $\alpha$ otherwise. And when either $\alpha\rightarrow -\infty$ or $\alpha \rightarrow +\infty$, we have $h(\alpha)\rightarrow +\infty$. Thus, there must be a limited interval $(\alpha_1,\alpha_2)$ such that $h(a)<0$ so long as $\alpha\in(\alpha_1,\alpha_2)$, and obviously $[-2,2]\subset(\alpha_1,\alpha_2)$. Since $v(s)=u(s)/\lambda$, this implies $\frac{u(x_l)-u(y)}{\lambda}\in(\alpha_1,\alpha_2)$.

For a given positive number $\kappa$, the points $\alpha_1,\alpha_2$ are determined by the solution to $\frac{\alpha-2}{\alpha+2}e^{\alpha}=\kappa$. In other words, for any $x_l$ and $y\in[y_s,y_l]$, we can always achieve $U(L1)<U(L2)$ as long as $u(x_l)-u(y_l)\geq \lambda\alpha_1$ and $u(x_l)-u(y_s)\leq\lambda\alpha_2$. So, we can conclude that for any $x_l>x_s>0$, $y_l>y_s>0$, any time length of lottery results and reference factor (which determines $D$ and $\kappa$), there exists some $\lambda$ that makes DM intertemporal correlation averse. Specifically, all $\lambda>\lambda^{**} ={\max}\{\frac{u(x_l)-u(y_l)}{\alpha_1},\frac{u(x_l)-u(y_s)}{\alpha_2}\}$ satisfy the target condition.

Notably, if $\lambda\leq\lambda^{**}$, we have $h(a)\geq0$, which by Equation (E2)(E3), indicates that under some conditions such as $D=0$, there will be $\partial^2 \xi/\partial \phi_l\partial \eta\geq0$ for all $y\in[y_s,y_l]$. In that case, at each level of $x_l$, the partial derivative of $U(L1)$ in terms of $x_l$ is greater than that of $U(L2)$. So, increasing $x_l$ by an increment from $x_s$ can induce a greater increase in $U(L1)$ than in $U(L2)$. This makes it possible that $U(L1)>U(L2)$. In short, DM may perform intertemporal correlation seeking when $\lambda\leq\lambda^{**}$.

## F. Proof of Proposition 6 {.unnumbered}

Before proving the proposition, we first show that in the DM's optimal consumption plan $s_{0\rightarrow T}$, the largest consumption must be $s_0$. We show this by contradiction. Suppose the largest consumption is $s_\tau$ ($\tau>0$). By Lemma 3, we obtain that if we exchange the consumption planned in $\tau$ with the consumption planned in period 0, the total value of consumption will be non-decreasing. So, the largest consumption must be the current one.

For convenience, henceforth we use $u_t$ to represent $u(s_t)$.

\noindent \textbf{Lemma 3}: *Suppose in* $s_{0\rightarrow T}$*, we have* $s_\tau= \max\{s_0,s_1,...,s_T\}$ *and* $\tau>0$. *Set* $u_0/\lambda=v_1$ *and* $u_{\tau}/\lambda=v_2$*. If we change* $u_0/\lambda$ *to* $v_2$ *and* $u_{\tau}/\lambda$ *to* $v_1$*,* $U(s_{0\rightarrow T})$ *will be non-decreasing.*

\noindent \emph{Proof}: Suppose that before we exchange consumption between period $t$ and $t+\tau$, the total value of consumption is $V/\lambda$ and $V=\sum_{t=0}^Td_t(u_t/\lambda)e^{u_t/\lambda}/\sum_{t=0}^Td_te^{u_t/\lambda}$. We define $P$ as the numinator of $V$ minus $\delta_1v_1e^{v_1}+\delta_2v_2e^{v_2}$ and define $Q$ as the denominator of $V$ minus $\delta_1e^{v_1}+\delta_2e^{v_2}$.

Set $d_0=\delta_1$, $d_\tau=\delta_2$. Note $v_2\geq V$. If changing changing $u_t/\lambda$ to $v_2$ and $u_{t+\tau}/\lambda$ to $v_1$ do not decrease $V$, we should have $$\tag{F1} \frac{\delta_1v_1e^{v_1}+ \delta_2v_2e^{v_2}+P}{\delta_1e^{v_1}+\delta_2e^{v_2}+Q} \leq  \frac{\delta_1v_2e^{v_2}+ \delta_2v_1e^{v_1}+P}{\delta_1e^{v_2}+\delta_2e^{v_1}+Q} $$where $\delta_1>\delta_2>0$, $v_2\geq v_1>0$. Rearranging Equation (F1), we can obtain$$\tag{F2} -(\delta_1+\delta_2)e^{v_1+v_2}(v_2-v_1)\leq e^{v_2}(Qv_2-P)-e^{v_1}(Qv_1-P) $$Clearly, Equation (F2) holds if $e^v(Qv-P)$ is increasing in $v$ when $v\in[v_1,v_2]$, and the latter implies $v_1\geq \frac{P}{Q}-1$.

Given that $V$ is a weighted mean of $v_1$, $v_2$ and $\frac{P}{Q}$, we can set $V=\omega_1v_1+\omega_2v_2+(1-\omega_1-\omega_2)\frac{P}{Q}$, where $\omega_1,\omega\in(0,1)$. If $v_1\geq V$, we must have $v_1\geq \frac{P}{Q}$. So, Equation (F6) must hold. If $v_1<V$, note that $\frac{\partial U}{\partial d_t}\propto u_t-U$. So, both decreasing $d_0$ to $\delta_2$ and increasing $d_\tau$ to $\delta_1$ would increase the total value of consumption. In summary, for either case, changing changing $u_t/\lambda$ to $v_2$ and $u_{t+\tau}/\lambda$ to $v_1$ do not decrease $V$. *QED*.

By calculating the first-order derivatives of $U$ in terms of $s_t$ and $s_{t+1}$, we have$$\tag{F3}
\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_t} = 
\delta \exp\{\frac{u_{t+1}-u_t}{\lambda}\}
\cdot\frac{u'_{t+1}}{u'_t}
\cdot\frac{u_{t+1}+\lambda-U}{u_t+\lambda-U}
$$Drawing on Equation (F3), we construct a function $\rho_t(s_{0\rightarrow T})=e^{u_t/\lambda}u'_t(u_t+\lambda-U)$. Henceforth, we use $\rho_t$ to represent $\rho_t(s_{0\rightarrow T})$, unless otherwise specified. Calculating the first-order derivative of $\rho_t$ in terms of $s_t$, we have$$\tag{F4}
\frac{\partial \rho_t}{\partial s_t} = 
e^{u_t/\lambda}[
(u_t+\lambda-U)((1-w_t)\frac{(u'_t)^2}{\lambda}+u''_t)
+(u'_t)^2]
$$

To prove the first part of Proposition 6, note that if$$\tag{F5}
\lambda\leq \min_{0 \leq t \leq T} \{-(u'_t)^2/u''_t \cdot(1-w_t)\}
$$we will always have $(1-w_t)\frac{(u'_t)^2}{\lambda}+u''_t \geq 0$. According to Equation (F4), this yields $\frac{\partial \rho_t}{\partial s_t}>0$ as long as $u_t+\lambda-U>0$.

Set $\underline{\lambda}$ as the RHS of Equation (F5). If $\lambda\leq \underline{\lambda}$, the DM will concentrate all consumption at period 0.

To see this, we first consider that $s_{0\rightarrow T}$ is a constant sequence. In this case, for all $t=0,1,...,T$, we have $\frac{\partial U}{\partial s_t}>0$. Also, by Equation (F1), we have $\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=\delta<1$. Hence, in a constant reward sequence, transferring an incremental consumption from period $t+1$ to $t$ will increase the total value of consumption $U$, and doing the opposite will decrease $U$. The DM would like to change this constant sequence to a decreasing sequence.

We discuss the implication of this change in two cases. First, suppose for all period $t$, we always have $u_t+\lambda-U>0$. Then, $\lambda \leq \underline{\lambda}$ can induce $\frac{\partial \rho_t}{\partial s_t}>0$ for any feasible $s_t$. So long as the DM's consumption plan is a decreasing sequence ($s_t>s_{t+1}$), we can obtain $\frac{\partial U}{\partial s_t}>0$ and $\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=\delta\frac{\rho_{t+1}}{\rho_{t}}<1$. In this case, the DM would like to keep transferring consumption from period $t+1$ to $t$, until all consumption is concentrated at period 0.

Second, suppose for some period $t>0$, reducing an amount of consumption will yield $u_t+\lambda-U< 0$ (note we always have $u_0+\lambda-U>0$, since $s_0$ is the largest consumption in plan). For $s_t$ that satisfies the given inequality, we have $\frac{\partial U}{\partial s_t}<0$. An increase in each of such $s_t$ would induce a reduction in $U$ and keeping reducing it would induce an increase in $U$. In this case, it is still rational for the DM to keep reducing future consumption until all consumption is concentrated at period 0.

For the remaining part of Proposition 6, we note that the statements hold only when DM does not concentrate all consumption at period 0. Suppose the optimal consumption plan is an interior solution to Equation (5). At the solution, we have$$\tag{F6}
\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=1
$$and $\frac{\partial^2 U}{\partial s_t^2}<0$. For period $\tau$ when the largest consumption occurs, we must have $u_\tau +\lambda - U>0$. Thus, to make Equation (F6) hold, we require that $u_t +\lambda - U>0$ for all $t=0,1,...,T$ (this can be satisfied by setting $\lambda \geq U$).

Without loss of generality, we focus on what happens on the DM's optimal consumption plan when she moves from period 0 to period 1. At period 1, we denote the rest of reward sequence by $s_{1 \rightarrow T}$, and the total value of consumption by $U_1\equiv U(s_{1\rightarrow T})$.

To prove the second part, note that setting $\frac{\partial^2 U}{\partial s_t^2}<0$ implies$$\tag{F7}
(1-2w_t)\frac{1}{\lambda} + \frac{1}{u_t+\lambda-U} < -\frac{u''_t}{(u'_t)^2} 
$$By Equation (F7), for any period $t$ such that $u_t+\lambda-U>0$, we have$$\tag{F8}
\frac{\partial \rho_t}{\partial s_t}<0 \;\Longleftrightarrow\;
(1-w_t)\frac{1}{\lambda}+\frac{1}{u_t+\lambda-U}<-\frac{u''_t}{(u'_t)^2}
$$When the DM moves from period 0 to period 1, in Equation (F8) $U$ is decreased to $U_1$. So, if Equation (F8) holds at period 0, then for period 1, we still have $\frac{\partial \rho_t}{\partial s_t}<0$. Meanwhile, if Equation (F8) holds, then Equation (F7) must hold.
