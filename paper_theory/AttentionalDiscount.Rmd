---
title: "An Attentional Model of Time Discounting"
author: "Zark Zijian Wang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: reference.bib
biblio-style: apalike
header-includes: 
  \usepackage{setspace,lscape}
  \usepackage{amsmath}
  \usepackage{caption,subcaption,multirow}
  \usepackage[hang]{footmisc}
  \usepackage{enumitem}
  \usepackage{standalone}
  \renewcommand{\arraystretch}{1.5}
  \captionsetup[table]{skip=5pt}
  \setstretch{1.5}
  \setlength{\parindent}{1em}
  \setlength{\footnotemargin}{3mm}
  \setlength{\footnotesep}{3mm}
  # \renewcommand{\[}{\begin{equation}} 
  # \renewcommand{\]}{\end{equation}}
fontsize: 12pt
geometry: margin=1in
editor_options: 
  markdown: 
    wrap: 72
output:
  #word_document:
    #number_sections: true
  pdf_document:
    number_sections: true
    citation_package: natbib
    keep_tex: true
    includes:
      after_body: Proof.tex
  html_document:
    toc: true
    number_sections: true
---

# Introduction

decision maker (DM)

Kullback--Leibler (KL) divergence (also called relative entropy)

hard attention

information avoidance

endogenous time preferences

optimal expectation

we present an axiomatic characterization of AAD with the optimal
discounting framework

# Model Setting

Assume time is discrete. Let $s_{0\rightarrow T}\equiv[s_0,s_1,...,s_T]$
denote a reward sequence that starts delivering rewards at period 0 and
ends at period $T$. At each period $t$ of $s_{0\rightarrow T}$, a
specific reward $s_t$ is delivered, where $t\in\{0,1,…,T\}$. Throughout
this paper, we only consider non-negative rewards and finite length of
sequence, i.e. we set $s_t \in \mathbb{R}_{\geq 0}$ and
$1\leq T<\infty$. The DM's choice set is constituted by a range of
alternative reward sequences which start from period 0 and end at some
finite period. When making an intertemporal choice, the DM seeks to find
a reward sequence $s_{0\rightarrow T}$ in her choice set, which has the
highest value among all alternative reward sequences. To calculate the
value of each reward sequence, we admit the additive discounted utility
framework. The value of $s_{0\rightarrow T}$ is defined as
$U(s_{0\rightarrow T})\equiv \sum_{t=0}^T w_{t}u(s_t)$, where $u(s_t)$
is the instantaneous utility of receiving $s_t$, and $w_t$ is the
decision weight (sometimes called discount factors) assigned to $s_t$.
We assume the function $u(.)$ is twice differentiable, $u'>0$, $u''<0$.

The determination of $w_t$ is central to this paper. We believe that,
due to the DM's limited attention and demand for information, the DM
tends to overweight the large rewards and underweight the small rewards
within the sequence. Specifically, we suggest $w_t$ follow a
(generalized) softmax function. We define any decision weight in this
style as an *attention-adjusted discount* factors (AAD), as in
Definition 1.

\noindent \textbf{Definition 1}: *Let* $\mathcal{W}\equiv[w_0,...,w_T]$
*denote the decision weights for all specific rewards in*
$s_{0\rightarrow T}$*.* $\mathcal{W}$ *is called attention-adjusted
discount factors (AADs) if for any* $t\in\{0,1,…,T\}$,$$\tag{1}
w_t = \frac{d_te^{u(s_t)/\lambda}}{\sum_{\tau=0}^T d_\tau e^{u(s_\tau)/\lambda}} 
$$*where* $d_t > 0$*,* $\lambda>0$*,* $u(.)$ *is the utility function.*

In intuition, how Definition 1 reflects the role of attention in
valuating reward sequences can be explained with four points. First,
each reward in a sequence could be viewed as an information source and
we assume the DM allocates limited information-processing resources
across those information sources. The AADs capture this notion by
normalizing the discount factors, i.e. fixing the sum of $w_t$ at 1. As
a result, increasing the decision weight of one reward would reduce the
decision weights of other rewards in the sequence, implying that
focusing on one reward would make DM insensitive to the values of other
rewards. Meanwhile, when there are more rewards in the sequence, DM
needs to split attention across a wider range to process each of them,
which may reduce the attention to, or decision weight of, each
individual reward.

Second, $w_t$ is strictly increasing with $s_t$, indicating that DM
would pay more attention to larger rewards. This is consistent with many
empirical studies that suggest people tend to pay more attention to
information associated with larger rewards. For instance, people perform
a "value-driven attentional capture" effect in visual search
[@della2009learning; @hickey2010reward; @anderson2011value;
@chelazzi2013rewards; @jahfari2017sensitivity]. In one study
[@hickey2010reward], researchers recruit participants to do a series of
visual search trials, in each of which participants earn a reward after
detecting a target object from distractors. If a target object is
associated with a large reward in previous trials, it can naturally
capture more attention. Therefore, in the next trial, presenting the
object as a distractor slows down the target detection.[^1] In addition,
in financial decision making, investors usually perform an ostrich
effect [@galai2006ostrich; @karlsson2009ostrich]. One relevant evidence
is that stock traders log in their brokerage accounts less frequently
after market declines [@sicherman2016financial].

[^1]: Some scholars may classify attention into two categories:
    "bottom-up control" and "top-down control". However, the evidence
    about value-driven attentional capture does not fall into either of
    these categories. Thus, in this paper, we do not describe attention
    with this dichotomy. Instead, we view attention as a mechanism that
    seeks to maximize the utility of information.

Third, $w_t$ is "anchored" in a reference weight $d_t$. For a certain
sequence of rewards, $d_t$ could denote the initial weight that the DM
would assign to a reward delivered at period $t$ without knowing its
realization. The determination of $d_t$ is mediated by the difficulty to
mentally represent a future event [@trope2003temporal] and the frequency
of time delays in a global context [@stewart2006decision]. The
constraint on the deviation between $w_t$ and $d_t$ indicates that
reallocating attention or acquiring new information is costly. The
deviation of $w_t$ from $d_t$ depends on parameter $\lambda$, which as
we discuss in the next section, can reflect the unit cost of information
acquisition. A large $\lambda$ implies a low learning rate and a high
cognitive cost in adapting the decision weights to the local context.

Fourth, we adopt the idea of @gottlieb2012attention and
@gottlieb2013information that attention can be understood as an active
information-sampling mechanism which selects information based on the
perceived utility of information. For intertemporal choices, we assume
the DM would selectively sample value information from each reward
(information source) when processing a reward sequence, and the AAD can
represent an approximately optimal sampling strategy. Note that the AADs
follow a softmax function. @matvejka2015rational and
@mackowiak2023rational claim that if a behavioral strategy conforms to
this type of function, then it can be interpreted as a solution to some
optimization problem under information constraints.

# Interpretation

## Information Maximizing Exploration \label{info_exploration}

In this section, we provide two approaches to characterize AAD: the
first is based on information maximizing exploration, and the second is
based on optimal discounting. These approaches are closely related to
the idea proposed by @gottlieb2012attention, @gottlieb2013information
and @sharot2020people, that people tend to pay attention to information
with high *instrumental utility* (help identifying the optimal action),
*cognitive utility* (satisfying curiosity), or *hedonic utility*
(inducing positive feelings). It is worth mentioning that the well-known
rational inattention theories are grounded in the instrumental utility
of information.[^2] Instead, in this paper, we draw on the cognitive and
hedonic utility of information to build our theory of time discounting.
Our first approach to characterizing AAD is relevant to the cognitive
utility: the DM's information acquisition process is curiosity-driven.
The model setting of this approach, similar with @gottlieb2012attention
and @gottlieb2013information, is based on a reinforcement learning
framework. Specifically, we assume the DM seeks to maximize the
information gain with a commonly-used exploration strategy. Our second
approach is relevant to the hedonic utility: the DM consider the
feelings of multiple selves and seeks to maximize their total utility
under some cognitive cost. The theoretical background for the second
approach is \citet{noor2022optimal,noor2024constrained}. We describe the
first approach in this subsection and the second approach in Section
\ref{optimal_discount}.

[^2]: The rational inattention theory assumes the DM learns information
    about different options in order to find the best option. For
    details, see @sims2003implications, @matvejka2015rational, and
    @mackowiak2023rational.

For the information maximizing exploration approach, we assume that
before having any information of a reward sequence, the DM perceives it
has no value. Then, each reward in the sequence $s_{0\rightarrow T}$ is
processed as an individual information source. The DM engages her
attention to actively sample signals at each information source and
update her belief about the sequence value accordingly. The signals are
nosiy. For any $t\in\{0,1,…,T\}$, the signal sampled at information
source $s_t$ could be represented by $x_t =u(s_t)+\epsilon_t$, where
each $\epsilon_t$ is i.i.d. and
$\epsilon_t \sim N(0,\sigma_\epsilon^2)$. The sampling weight for
information source $s_t$ is denoted by $w_t$.

The DM's belief about the sequence value $U(s_{0\rightarrow T})$ is
updated as follows. At first, she holds a prior $U_0$, and given she
perceives no value from the reward sequence, the prior could be
represented by $U_0 \sim N(0, \sigma^2)$. Second, she draws a series of
signals at each information source $s_t$. Note we define
$U(s_{0\rightarrow T})$ as a weighted mean of instantaneous utilities,
i.e. $U(s_{0\rightarrow T})=\sum_{t=0}^Tw_tu(s_t)$. Let $\bar{x}$ denote
the mean sample signal and $U$ denote a realization of
$U(s_{0\rightarrow T})$. If there are $k$ signals being sampled in
total, we should have
$\bar{x} | U, \sigma_\epsilon\sim N(U,\frac{\sigma_{\epsilon}^2}{k})$.
Third, she uses the sampled signals to infer $U(s_{0\rightarrow T})$ in
a Bayesian fashion. Let $U_k$ denote the valuer's posterior about the
sequence value after receiving $k$ signals. According to the Bayes'
rule, we have $U_k\sim N(\mu_k,\sigma_k^2)$ and$$
\mu_k = \frac{k^2\sigma_\epsilon^{-2}}{\sigma^{-2}+k^2\sigma_\epsilon^{-2}}\bar{x}\qquad,\qquad 
\sigma_k^2 =  \frac{1}{\sigma^{-2}+k^2\sigma_\epsilon^{-2}}
$$We assume the DM takes $\mu_k$ as the valuation of reward sequence. It
is clear that as $k\rightarrow \infty$, the sequence value will converge
to the mean sample signal, i.e. $\mu_k \rightarrow \bar{x}$.

The DM's goal of sampling signals is to maximize her information gain.
The information gain is defined as the KL divergence from the prior
$U_0$ to the posterior $U_k$. In intuition, the KL divergence provides a
measure for distance between distributions. As the DM acquires more
information about $s_{0\rightarrow T}$, her posterior belief should move
farther away from the prior. We let $p_0(U)$ and $p_k(U)$ denote the
probability density functions of $U_0$ and $U_k$. Then, the information
gain is$$\tag{2}
\begin{aligned}
D_{KL}(U_k||U_0)&=\int_{-\infty}^{\infty} p_k(U) \log\left(p_k(U)/p_0(U)\right)dU \\
&=\frac{\sigma_k^2+\mu_k^2}{2\sigma^2} - \log\left(\frac{\sigma_k}{\sigma}\right)-\frac{1}{2}
\end{aligned}
$$Notably, in Equation (2), $\sigma_k$ depends only on sample size $k$
and $\mu_k$ is proportional to $\bar{x}$. Therefore, the problem of
maximizing $D_{KL}(U_k||U_0)$ could be reduced to maximizing $\bar{x}$
(as each $u(s_t)$ is non-negative). The reason is that, drawing more
samples can always increase the precision of the DM's estimate about
$U(s_{0\rightarrow T})$, and a larger $\bar{x}$ implies more "surprises"
in comparison to the DM's initial perception that $s_{0\rightarrow T}$
contains no value.

Maximizing the mean sample signal $\bar{x}$ under a limited sample size
$k$ is actually a multi-armed bandit problem
\citep[][Ch.2]{sutton2018reinforcement}. On the one hand, the DM wants
to draw more samples at information sources that are known to produce
greater value signals (exploit). On the other hand, she wants to learn
some value information from other information sources (explore). We
assume the DM would take a softmax exploration strategy to solve this
problem. That is,$$
w_t \propto d_t e^{\bar{x}_t/\lambda}
$$where $\bar{x}_t$ is the mean sample signal generated by information
source $s_t$ so far, $1/\lambda$ is the learning rate, and $d_t$ is the
initial sampling weight for $s_t$.[^3] Note $\bar{x_t}$ cannot be
calculated without doing simulations under a certain $\sigma_\epsilon$.
For researchers, modelling an intertemporal choice in this way requires
conducting a series of simulations and then calibrating
$\sigma_\epsilon$ for every choiceable option, which could be
computationally expensive. Fortunately, according to the weak law of
large numbers, as the sample size $k$ gets larger, $\bar{x}_t$ is more
likely to fall into a neighborhood of $u(s_t)$. Thus, the AAD which
assumes $w_t \propto d_t e^{u(s_t)/\lambda}$ could be viewed as a proper
approximation to the softmax exploration strategy.

[^3]: The softmax strategy is popular in reinforcement learning. Classic
    softmax strategy assumes the initial probability of taking an action
    follows an uniform distribution. We relax this assumption by
    importing $d_t$, so that the DM can hold an (initial) preference
    over the dated rewards.

Those who familiar with reinforcement learning algorithms may notice
that here $u(s_t)$ is a special case of action-value function (assuming
that the learner only cares about the value of current reward in each
draw of sample). The AAD thus can be viewed as a specific version of the
soft Q-learning or policy gradient method for solving the given
multi-armed bandit problem [@haarnoja2017reinforcement;
@schulman2017equivalence]. Such methods are widely used (and
sample-efficient) in reinforcement learning. Moreover, one may argue
that the applicability of softmax exploration strategy is subject to our
model assumptions. Under alternative assumptions, the strategy may not
be ideal. We acknowledge this limitation and suggest that researchers
interested in modifying our model consider different objective functions
or different families of noises. For example, if the DM aims to minimize
the regret rather than maximizing $\bar{x}$, the softmax exploration
strategy can produce suboptimal actions and one remedy is to use the
Gumbel--softmax strategy [@cesa2017boltzmann]. If noises
$\epsilon_0,...,\epsilon_T$ do not follow an i.i.d normal distribution,
the information gain $D_{KL}(U_k||U_0)$ may be complex to compute, thus
one can use its variational bound as the objective [@houthooft2016vime].
Compared to these complex settings, the model setting in this subsection
aims to provides a simple benchmark for understanding the role of
attention in mental valuation of a reward sequence.

Two strands of literature can justify the information maximizing
approach to characterizing AAD. First,

Second, the softmax exploration strategy is widely used by
neuroscientists in fitting human actions in reinforcement learning tasks
[@daw2006cortical; @niv2012neural; @fitzgerald2012action;
@niv2015reinforcement; @leong2017dynamic]. For instance,
@daw2006cortical find the softmax strategy can characterize humans'
exploration behavior better than other classic strategies (e.g.
$\epsilon$-greedy). Besides, @collins2014opponent show that models based
on this strategy exhibit a good performance in explaining behaviors of
the striatal dopaminergic system (which is central in brain's sensation
of pleasure and learning of rewarding actions) in reinforcement
learning.

## Optimal Discounting \label{optimal_discount}

The second approach to characterize AAD is based on the optimal
discounting model \citep{noor2022optimal,noor2024constrained}. In one
version of this model, the authors assume that the DM has a limited
capacity of attention (or in their term, "empathy"), and before
evaluating a reward sequence $s_{0\rightarrow T}$, she naturally focuses
on the current period. The instantaneous utility $u(s_t)$ represents the
well-being that the DM's self of period $t$ can obtain from the reward
sequence. For valuating $s_{0\rightarrow T}$, the DM needs to split
attention over $T$ time periods to consider the feeling of each self.
This re-allocation of attention is cognitive costly. The DM seeks to
find a balance between improving the overall well-being of multiple
selves and reducing the incurred cognitive cost.
\citet{noor2022optimal,noor2024constrained} specify an optimization
problem to capture this decision. In this paper, we adopt a variant of
their original model. The formal definition of the optimal discounting
problem is given by Definition 2. [^4]

[^4]: There are three differences between our Definition 2 and the
    original optimal discounting model
    \citep{noor2022optimal,noor2024constrained}. First, in our setting,
    shifting attention to future rewards may reduce the attention to the
    current reward, while this would never happen in
    \citet{noor2022optimal,noor2024constrained}. Second, the original
    model assumes $f'(w_t)$ must be continuous at 0 and $w_t$ must be no
    larger than 1. We relax these assumptions since $w_t=0$ or
    $w_t\geq1$ is not in our solutions. Third, the original model
    assumes that $f'(w_t)$\$ is left-continuous and $f'(w_t)=0$ when
    $w_t$ is under a lower bound $\underline{w}$, $f'(w_t)=\infty$ when
    $w_t$ is above a upper bound $\bar{w}$, and $f'(w_t)$ is strictly
    increasing when $w_t \in [\underline{w},\bar{w}]$. To keep
    simplicity, this paper restricts $f'(w_t)$ to be continuous and
    strictly increasing. Our assumption can apply to many commonly used
    cost functions, including the power cost function discussed in
    \citet{noor2022optimal,noor2024constrained}.

\noindent \textbf{Definition 2}: *Given reward sequence*
$s_{0\rightarrow T}=[s_0,...,s_T]$*, the following optimization problem
is called an optimal discounting problem for* $s_{0\rightarrow T}$*:*$$
\begin{aligned}
&\max_{\mathcal{W}}\;&&\sum_{t=0}^T w_tu(s_t) - C(\mathcal{W}) \\
&s.t.\; &&\sum_{t=0}^Tw_t \leq M \\
&&& w_t \geq 0 \text{ for all } t\in \{0,1,...,T\}
\end{aligned}
$$*where* $M>0$*,* $C(\mathcal{W})\geq 0$. For any $t\in\{0,1,…,T\}$,
$u(s_t)\in\mathbb{R}$. $C(\mathcal{W})$ *is the cognitive cost function
and is constituted by time-separable costs, i.e.*
$C(\mathcal{W})=\sum_{t=0}^Tf_t(w_t)$*, where for all* $w_t\in(0,1)$*,*
$f_t(w_t)$ *is differentiable and* $f'_t(w_t)$ *is continuous and*
stric*tly increasing.*

Here $w_t$ reflects the attention paid to consider the feeling of
$t$-period self. The DM's objective function is the attention-weighted
sum of utilities obtained by the multiple selves minus the cognitive
cost of attention re-allocation. As is illustrated by
\citet{noor2022optimal,noor2024constrained}, a key feature of this model
is that decision weight $w_t$ is increasing with $s_t$, indicating the
DM tends to pay more attention to larger rewards. Moreover, it is easy
to validate that if the following three conditions are satisfied, the
solution to the optimal discounting problem will take an AAD form:

(i) The constraint on sum of decision weights is always tight. That is,
    $\sum_{t=0}^Tw_t=M$. Without loss of generality, we can set $M=1$.

(ii) There exists a realization of decision weights
     $\mathcal{D}=[d_0,...,d_T]$ such that $d_t>0$ for all
     $t\in\{0,…,T\}$ and the cognitive cost is proportional to the KL
     divergence from $\mathcal{D}$ to the DM's strategy $\mathcal{W}$
     where applicable. That is,
     $C(\mathcal{W})= \lambda\cdot D_{KL}(\mathcal{W}||\mathcal{D})$,
     where $\lambda>0$.

Here $d_t$ sets a reference for determining the decision weight $w_t$,
the parameter $\lambda$ indicates how costly the attention re-allocation
process is, and
$D_{KL}(\mathcal{W}||\mathcal{D})=\sum_{t=0}^Tw_t\log(\frac{w_t}{d_t})$.
The solution to the optimal discounting problem under condition (i)-(ii)
can be derived in the same way as Theorem 1 in @matvejka2015rational.
Note this solution is equivalent to that of a bounded rationality model:
assuming the DM wants to find a $\mathcal{W}$ that maximizes
$\sum_{t=0}^Tw_tu(s_t)$ but can only search for solutions within a KL
neighborhood of $\mathcal{D}$. Related models can be found in
@todorov2009efficient.

We interpret the implications of condition (i)-(ii) with behavioral
axioms. Note if each $s_t$ is an independent option and $\mathcal{W}$
simply represents the DM's choice strategy across options, then these
condition can be characterized by rational inattention theories, e.g.
@caplin2022rationally. However, here $\mathcal{W}$ is a component of
sequence value $U(s_{0\rightarrow T})$, and the DM is assumed to choose
the option with highest sequence value. Thus, the behavioral
implications of condition (i)-(ii) should be derived in different
ways*.* To illustrate, let $\succsim$ denote the preference relation
between two reward sequences.[^5] For any reward sequence
$s_{0\rightarrow T}=[s_0,...,s_T]$, we define
$s_{0\rightarrow t}=[s_0,...,s_t]$ as a sub-sequence of it, where
$1\leq t\leq T$.[^6] We first introduce two axioms for $\succsim$:

[^5]: If $a \succsim b$ and $b\succsim a$, we say $a\sim b$ ("$a$ is the
    same good as $b$"). If $a \succsim b$ does not hold, we say
    $b\succ a$ ("$b$ is better than $a$"). $\succsim$ can also
    characterize the preference relation between single rewards as the
    single rewards can be viewed as one-period sequences.

[^6]: Notably, every sub-sequence starts with period 0.

\noindent \textbf{Axiom 1}: $\succsim$ *has the following properties:*

(a) *(complete order)* $\succsim$ *is complete and transitive.*

(b) *(continuity) For any reward sequences* $s,s'$ *and reward*
    $c\in \mathbb{R}_{\geq 0}$*, the sets*
    $\{\alpha \in(0,1) | \alpha\cdot s + (1-\alpha)\cdot c \succsim s'\}$
    *and*
    $\{\alpha \in(0,1) | s' \succsim \alpha\cdot s + (1-\alpha)\cdot c \}$
    *are closed.*

(c) *(state-independent) For any reward sequences* $s,s'$ *and reward*
    $c\in \mathbb{R}_{\geq 0}$*,* $s \succsim s'$ *implies for any*
    $\alpha \in (0,1)$*,*
    $\alpha\cdot s + (1-\alpha)\cdot c \sim \alpha \cdot s' + (1-\alpha) \cdot c$*.*

(d) *(reduction of compound alternatives) For any reward sequences*
    $s,s',y$ *and rewards* $c_1,c_2\in \mathbb{R}_{\geq 0}$*, if there
    exist* $\alpha, \beta \in (0,1)$ *such that*
    $s \sim \alpha \cdot y + (1-\alpha) \cdot c_1$*, then*
    $s' \sim \beta \cdot s + (1-\beta)\cdot c_2$ *implies*
    $s' \sim \beta\alpha\cdot y+\beta(1-\alpha)\cdot c_1 + (1-\beta)\cdot c_2$*.*

\noindent \textbf{Axiom 2}: *For any* $s_{0\rightarrow T}$ *and any*
$\alpha_1,\alpha_2 \in (0,1)$*, there exists* $c\in \mathbb{R}_{\geq 0}$
*such that*
$\alpha_1 \cdot s_{0\rightarrow T-1}+\alpha_2\cdot s_T \sim c$*.*

The two axioms are almost standard in decision theories. The assumption
of complete order implies preferences between reward sequences can be
characterized by an utility function. Continuity and state-independence
ensure that in a stochastic setting where the DM can receive one reward
sequence under some states and receive a single reward under other
states, her preference can be characterized by expected utility
[@herstein1953axiomatic]. Reduction of compound alternatives ensures
that the DM's valuation on a reward sequence is constant across states.
Axiom 2 is an extension of the Constant-Equivalence assumption in
@bleichrodt2008koopmans. It implies there always exists a constant that
can represent the value of a linear combination of sub-sequence
$s_{0\rightarrow T}$ and the end-period reward $s_T$, as long as the
weights lie in $(0,1)$.

For a given $s_{0\rightarrow T}$, the optimal discounting model can
generate a sequence of decision weights $[w_0,...,w_T]$. Furthermore,
the model assumes the DM's preference for $s_{0\rightarrow T}$ can be
characterized by the preference for
$w_0\cdot s_0+w_1\cdot s_1 +...+w_T\cdot s_T$. We use Definition 3 to
capture this assumption.[^7]

[^7]: In \citep{noor2022optimal}, it is claimed that $\succsim$
    satisfying Definition 3 has a Costly Empathy representation.

\noindent \textbf{Definition 3}: *Given reward sequence*
$s_{0\rightarrow T}=[s_0,...,s_T]$ *and*
$s'_{0\rightarrow T'}=[s'_0,...,s'_{T'}]$*, the preference relation*
$\succsim$ *has an optimal discounting representation if* $$
s_{0\rightarrow T} \succsim s'_{0\rightarrow T'}\quad
\Longleftrightarrow \quad \sum_{t=0}^T w_t\cdot s_t
\succsim \sum_{t=0}^{T'} w'_t \cdot s'_t
$$ *where* $\{w_t\}_{t=0}^T$ *and* $\{w'_t\}^{T'}_{t=0}$ *are solutions
to the optimal discounting problems for* $s_{0\rightarrow T}$ *and*
$s'_{0\rightarrow T'}$ *respectively.*

Furthermore, if Definition 3 is satisfied and $\{w_t\}_{t=0}^T$ as well
as $\{w'_t\}^{T'}_{t=0}$ takes the AAD form, we say $\succsim$ has an
*AAD representation*. Now we specify two behavioral axioms that are key
to characterize the AAD functions.

\noindent \textbf{Axiom 3} (sequential outcome-betweenness): *For any*
$s_{0\rightarrow T}$*, there exists* $\alpha\in(0,1)$ *such that*
$s_{0\rightarrow T} \sim \alpha\cdot s_{0\rightarrow T-1}+(1-\alpha) \cdot s_T$*.*

\noindent \textbf{Axiom 4} (sequential bracket-independence): *Suppose*
$T\geq 2$. *For any* $s_{0\rightarrow T}$*, if there exist*
$\alpha_1,\alpha_2,\beta_0,\beta_1,\beta_2\in(0,1)$ *such that*
$s_{0\rightarrow T}\sim \alpha_1 \cdot s_{0\rightarrow T-1} + \alpha_2 \cdot s_{T}$
*and*
$s_{0\rightarrow T}\sim \beta_0 \cdot s_{0\rightarrow T-2}+\beta_1 \cdot s_{T-1}+\beta_2 \cdot s_{T}$*,
then we must have* $\alpha_2 = \beta_2$*.*

Axiom 3 implies that for a reward sequence $s_{0\rightarrow T-1}$, if we
add a new reward $s_T$ at the end of the sequence, then the value of the
new sequence should lie between the original sequence
$s_{0\rightarrow T-1}$ and the newly added reward $s_T$. This
characterizes condition (i), i.e. the sum of decision weights is bounded
tightly at 1. Notably, Axiom 3 is consistent with the empirical evidence
about *violation of dominance* [@scholten2014better; @jiang2017better]
in intertemporal choice. Suppose the DM is indifferent between a
small-sooner reward (SS) "receive \$75 today" and a large-later reward
(LL) "receive \$100 in 52 weeks". @scholten2014better find when we add a
tiny reward after the payment in SS, e.g. changing SS to "receive \$75
today and \$5 in 52 weeks", the DM would be more likely to prefer LL
over SS. @jiang2017better find the same effect can apply to LL. That is,
if we add a tiny reward after the payment in LL, e.g. changing LL to
"receive \$100 in 52 weeks and \$5 in 53 weeks", the DM may be more
likely to prefer SS over LL.

Axiom 4 implies that no matter how the DM brackets the rewards into
sub-sequences (or how the sub-sequences get further decomposed), the
decision weights for rewards outside them should not be affected.
Specifically, suppose we decompose reward sequence $s_{0\rightarrow T}$
and find its value is equivalent to a linear combination of
$s_{0\rightarrow T-1}$ and $s_T$. We also can further decompose
$s_{0\rightarrow T-1}$ to a linear combination of $s_{0\rightarrow T-2}$
and $s_{T-1}$. But no matter how we operate, as long as the
decomposition is carried out inside $s_{0\rightarrow T-1}$, the weight
of $s_T$ in the valuation of $s_{0\rightarrow T}$ will always remain the
same. This axiom is an analog to independence of irrelevant alternatives
in discrete choice problems (which is a key feature of softmax choice
function). We show in Proposition 1 that the optimal discounting model
plus Axiom 1-4 can exactly produce AAD.

\noindent \textbf{Proposition 1}: *Suppose* $\succsim$ *has an optimal
discounting representation, then it has an AAD representation if and
only if it satisfies Axiom 1-4.*

The proof of Proposition 1 is in Appendix A.

# Implications for Decision Making

hidden-zero effect

common-difference effect

concavity of discount function

S-shaped value function

Intertemporal correlation aversion

Learning and inconsistent planning

```{r child='Implications.Rmd'}
```

# Discussion

## Relation to Other Models of Intertemporal Choices

The theory most similar to AAD is the salience theory
[@bordalo2012salience; @bordalo2013salience; @bordalo2020memory].

rational inattention

focus-weighted utility

bayesian updating and discounting

optimal precision

Relation with money/delay trade-off

## Other relevant phenomena

## Limitation

attention biases learning: learning rate is high for attended reward

# Conclusion

# Reference
