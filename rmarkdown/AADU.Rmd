---
title: "Attention-Adjusted Discounted Utility"
author: "Zijian Zark Wang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: reference.bib
biblio-style: apalike
header-includes: 
  \usepackage{setspace}
  \usepackage{amsmath}
  \usepackage{array}
  \usepackage{caption}
  \usepackage{longtable}
  \usepackage{booktabs}
  \renewcommand{\arraystretch}{1}
  \captionsetup[table]{skip=5pt}
  \setstretch{1.5} 
fontsize: 12pt
geometry: margin=1in
editor_options: 
  markdown: 
    wrap: 72
output:
  #word_document:
    #number_sections: true
  pdf_document:
    number_sections: true
    citation_package: natbib
    keep_tex: true
  html_document:
    toc: true
    number_sections: true
---

# Introduction

It has been a long tradition that researchers in economics and
behavioral sciences use the discounted utility framework for modelling
intertemporal choices

I develop a novel model of intertemporal choice, which I term
"*attention-adjusted discounted utility*" (ADU). I postulate that the
overall utility a decision maker can obtain from a reward sequence is
decided by the weighted sum of utilities that can be obtained in each
time period. The initial weight allocation is exponential, i.e. the
decision maker is initially time-stationary. However, when evaluating
the given reward sequence, she tends to assign more weights (pay more
attention) to the time periods with larger rewards, in order to
subjectively maximize her overall utility. This attention adjustment
process incurs a cognitive cost; and the more the weight allocation
deviates from the initial allocation, the greater the cost is. The
decision maker optimally re-allocates the weights across time periods. I
term the discounting factors that can be represented by such weights as
*attention-adjusted* discounting factors.

In this paper, I show that a set of intertemporal choice anomalies can
be attributed to such attention adjustment processes (that is, can be
explained by ADU to some extent), including common difference effect and
magnitude effect [@loewenstein_anomalies_1992], risk aversion over time
lotteries [@onay_intertemporal_2007; @dejarnette_time_2020],
non-additive time intervals [@read_is_2001; @scholten_discounting_2006],
intertemporal correlation aversion [@andersen_multiattribute_2018], and
dynamic inconsistency. The model can also offer insights on the
preferences for sequences of outcomes [@loewenstein_preferences_1993]
and the formation of reference-dependent preferences
[@koszegi_model_2006]. In an empirical test, I find ADU outperforms a
set of time discounting models in predicting human intertemporal
choices. Therefore, I think there is a need to rethink the foundation of
many behavioral phenomena.

The remaining part of this document is organized as follows. Section
\ref{model} outlines the model of attention-adjusted discounted utility
(ADU). Section \ref{behavioral} explains how the model can help explain
some empirical findings in intertemporal choice.

# The Model \label{model}

## Rational Inattention and Time Discounting

Consider a reward sequence $x = [x_0,x_1,...,x_T]$ that yields reward
$x_t$ in time period $t$. The time length of this sequence, denoted by
$T$, is finite. For any $t \in \{0,1,...,T\}$, the reward level $x_t$ is
a random variable defined on $R_{+}$. The support of $x$ is $X$, which
is a subset of $R_{+}^T$.

Suppose a decision maker evaluates reward sequence $x$ by three steps:
At first, she randomly draws some potential realizations of $x$ from
$X$. Then, from each drawn realization of $x$, she draws some time
periods at random, taking the rewards of these periods into a sample.
Finally, she uses the mean utility of sampled rewards as a value
representation of $x$. Let $s=[s_0,s_1,...,s_T]$ be a potentially
realized outcome of $x$ and $p(s)$ be the probability that $s$ is drawn.
I use $w(.)$ and $u(.)$ to denote the decision maker's weight function
and utility function, where $w(s_t)$ is the probability that the reward
of the $t$-th period in a potentially realized sequence $s$ is sampled,
$u(s_t)$ is the utility obtained by reward $s_t$
($t \in \{0,1,...,T\}$), $u'>0$, $u''<0$.

The sampling process is sequential, and the decision maker wants to find
a sampling strategy, denoted by function $w(.)$, that maximizes her
overall utility. In a given potentially realized sequence $s$, the
periods with larger reward levels should be sampled more frequently.
However, at the very beginning, the decision maker has no information
about which period in $s$ has a larger reward -- she learns such
information gradually in the process of sampling. This learning process
triggers a cognitive cost. Hence, her overall utility is the mean
utility of sampled rewards minus the cognitive cost of learning.

Suppose when having no information, the weight on period $t$ across each
potentially realized sequence is equal ($\equiv w^0_t$). Let $W$ and $P$
be the minimal sets that contain all available function $w$ and $p$
respectively. We can use an optimization problem to represent the
described evaluation procedure: $$ 
\begin{aligned}
\max_{w\in W}  \quad & \sum_{s\in X}\sum_{t=0}^T w(s_t)u(s_t) - C(w;\theta) \\
s.t. \quad &  \sum_{s\in X}\sum_{t=0}^T w(s_t)=1 \\
& w(s_t)>0, \forall s\in X,t=0,1,â€¦,T \\
\end{aligned}
$$where $C(.)$ is a cognitive cost function with $\theta$ as its
parameters. To solve this optimization problem, I add two additional
assumptions. The first is that the weight updating process is consistent
with Bayes rule, that is, $w^0_t=\sum_{s\in X} w(s_t)$. The second is
that the cognitive cost function takes a form similar to Shannon mutual
information, that is$$
C(\textbf{w};\theta)= \lambda \sum_{s\in X}\sum_{t=0}^T w(s_t) \log\left(\frac{w(s_t)}{p(s)w_t^0}\right)
$$where $p(s)w^0_t$ is the probability of $s_t$ being sampled when no
information is learned, $w(s_t)$ is the probability of that after
learning the information about $x$. Shannon mutual information
quantifies the amount of information gain when learning about which time
period has a larger reward in any initially unknown $s$. Consistent with
@matejka_rational_2015, I set $C(\textbf{w};\theta)$ linear to that.
Parameter $\lambda$ denotes unit cost of information ($\lambda>0$).

Define $w(s_t|s) = \frac{w(s_t)}{p(s)}$. As is shown in
@matejka_rational_2015, the optimization problem can be easily solved by
Lagrangain method. The solution is$$ \tag{1}
w(s_t|s) =\frac{w_t^0e^{u(s_t)/\lambda}}{\sum_{t=0}^T w_\tau^0 e^{u(s_t)/\lambda}}
$$Note $w(s_t|s)$ reveals how the decision maker weights the utility of
time period $t$ in a drawn sequence $s$. It can naturally represent the
discounting factor. $w(s_t|s)$ is increasing in $s_t$, which implies the
decision maker exhibit more patience for a larger reward.

While building the model, I was mainly inspired by the theories of
rational inattention [@matejka_rational_2015; @jung_discrete_2019;
@mackowiak_rational_2023]. In @matejka_rational_2015's theory of
rational inattention, the decision maker makes choices between discrete
alternatives; she evaluates each alternative via a costly information
acquisition process, then decides the optimal choice strategy. The
theory deduces the probability of each alternative being chosen should
follow a logistic-like distribution. In ADU, I assume the discounting
factors are generated by a similar process; hence, she subjectively
weights each time period according to a logistic-like distribution -- as
Equation (1) does -- as well.

The reason why I use Shannon mutual information as the cognitive cost
function is twofold. First, note that
$w(s_t|s) \propto w^0_t e^{u(s_t)/\lambda}$. Given a certain stream $s$
and two time periods $t_1$ and $t_2$ ($t_2>t_1$), the relative weight
between them $\frac{w(s_{t_1}|s)}{w(s_{t_2}|s)}$ is only relevant to
$s_{t_1}$ and $s_{t_2}$. Therefore, changing the reward of a third
period has no impact on how the reward in $t_2$ should be discounted
relative to that in $t_1$. Second, under such settings, the objective
function can be rewritten as$$
\sum_{s\in X} p(s)[w(s_t|s)u(s_t) - \lambda D_{KL}]
$$

where $D_{KL}$ is the KL divergence between the initial weights over
time periods and the weights updated given the stream $s$ is drawn.
Clearly, the determination of $w(s_t|s)$ in each $s$ can be separated
from each other. In other words, given two potentially realized streams
$s$ and $s'$, the changes in $s'$ has no impact on the determination of
discounting factors in $s$. This property is consistent with many forms
of optimal sequential learning (for example, @zhong_optimal_2022 ).
@matejka_rational_2015 and @caplin_rationally_2022 show that the two
properties are jointly satisfied if and only if the solution of
$w(s_t|s)$ follows Equation (1).

## Axiomatic Characterization

## Related Literature

In addition to ADU, there are other models that attempt to incorporate
attention mechanism into the formation of time preferences. For example,
@steiner_rational_2017 consider a decision maker adjusting the belief
$p(s)$ over time but holding the discounting factor $w(s_t|s)$ constant.
In each time period, given that her ability to learn new information is
limited, the updated belief cannot deviate from that in a previous
period by too much, which causes behavioral inertia. Instead, ADU
assumes the decision maker re-allocates $w(s_t|s)$ each time period.
Thus, the process of attention adjustment not only affects dynamic
decision-making but also affects the choices in "Money Earlier or Later"
(MEL) tasks. Besides, @gabaix_myopia_2017 assume the perception of
future rewards is noisy and the decision maker infers the value of them
by sampling from normal distributions; @gershman_rationally_2020 allow
the decision maker optimally chooses sample variance to minimize the
mean sample squared error. Such theories, together with a certain
specification on rate-distortion function, can lead to
magnitude-increasing patience and hyperbolic-like discounting.
Discounting factors in this style can be viewed as a special case of
those in ADU. @noor_optimal_2022, @noor_constrained_2023 construct an
optimization problem similar to ADU. However, they use a different
cognitive cost function. I compare the performance of ADU with models of
@gershman_rationally_2020, @noor_optimal_2022 and some other papers in
predicting human choices in MEL tasks.

# Explaining Non-Stationary Time Preferences \label{behavioral}

## Evaluating A Delayed Reward

Suppose there is a reward sequence with a time length of $T$. A decision
maker receives a positive detereminstic reward in time period $j$ (and
no reward in other periods), that is, for any $t\in[0,1,â€¦,T]$ and
$t \neq j$, $x_t = 0$. The decision maker evaluates the reward sequence
by implementing the ADU evaluation procedure. For simplicity, I set
$v(0)=0$. I also set the decision maker initially holds stationary time
preferences, i.e. $w^0_t=\delta^t$, where $\delta\in(0,1]$. When
$\delta=1$, we say that the initial attention is uniformly distributed
across time periods. Given the reward is detereminsic, one can omit $s$
in $w(s_t|s)$ and directly represent the weight on each time period $t$
by $w_t$.

$$
w_j = \left\{ \begin{aligned}
& \delta^j \cdot\frac{1}{1+\frac{\delta}{1-\delta}(1-\delta^T)e^{-v(x_j)}}\;, & 0<\delta<1 \\
& \frac{1}{1+T\cdot e^{-v(x_j)}}\; , & \delta=1
\end{aligned}
\right.
$$

When $\delta=1$ and $j=T$, the attention weight $w_j$ takes a form
similar to hyperbolic discounting.
$w_T = (1+(1-\delta)^{-1}(\delta^{-T}-1)e^{u(x_T)})^{-1}$

the hidden zero effect

## Common Difference Effect

Suppose there are a large later reward $x_l$ arriving at period $t_l$
(denoted by LL) and a small sooner reward $x_s$ arriving at period $t_s$
(denoted by SS), where $x_l>x_s>0$, $t_l>t_s>0$. Assuming
$w_{t_l}(x_l)u(x_l)=w_{t_s}(x_s)u(x_s)$, common difference effect
implies $w_{t_l+\Delta t}(x_l)u(x_l)>w_{t_s+\Delta t}(x_s)u(x_s)$ for
any positive integer $\Delta t$, magnitude effect implies
$w_{t_l}(x_l)u(x_l+\Delta x)>w_{t_s}(x_s)u(x_s+\Delta x)$ for any
positive real number $\Delta x$ [@loewenstein_anomalies_1992].

Proposition 1: The common difference effect

(1) holds when
$v(x_l)-v(x_s)+\ln\frac{u(x_l)}{u(x_s)}>-(t_l-t_s)\ln\delta$, if
$T_l = t_l$ and $T_s=t_s$

(2) always holds, if $T_s = T_l$.

When $\delta = 1$, ADU predicts that the decision maker always performs
common difference effect. This is obvious because the discounting factor
$w_T$ takes a hyperbolic-like form. When $\delta<1$, the decision maker
performs common difference effect only when the difference between $x_l$
and $x_s$ are much larger than the difference between $t_l$ and $t_s$.

The ADU's prediction on common difference effect can be understood as
follows. Note that $w_t \propto \delta^t e^{u(x_t)/\lambda}$. If we omit
the constraint that the sum of weights on each time period is fixed
(i.e. attention is limited), then
$w_{t_l+\Delta t}(x_l) = \delta^{\Delta t} \cdot w_{t_l}$ and the same
can be applied to $w_{t_s+\Delta t}$. Thus,
$w_{t_l+\Delta t} / w_{t_s+\Delta t}$ keeps constant for any $\Delta t$.
However, given the decision maker's attention is limited, the change
from $w_{t_l}$ to $w_{t_l+\Delta t}$ is not only driven by
$\delta^{\Delta t}$, but also driven by the effect that the final
period, with a positive reward, can naturally grab attention from the
previous periods which has no reward. Since $x_l > x_s$, this
attention-grabbing effect is greater for LL than for SS. Meanwhile, when
extending the time length, the average attention that can be allocated
to each period should shrink. The decision makers performs common
difference effect only when the former effect exceeds the latter effect.

## Magnitude Effect

Proposition 2: The magnitude effect holds when

$$
RRA_u(x)-\alpha\frac{\partial \epsilon_u (x)}{\partial u} <1
$$

Corollary 1: Suppose $u(x)=x^\gamma/\lambda$, where $\gamma>0$ and
$\lambda>0$. Then magnitude effect always holds if and only if
$0<\gamma<1$.

## Reference-Dependent Preferences

ADU predicts that the larger the unit cost of information $\lambda$ or
the smaller the magnitude of $x_l$ and $x_s$ is, the more likely it is
that the decision maker performs magnitude effect.

First, note that the magnitude effect requires the decision maker's
overall utility $w_T(x_T)u(x_T)$ to be a convex function of $x_T$. Given
that $u(.)$ is concave, whether the magnitude effect holds should depend
on $w_T$. Then, set $z = u(x_T)-\lambda\log G(T)$. We can rewrite
Equation (2) as a logistic function of $z$, i.e.
$w_T = 1/(1+e^{-z/\lambda})$. By the shape of logistic function, $w_T$
is convex in $u(x_T)$ if and only if $u(x_T)<\lambda \log G(T)$ (that
is, when $x_T$ is small relative to $T$ or when $\lambda$ is large).
Finally, it is notable that the given condition is necessary but not
sufficient to yield magnitude effect.

In summary, holding the others equal, the decision makers' overall
utility can be convex in a future reward when the level of it is under a
certain threshold, and be concave when it is above the threshold. This
is also consistent to the theories about reference-dependent preferences
[@koszegi_model_2006].

## Non-Additive Time Intervals

Both exponential and hyperbolic discounting models predict the decision
maker is risk seeking over time lotteries. That is, suppose a
deterministic reward of level $c$ ($c>0$) is delivered in period $t_s$
with probability $\pi$ and is delivered in period $t_l$ with probability
$1-\pi$ ($0<\pi<1$); another deterministic reward, of the same level, is
delivered in a certain period $\pi t_s +(1-\pi) t_l$. The decision maker
should prefer the former case to the latter case. However,
@onay_intertemporal_2007 find in experiments that people are only risk
seeking over time lotteries when $\pi$ is small and are risk averse over
time lotteries when $\pi$ is large. This finding can be explained by the
convexity of $w_T$.

Let $t_m = \pi t_s +(1-\pi) t_l$. By definition, the decision makers are
risk seeking over time lotteries when
$\pi w_{t_s}(c)+(1-\pi)w_{t_l}(c)>w_{t_m}(c)$. First, note the LHS
equals to the RHS when $\pi=0$ or $\pi=1$. Fixing $t_s$ and $t_l$, the
inequality implies $w_{t_m}(c)$ is convex in $t_m$. Second, it can be
proved that $w_T(c)$ is convex in $T$ if and only if $T$ is above a
certain threshold. This is also consistent with
@takeuchi_non-parametric_2011 that suggests the discount function should
be inverse S-shaped with respect to time. By contrast, in many models
such as exponential and hyperbolic discounting, discounting factors are
typically decided by a convex function of $T$. Third, note $t_m$ is
linearly decreasing with $\pi$, thus the decision maker is more likely
to be risk seeking over time lotteries when $\pi$ is small. The same can
be applied to the risk aversion case.

Now consider $T$ is small enough to make $w_T$ concave in $T$. In this
case, adding an extension to $T$ will increase the rate at which $w_T$
declines with $T$ -- this property is termed "super-additive time
intervals" by @read_is_2001. Moreover, ADU predicts intervals are
sub-additive when the total time length $T$ is large, and are
super-additive when $T$ is small, which is consistent with
@scholten_discounting_2006.

## Non-Separable Sequences

Let $x$ and $y$ denote two 2-period risky reward sequences. For $x$, the
realized sequence is [Â£100,Â£100] with probability 1/2, and is [Â£3,Â£3]
with probability 1/2. For $y$, the realized sequence is [Â£3,Â£100] with
probability 1/2, and is [Â£100,Â£3] with probability 1/2. Classical models
of intertemporal choice, such as @fishburn_time_1982, typically assume
the separability of potentially realized sequences. This implies that
the decision maker is indifferent between $x$ and $y$. However,
@andersen_multiattribute_2018 find evidence of intertemporal correlation
aversion, that is, people often prefer $y$ to $x$. Such a property is
also termed "weak separability" in @noor_constrained_2023.

ADU can naturally yield intertemporal correlation aversion. For
simplicity, suppose the initial attention is uniformly distributed
across the two periods. For $x$, under each potentially realized
sequence, the decision maker equally weights each period. For $y$,
decision maker tends to assign more weight to the period with a reward
of Â£100 (suppose that weight is $w$). Then the value of $x$ is
$\frac{1}{2} u(100) + \frac{1}{2} u(3)$ and the value of $y$ is
$w\cdot u(100) +(1-w) \cdot u(3)$. Given that $x>\frac{1}{2}$, the
decision makers should strictly prefer $y$ to $x$.

# Explaining Dynamic Inconsistent Behavior

Suppose a decision maker has budget $m$ ($m>0$) and is considering how
to spend it over different time periods. We can use a reward sequence
$x$ to represent this decision problem, where the decision maker's
spending in period $t$ is $x_t$. In period 0, she wants to find a $x$
such that$$ \tag{3}
\max_{x}\;\sum_{t=0}^T w_t u(x_t)\quad s.t. \;\sum_{t=0}^T x_t = m  
$$

where $w_t$ is the attention-adjusted discounting factor in period $t$.
I assume
$w_t=\delta^t e^{u(x_t)/\lambda}/\sum_{t=\tau}^T \delta^{\tau} e^{u(x_\tau)/\lambda}$
and there is no risk under this setting.

In models like exponential and hyperbolic discounting, the discounting
factor of a future period is consistently smaller than that of the
current period. Thus, the decision maker should spend more at the
present than in the future. By contrast, in ADU, when increasing the
spending in a certain period, the discounting factor corresponding to
that period should also increase. So it is possible that the decision
maker spends more in the future and that a future period has a greater
discounting factor than the current period. This is consistent with
@loewenstein_preferences_1993 that find people sometimes prefer
improving sequences to declining sequences.

ADU suggests there are two mechanisms that can help explain why people
may perform dynamically inconsistent behavior. The first is
*attention-grabbing effect*, that is, keeping the others equal, when we
increase $x_t$ (which lead to an increase in $w_t$), the discounting
factor in any other period should decrease due to limited attention.
After omitting a previous period from the decision problem in Equation
(3), the decision maker can assign more weights to remaining periods;
thus, the attention-grabbing effect is enhanced. The increased
attention-grabbing effect will offset some benefit of increasing
spending toward a certain period. Therefore, when the decision maker
prefers improving sequences, the attention-grabbing effect will make her
perform a present bias-like behavior (always feeling that she should
spend more at the present than the original plan); when the decision
maker prefers declining sequences, this effect will maker her perform a
future bias-like behavior (always feeling she should spend more in the
future).

The second mechanism is *initial attention updating*. As is assumed
above, in period 0, prior to evaluating each reward sequence, the
decision maker's initial weight on period $t$ is proportional to
$\delta^t$; after evaluation, the weight becomes being proportional to
$\delta^t e^{u(x_t)/\lambda}$. In period 1, if she implements the
evaluation based on the information attained in period 0, the initial
weight should be updated to being proportional
$\delta^t e^{u(x_t)/\lambda}$; thus, the weight after evaluation should
become being proportional to $\delta e^{2u(x_t)/\lambda}$. As a result,
the benefit of increasing spending toward a certain period gets
strengthened. The updated initial attention can make those who prefer
improving sequences perform present bias and those who prefer declining
sequences perform future bias.

Both the attention-grabbing effect and initial attention updating are
affected by the curvature of utility function. They jointly decide which
behavior pattern that people should perform in dynamics.

# Consumption Over-Smoothness and Over-Sensitivity

# Discussion

# Reference
