\newpage

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{a.-proof-of-proposition-1}{%
\subsection{A. Proof of Proposition 1}\label{a.-proof-of-proposition-1}}

The sufficiency is easy to validate. We present the proof of necessity
here. That is, if \(\succsim\) has an optimal discounting representation
and satisfies Axiom 1-4, then it has an AAD representation.

\noindent \textbf{Lemma 1}: \emph{If Axiom 1 and 3 hold, for any}
\(s_{0\rightarrow T}\)\emph{, there exist} \(w_0, w_1, …, w_T > 0\)
\emph{such that}
\(s_{0\rightarrow T} \sim w_0 \cdot s_0 + ...+w_T\cdot s_T\)\emph{,
where} \(\sum_{t=0}^T w_t=1\)\emph{.}

\noindent \emph{Proof}: If \(T=1\), Lemma 1 is a direct application of
Axiom 3. If \(T\geq 2\), for any \(2\leq t\leq T\), there should exist
\(\alpha_t\in(0,1)\) such that
\(s_{0\rightarrow t}\sim \alpha_t\cdot s_{0\rightarrow t-1}+(1-\alpha_t)\cdot s_{t}\).
By state-independence and reduction of compound alternatives, we can
recursively apply this equivalence relation as follows:\[
\begin{aligned}
s_{0\rightarrow T} &\sim \alpha_{T-1}\cdot s_{0\rightarrow T-1} + (1-\alpha_{T-1})\cdot s_T \\
&\sim  \alpha_{T-1}\alpha_{T-2}\cdot s_{0\rightarrow T-2} + \alpha_{T-1}(1-\alpha_{T-2})\cdot s_{T-1} + (1-\alpha_{T-1})\cdot s_T \\
& \sim ...\\
& \sim w_0 \cdot s_0 + w_1\cdot s_1 +... +w_T\cdot s_T
\end{aligned}
\]where \(w_0=\prod_{t=0}^{T-1}\alpha_t\), \(w_T = 1-\alpha_{T-1}\), and
for \(0<t<T\),
\(w_t=(1-\alpha_{t-1})\prod_{\tau=t}^{T-1}\alpha_{\tau}\). It is easy to
show the sum of \(w_0,…,w_T\) is equal to 1. \emph{QED}.

Therefore, if Axiom 1 and 3 hold, for any reward sequence
\(s_{0\rightarrow T}\), we can always find a convex combination of all
its elements, such that the DM is indifferent between the reward
sequence and this convex combination. If \(s_{0\rightarrow T}\) is a
constant sequence, i.e.~all its elements are constant, then we can
directly assume \(\mathcal{W}\) is AAD-style. Henceforth, we discuss
whether AAD can also apply to non-constant sequences.

By Lemma 2, we show adding a new reward to the end of
\(s_{0\rightarrow T}\) has no impact on the relative decision weights of
rewards in the original reward sequence.

\noindent \textbf{Lemma 2}: \emph{For any}
\(s_{0\rightarrow T+1}\)\emph{, if}
\(s_{0\rightarrow T}\sim \sum_{t=0}^T w_t \cdot s_t\) \emph{and}
\(s_{0\rightarrow T+1} \sim \sum_{t=0}^{T+1} w'_t\cdot s_t\)\emph{,
where} \(w_t, w'_t>0\) and \(\sum_{t=0}^Tw_t=1\)\emph{,}
\(\sum_{t=0}^{T+1}w'_t=1\)\emph{, then when Axiom 1-4 hold, we can
obtain} \(\frac{w'_0}{w_0}=\frac{w'_1}{w_1}=…=\frac{w'_T}{w_T}\)\emph{.}

\noindent \emph{Proof}: According to Axiom 3, for any
\(s_{0\rightarrow T+1}\), there exist \(\alpha,\zeta \in (0,1)\) such
that\[\tag{A1}
\begin{aligned}
s_{0 \rightarrow T}\sim\alpha\cdot s_{0 \rightarrow T-1} + (1-\alpha)\cdot s_T \\
s_{0\rightarrow T+1} \sim \zeta\cdot s_{0\rightarrow T} + (1-\zeta)\cdot s_{T+1}
\end{aligned}
\]On the other hand, we drawn on Lemma 1 and set\[\tag{A2}
s_{0\rightarrow T+1} \sim \beta_0\cdot s_{0 \rightarrow T-1} + \beta_1\cdot s_T + (1-\beta_0-\beta_1)\cdot s_{T+1}
\]where \(\beta_0, \beta_1 > 0\). According to Axiom 4,
\(1-\zeta=1-\beta_0-\beta_1\). So, \(\beta_1=\zeta-\beta_0\). This also
implies \(\zeta > \beta_0\).

According to Axiom 2, we suppose there exists a reward sequence \(s\)
such that
\(s \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1\).
By Equation (A2) and reduction of compound alternatives, we have
\(s_{0\rightarrow T+1}\sim \zeta \cdot s + (1-\zeta)\cdot s_{T+1}\).
Combining Equation (A2) with the second line of Equation (A1) and
applying transitivity and state-independence, we obtain
\(s_{0\rightarrow T} \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1\).

We aim to prove that for any \(s_{0\rightarrow T+1}\), we can obtain
\(\alpha=\frac{\beta_0}{\zeta}\). To do this, we first assume (without
loss of generality) that \(\alpha > \frac{\beta_0}{\zeta}\).

Consider the case that \(s_{0 \rightarrow T-1} \succ s_T\). By
state-independence, for any \(c\in \mathbb{R}_{\geq 0}\), we have
\((\alpha - \frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c \succ (\alpha - \frac{\beta_0}{\zeta})\cdot s_T + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c\).
By Axiom 2, there exists \(z\in \mathbb{R}_{\geq 0}\) such that
\((1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}\sim z\).
Given \(c\) is arbitrary, we set
\((1-\alpha+\frac{\beta_0}{\zeta})\cdot c \sim z\). By reduction of
compound alternatives, we can derive that\[
(\alpha-\frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1} \succ (\alpha-\frac{\beta_0}{\zeta})\cdot s_T +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}
\]where the LHS can be rearranged to
\(\alpha\cdot s_{0\rightarrow T-1} + (1-\alpha)\cdot s_T\), and the RHS
can be rearranged to
\(\frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1\).
They both should be indifferent from \(s_{0\rightarrow T}\). This
results in a contradiction. Similarly, in the case that
\(s_T \succ s_{0 \rightarrow T-1}\), we can also derive a contradiction.
Meanwhile, when \(s_{0\rightarrow T}\sim s_T\), \(\alpha\) and
\(\frac{\beta_0}{\zeta}\) can be any number within \((0,1)\). So, we can
directly set \(\alpha = \frac{\beta_0}{\zeta}\).

Thus, we have \(\alpha = \frac{\beta_0}{\zeta}\) for any
\(s_{0\rightarrow T+1}\), which indicates
\(\frac{\beta_0}{\alpha}=\frac{\beta_1}{1-\alpha}=\zeta\). We can
recursively apply this equality to any sub-sequence
\(s_{0\rightarrow t}\) (\(t\leq T\)) of \(s_{0\rightarrow T+1}\), so
that the lemma will be proved. \emph{QED}.

Now we move on to prove Proposition 1. The proof contains six steps.

First, we add the constraints \(\sum_{t=0}^T w_t=1\) and \(w_t>0\) to
the optimal discounting problem for \(s_{0\rightarrow T}\) so that the
problem is compatible with Lemma 1. According to the FOC of its
solution, for all \(t=0,1,….,T\), we have\[\tag{A3}
f_t'(w_t)=u(s_t)+\theta
\]where \(\theta\) is the Lagrange multiplier. Given that \(f'_t(w_t)\)
is strictly increasing, \(w_t\) is increasing with \(u(s_t)+\theta\). We
define the solution as \(w_t =\phi_t(u(s_t)+\theta)\).

Second, we add a new reward \(s_{T+1}\) to the end of
\(s_{0\rightarrow T}\) and apply Lemma 2 as a constraint to optimal
discounting problem. Look at the optimal discounting problem for
\(s_{0\rightarrow T+1}\). For all \(t\leq T\), the FOC of its solution
will take the same form as Equation (A3). So, if importing \(s_{T+1}\)
changes some \(w_t\) to \(w'_t\) (\(w'_t \neq w_t\), where \(w_t\) is
the solution to optimal discounting problem for \(s_{0\rightarrow T}\)),
the only way is through changing the multiplier \(\theta\). Suppose
importing \(s_{T+1}\) changes \(\theta\) to \(\theta-\Delta \theta\), we
have \(w'_t = \phi_t(u(s_t)+\theta-\Delta \theta)\).

By Lemma 2, we know
\(\frac{w_0}{w'_0}=\frac{w_1}{w'_1}=…=\frac{w_T}{w'_T}\). In other
words, for \(t=0,1,…,T\), we have
\(w_t \propto \phi_t(u(s_t)+\theta-\Delta \theta)\). We can rewrite
\(w_t\) as \[\tag{A4}
w_t = \frac{\phi_t(u(s_t)+\theta-\Delta \theta)}{\sum_{\tau=0}^{T}\phi_\tau(u(s_\tau)+\theta-\Delta \theta)}
\]

Third, we show that in \(s_{0\rightarrow T}\), if we change each \(s_t\)
to \(z_t\) such that \(u(z_t)=u(s_t)+\Delta u\), the decision weights
\(w_0,…,w_T\) will remain the same. Note
\(\sum_{t=0}^T \phi_t(u(s_t)+\theta)=1\). It is clear that
\(\sum_{t=0}^T \phi_t(u(z_t)+\theta-\Delta u)=1\). Suppose changing
every \(s_t\) to \(z_t\) moves \(\theta\) to \(\theta'\) and
\(\theta'<\theta-\Delta u\). Then, we must have
\(\phi_t(u(z_t)+\theta')<\phi_t(u(z_t)+\theta-\Delta u)\) since
\(\phi_t(.)\) is strictly increasing. This results in
\(\sum_{t=0}^T \phi_t(u(z_t)+\theta')<1\), which contradicts with the
constraint that the sum of all decision weights is 1. The same
contradiction can apply to the case that \(\theta'>\theta-\Delta u\).
Therefore, changing every \(s_t\) to \(z_t\) must move \(\theta\) to
\(\theta - \Delta u\), and each \(w_t\) can only be moved to
\(\phi_t(u(z_t)+\theta -\Delta u)\), which is exactly the same as the
original decision weight.

A natural corollary of this step is that, subtracting or adding a common
number to all intantaneous utilities in a reward sequence has no effect
on decision weights. What actually matters for determining the decision
weights is the difference between instantaneous utilities. This
indicates, for convenience, we can subtract or add an arbitrary number
to the utility function.

In other words, for a given \(s_{0\rightarrow T}\) and \(s_{T+1}\), we
can define a new utility function \(v(.)\) such that
\(v(s_t) = u(s_t) +\theta-\Delta \theta\). So, Equation (A4) can be
re-written as\[\tag{A5}
w_t = \frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))}
\]If \(w_t\) takes the AAD form under the utility function \(v(.)\),
i.e.~\(w_t \propto d_t e^{v(s_t)/\lambda}\), then it should also take
the AAD form under the original utility function \(u(.)\).

Fourth, we show that in Equation (A4), \(\Delta \theta\) has two
properties: (i) \(\Delta \theta\) is strictly increasing with
\(u(s_{T+1})\); (ii) suppose \(\Delta \theta = \underline{\theta}\) when
\(u(s_{T+1})=\underline{u}\) and \(\Delta\theta=\bar{\theta}\) when
\(u(s_{T+1})=\bar{u}\), where \(\underline{u}<\bar{u}\), then for any
\(l \in(\underline{\theta},\bar{\theta})\), there exists
\(u(s_{T+1})\in(\underline{u},\bar{u})\) such that
\(\Delta \theta = l\).

The property (i) can be shown by contradiction. Given \(w_0,…,w_{T+1}\)
a sequence of decision weights for \(s_{0\rightarrow T+1}\). Suppose
\(u(s_{T+1})\) is increased but \(\Delta \theta\) is constant. In this
case, each of \(w'_0,…,w'_T\) should also be constant. However,
\(w'_{T+1}\) must increase as it is strictly increasing with
\(u(s_{T+1})+\theta-\Delta \theta\) (\(\theta\) is determined by the
optimal discounting problem for \(s_{0\rightarrow T}\); thus, any
operations on \(s_{T+1}\) should have no effect on \(\theta\)). This
contradicts with the constraint that \(\sum_{t=0}^{T+1} w'_t =1\). The
only way to avoid such contradictions is to set \(\Delta \theta\)
strictly increasing with \(s_{T+1}\), so that \(w'_0,…,w'_T\) are
decreasing with \(u(s_{T+1})\).

For property (ii), note that for any reward sequence
\(s_{0\rightarrow T+1}\) and a given \(\theta\), \(\Delta\theta\) is
defined as the solution to
\(\sum_{t=0}^{T+1} \phi_t(u(s_t)+\theta-\Delta\theta)=1\). Given an
arbitrary number \(l\in(\underline{\theta},\bar{\theta})\), the proof of
property (ii) consists of two stages. First, for \(t=0,1,…,T\), we need
to show that \(u(s_t)+\theta-l\) is still in the domain of
\(\phi_t(.)\). Second, for period \(T+1\), we need to show for any
\(\omega\in(0,1)\), there exists \(u(s_{T+1})\in \mathbb{R}\) such that
\(\phi_{T+1}(u(s_{T+1})+\theta-l)=\omega\).

For the first stage, note \(\phi_t(.)\) is the inverse function of
\(f'_t(.)\). Suppose when \(\Delta\theta=\bar{\theta}\), we have
\(f'_t(w^{a}_t)=u(s_t)+\theta-\bar{\theta}\), and when
\(\Delta\theta=\underline{\theta}\), we have
\(f'_t(w^{b}_t)=u(s_t)+\theta-\underline{\theta}\). For any
\(l\in(\underline{\theta},\bar{\theta})\), we have
\(u(s_t)+\theta-l \in (f'_t(w^a_t),f'_t(w^b_t))\). Given that
\(f'_t(.)\) is continuous and strictly increasing, there must be
\(w_t\in(w^a_t,w^b_t)\) such that \(f'_t(w_t)=u(s_t)+\theta-l\). So,
\(u(s_t)+\theta-l\) is in the domain of \(\theta_t(.)\). For the second
stage, given an arbitrary \(\omega\in(0,1)\), we can set
\(u(s_{T+1})=f'(\omega)-\theta+l\), so that the desired condition is
satisfied.

A corollary of this step is that we can manipulate \(\Delta \theta\) in
Equation (A4) at any level in \([\underline{\theta},\bar{\theta}]\) by
changing a hypothetical \(s_{T+1}\).

Fifth, we show \(\ln \phi_t(.)\) is linear under some conditions. To do
this, let us add a hypothetical \(s_{T+1}\) to the end of \(s_T\) and
let \(w'_t=\phi_t(v(s_t))\) denote the decision weights for
\(t=0,1,…,T+1\) in \(s_{0\rightarrow T+1}\). We change the hypothetical
\(s_{T+1}\) within the set
\(\{s_{T+1}|v(s_{T+1})\in[\underline{v},\bar{v}]\}\) and see what will
happen to the decision weights from period 0 to period \(T\). Suppose
this changes each \(w'_t\) to \(\phi_t(v(s_t)-\eta)\). Set
\(\eta=\underline{\eta}\) when \(u(s_{T+1})=\underline{v}\) and
\(\eta=\bar{\eta}\) when \(u(s_{T+1})=\bar{v}\). By Equation (A5), we
have\[\tag{A6}
\frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))} = \frac{\phi_t(v(s_t)-\eta)}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau)-\eta)}
\]For each \(t=0,1,...,T\), we can rewrite \(\phi_t(v(s_t))\) as
\(e^{\ln \phi_t(v(s_t))}\). For the LHS of Equation (A6), multiplying
both the numerator and the denominator by a same number will not affect
the value. Therefore, Equation (A6) can be rewritten as \[
\frac{e^{\ln\phi_t(v(s_t))-\kappa\eta}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau))-\kappa\eta}} = \frac{e^{\ln\phi_t(v(s_t)-\eta)}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau)-\eta)}}
\]where \(\kappa\) can be any real-valued constant. By properly
selecting \(\kappa\), we for all \(t=0,1,...,T\), can obtain\[\tag{A7}
\ln \phi_t(v(s_t))-\kappa\eta=\ln \phi_t(v(s_t)-\eta)
\]as long as \(\eta \in [\underline{\eta},\bar{\eta}]\). And given
\(\ln\phi_t(.)\) is strictly increasing, for any \(\eta\neq 0\), we have
\(\kappa>0\).

Finally, we denote the maximum and minimum of \(\{v(s_t)\}_{t=0}^T\) by
\(v_{\max}\) and \(v_{\min}\), and show that Equation (A7) can hold if
\(\eta = v_{\max} - v_{\min}\). That is,
\(v_{\max}-v_{\min}\in [\underline{\eta},\bar{\eta}]\), where
\(\underline{\eta}, \bar{\eta}\) are the realizations of \(\eta\) when
\(v(s_{T+1})=\underline{v}\) and \(v(s_{T+1})=\bar{v}\). Obviously,
\(\underline{\eta}\) can take the value \(\underline{\eta}=0\). Thus, we
focus on whether \(\bar{\eta}\) can take a value
\(\bar{\eta}\geq v_{\max}-v_{\min}\).

The proof is similar with the fourth step and consists of two stages.
First, we show that for \(t=0,1,…,T\), \(v(s_t)-v_{\max}+v_{\min}\) is
in the domain of \(\phi_t(.)\). That is, under some \(w_t\), we have
\(f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}\). Note
\(v_{\max}-v_{\min}\in[0,+\infty)\). On the one hand, there exists
\(w_t\in(0,1)\) for \(f'_t(w_t)=v(s_t)\), which is the solution to
Equation (A5). On the other hand, by Definition 2, we know
\(\lim_{w_t\rightarrow 0}f'_t(w_t)=-\infty\). Given \(f'_t(w_t)\) is
continuous and strictly increasing, there must be a solution \(w_t\) for
\(f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}\). Second, we show that for any
\(\omega\in(0,1)\), there exists some \(v(s_{T+1})\) such that
\(\phi_{T+1}(v(s_{T+1})-v_{\max}+v_{\min})=\omega\). This can be
achieved by setting \(v(s_{T+1})=f'_{T+1}(\omega)+v_{\max}-v_{\min}\).

As a result, for any period \(t\) in \(s_{0\rightarrow T}\), by Equation
(A7), we have \(\ln \phi_t(v(s_t))=\ln\phi_t(v(s_t)-\eta)+\kappa\eta\)
as long as \(\eta\in[0,v_{\max}-v_{\min}]\), where \(\kappa>0\). We can
rewrite each \(\ln \phi_t(v(s_t))\) as
\(\ln \phi_t(v_{\min})+\kappa(v(s_t)-v_{\min})\). Therefore, we
have\[\tag{A8}
w_t \propto \phi_t(v_{\min})\cdot e^{\kappa(v(s_t)-v_{\min})}
\]and \(\sum_{t=0}^T w_t=1\). In Equation (A8), setting
\(\phi_t(v_{\min})=d_t\), \(\lambda = 1/\kappa\), and apply the
corollary of the third step, we can conclude that
\(w_t\propto d_t e^{u(s_t)/\lambda}\), which is of the AAD form.
\emph{QED}.
