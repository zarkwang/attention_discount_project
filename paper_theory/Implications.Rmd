---
bibliography: reference.bib
# biblio-style: apalike
# header-includes: 
#   \usepackage{setspace}
#   \usepackage{amsmath}
#   \usepackage{array}
#   \usepackage{caption}
#   \usepackage{longtable}
#   \usepackage{booktabs}
#   \usepackage{enumitem}
#   \renewcommand{\arraystretch}{1}
#   \captionsetup[table]{skip=5pt}
#   \setstretch{1.5} 
# fontsize: 12pt
# geometry: margin=1in
# editor_options: 
#   markdown: 
#     wrap: 72
# output:
#   #word_document:
#     #number_sections: true
#   pdf_document:
#     number_sections: true
#     citation_package: natbib
#     keep_tex: true
#   html_document:
#     toc: true
#     number_sections: true
---

## Hidden Zero Effect

Empirical evidence suggests the subjective discount factor of a reward is dependent not just on delay but also on the framing of reward sequences. In this subsection, we discuss the evidence of (asymmetric) hidden zero effect [@magen2008hidden; @radu2011mechanism; @read2017value]. Similar with violation of dominance, this effect also provides a justification for our assumption that the sum of AADs is a fixed amount.

To illustrate, suppose the DM is indifferent between "receive £100 today" (SS) and "receive £120 in 25 weeks" (LL). The hidden zero effect suggests that people are more likely to choose LL when SS is framed as a sequence rather than as a single-period reward. In other words, if we frame SS as "receive £100 today and £0 in 25 weeks" (SS1), the DM would prefer LL to SS1. Moreover, @read2017value find that framing LL as "receive £0 today and £120 in 25 weeks" (LL1) has no effect on preference.

The hidden zero effect can be explain by the AAD model. When a DM valuates a reward sequence $s_{0\rightarrow T}$, the AAD model assumes that she splits a fixed amount of attention over $T$ periods. For the given example, the DM may perceive the time length of SS as "today" and perceives the time length of SS1 as "25 weeks". In the former case, she can focus her attention on the current period when she can get £100. While in the latter case, she have to spend some attention to future periods in which no reward is delivered, which also decreases the decision weight assigned to the current period. As a result, she values SS1 lower than SS. By contrast, the DM may perceive the time length of both LL and LL1 as "25 weeks". When she valuate LL, she has already paid some attention to periods earlier than "25 weeks". Therefore, changing LL to LL1 does not change the choice.

## Relation to Hyperbolic Discounting

Most of the intertemporal choice studies only involve comparisons between single-period rewards (SS and LL). Here we derive the discount factor for SS/LL under the AAD model and use that to illustrate how attention allocation can account for the anomalies in such decision settings. For simplicity, we assume the reference weight $d_t = \delta^t$, $\delta\in(0,1]$, i.e. the DM initial discount factor (before learning information about $s_{0\rightarrow T}$) is exponential.[^1]

[^1]: @strotz1955myopia shows that if, for any reward delivered at period $t$, the DM's discount factor is $\delta^t$, then her preference will be stationary and consistent over time.

Consider a reward sequence $s_{0\rightarrow T}$ where for all $t\leq T$, $u(s_t)=0$ and only $u(s_T)>0$. This implies the DM receives nothing until period $T$. In this case, the DM's valuation of $s_{0\rightarrow T}$ is $U(s_{0\rightarrow T})=w_Tu(s_T)$. Let $v(x)=u(x)/\lambda$. By Definition 1, we can derive that $w_T$ is a function of $s_T$:$$\tag{3}
 w_T = \frac{1}{1+G(T)e^{-v(s_T)}}
$$where$$ G(T) = \left\{ \begin{aligned} & \frac{1}{1-\delta}(\delta^{-T}-1) \; ,& 0<\delta<1\\ & T\; ,& \delta=1\ \end{aligned} \right. $$This $w_T$ can represent the discount function for a single reward $s_T$, delivered at period $T$. Interestingly, when $\delta=1$, $w_T(s_T)$ takes a form similar with hyperbolic discounting. In recent years, some studies have attempted to provide a rational account for hyperbolic discounting. For instance, @gabaix2017myopia propose a model with similar assumptions to our information maximizing exploration approach to AAD: the DM's perception of utility is noisy and the DM updates her beliefs about utility with the Bayes' rule. Nevetherless, @gabaix2017myopia account for hyperbolic discounting with an assumption that the variance of utility signal is proportional to delay, whereas we propose the DM seeks to maximize her information gain when learning information about utilities. Meanwhile, when $v(s_t)=\ln(\beta s_t+1)$, where $\beta>0$, the discount function $w_T$ takes a similar form to @gershman2020rationally. Therefore, some remarks about such models can also be generated by the AAD in a special case.

In the following three subsections, we use Equation (3) to explain three decision anomalies: the common difference effect (and its reverse), timing risk aversion, and S-shaped value function.

## Common Difference Effect

The common difference effect [@loewenstein1992anomalies] implies that, when the DM faces a choice between LL and SS, adding a common delay to both options can increase her preference for LL. For example, suppose the DM is indifferent between "receive £120 in 25 weeks" (LL) and "receive £100 today" (LL). Then, she would prefer "receive £120 in 40 weeks" to "receive £100 in 15 weeks".

Let $(v_l,t_l)$ denote a reward of utility $v_l$, delivered at period $t_l$ and $(v_s,t_s)$ denote a reward of utility $v_s$, delivered at period $t_s$. We set $v_l>v_s>0$, $t_l>t_s>0$. So, $(v_l,t_l)$ can represent a LL and $(v_s,t_s)$ can represents a SS. We represent the discount factors for LL and SS by $w_{t_l}(v_l)$ and $w_{t_s}(v_s)$. Suppose $w_{t_l}(v_l)\cdot v_l = w_{t_s}(v_s)\cdot v_s$, then the common difference effect implies that $w_{t_l+\Delta t}(v_l)\cdot v_l > w_{t_s+\Delta t}(v_s)\cdot v_s$, where $\Delta t >0$. Given Equation (3), the conditions for the common difference effect are derived in Proposition 2.

\noindent \textbf{Proposition 2}: *The following statements are true:*

(a) *If* $\delta=1$*, the common difference effect always holds.*

(b) *If* $0<\delta<1$*, i.e. the DM is initially impatient, the common difference effect holds when and only when*$v_l-v_s+\ln(v_l/v_s)>(t_l-t_s)\ln(1/\delta)$*.*

The proof of Proposition 2 is in Appendix B. The part (b) of Proposition 2 yields a novel prediction about the common difference effect. That is, for an impaient DM, to make this effect hold, the relative and absolute differences in reward utility between LL and SS must be significantly larger than their aboslute difference in time delay. In the opposite, if the difference in delay is significantly larger than the difference in reward utility, we may observe a reversed common difference effect.[^2] Figure \ref{fig:common_diff} demonstrates an example for this reversed effect.

[^2]: It is worth mentioning that if we make the "hidden zeros" explicit in LL and SS, adding a common delay under the AAD model would always yield a the common difference effect.

```{=tex}
\begin{figure}[h]
  \centering   
  \includegraphics[width=0.65\textwidth]{figures/common_diff.png}   
  \caption{The common difference effect and its reverse}
  \vspace{8pt}
  \begin{minipage}{1.0\textwidth}
{\par\footnotesize Note: $x_l$ and $x_s$ are the positive reward levels for LL and SS. The values of LL and SS are calculated based on Equation (3). $d_t=0.75^t$, $u(x)=x^{0.6}$, $\lambda=2$. For each certain $t_s$, we identify the delay $t_l$ that makes the value of LL equivalent to SS. If the common different effect is valid, for one unit increase in $t_s$, the resultant $t_l$ should increase by a level smaller than one unit. }
\end{minipage}
  \label{fig:common_diff} 
\end{figure}
```
When the DM is impatient, adding a common delay would naturally make $v_l$ and $v_s$ more discounted, i.e. less attention is paid to the corresponding rewards. Since the sum of decision weights is fixed, this implies the the DM frees up some attention and she needs to reallocate it across the periods in each reward sequence (LL and SS). There are three mechanisms jointly determining whether we could observe the common difference effect or not.

First, the existing periods with no reward delivered would grab some attention. That is, the DM would attend more to the rewards of zero utility, delivered in duration $[0,t_l)$ for LL and in duration $[0,t_s]$ for SS. Given $t_l >t_s$, the relevant duration in LL may naturally capture more attention than that in SS. In other words, the common delay makes the DM focus more on the waiting time in LL than in SS, which decreases her preference for LL.

Second, the newly added time intervals also grab some attention. That is, the DM needs to pay some attention to rewards (of zero utility as well) delivered in duration $(t_l,t_l+\Delta t]$ in LL and in duration $(t_s, t_s+\Delta t]$ in SS. For LL, there are already plenty of periods over which DM has to split her attention. So, the duration $(t_l,t_l+\Delta t]$ in LL can capture less attention than its counterpart in SS. This increases the DM's preference for LL.

Third, the only positive reward, delivered in $t_l$ for LL and in $t_s$ for SS, may draw some attention back. Given that the DM in general tends to pay more attention to larger rewards, the positive reward in LL can capture more "free" attention than that that in SS. This also increases the preference for LL. If the latter two mechanisms override the first mechanism, we would observe a common difference effect in DM's choices.

## Concavity of Discount Function

Many time discounting models, such as exponential and hyperbolic discounting, assume the discount function is convex in time delay. This style of discount function predicts DM is *risk seeking over time lotteries*. To illustrate, suppose a reward of level $x$ is delivered at period $t_l$ with probability $\pi$ and is delivered at period $t_s$ with probability $1-\pi$, where $0<\pi<1$. Meanwhile, another reward of the same level is delivered at period $t_m$, where $t_m=\pi t_l +(1-\pi) t_s$. Under such discount functions, the DM should prefer the former reward to the latter reward. For instance, she may prefer receiving an amount of money today or in 20 weeks with equal chance, rather than receiving it in 10 weeks with certainty. However, experimental studies suggest that people are often *risk averse over time lotteries*, i.e. they prefer the reward to be delivered at a certain time [@onay2007intertemporal; @dejarnette2020time].

One way to accommodate the evidence about risk aversion over time lotteries is to make the discount function concave in terms of delay. Notably, @onay2007intertemporal find that people are more likely to be risk averse over time lotteries when $\pi$ is small, and to be risk seeking when $\pi$ is large. Given that when $\pi$ gets larger, $t_m$ is also larger, we can conclude that the discount function may be concave in delay for the near future but convex for the far future. That is, the discount function is of inverse-S shape. @takeuchi2011non also find evidence that supports this shape of discount function.

In Proposition 3, we apply Equation (3) and show that the AAD is compatible with this shape of discount function as long as the DM is impatient and the reward level $x$ is large enough.

\noindent \textbf{Proposition 3}: *If* $\delta =1$*, then the discount function* $w_T$ i*s convex in* $T$*. If* $0<\delta<1$*, there exist a reward threshold* $\underline{x}>0$ *and a time threshold* $\underline{T}>0$ *such that:*

(a) *when* $x\leq \underline{x}$*, the discount function is convex in* $T$;
(b) *when* $x > \underline{x}$*, the discount function is convex in* $T$ *given* $T\geq \underline{T}$*, and it is concave in* $T$ *given* $0<T<\underline{T}$*.*

The proof of Proposition 3 is in Appendix C. Figure \ref{fig:discount_value_function}(a) demonstrates the convex discount function (blue line) and the inverse-S shaped discount function (red line) that could be yielded by Equation (3).

```{=tex}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/concave_discount.png} 
        \caption{discount function}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/s_shape_value.png}
        \caption{value function}
    \end{subfigure}
    \caption{Discount function and value function for a delayed reward}

  \vspace{8pt}
  \begin{minipage}{1.0\textwidth}
{\par\footnotesize Note: A reward of level $x_T$ is delivered at period $T$. The discount function and value function are calculated based on Equation (3). $d_t=0.75^t$, $u(x)=x^{0.6}$, $\lambda=2$.}
\end{minipage}
    
    \label{fig:discount_value_function}
\end{figure}
```
## S-Shaped Value Function

In decision theories, it is commonly assumed that the utility function $u(.)$ satisfies $u''<0$. This usually suggests the value function of a reward is concave. However, empirical evidence suggests that the value functions are often S-shaped. Such S-shaped value functions can be generated by various sources, such as reference dependence [@kahneman1979prospect] and efficient coding of numbers [@louie2012efficient]. Through the AAD model, we provide a novel account for S-shaped value function based on the insight that larger rewards capture more attention.

Consider a reward of level $x$ delivered at period $T$. Its value function can be represented by $U(x,T)=w_T(x)u(x)$. We assume $u'>0$, $u''<0$, and $w_T$ is determined by Equation (3). $w_T$ is increasing with $x$ as the DM tends to pay more attention to larger rewards. Both functions $u(x)$ and $w_T(x)$ are concave in $x$; so when $x$ is small, they both grow fast. At some conditions, it is possible that the product of the two functions is convex in $x$ when $x$ is small enough. We derive the conditions for the S-shaped value function in Proposition 4.

\noindent \textbf{Proposition 4}: *Suppose* $T\geq1$*,* $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ *is continuous in* $(0,+\infty)$*, then:*

(a) *There exists a threshold* $\bar{x} \in\mathbb{R}_{\geq0}$ *such that* $U(x,T)$ *is strictly concave in* $x$ *when* $x\in [\bar{x},+\infty)$*;*

(b) *If* $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ *is right-continuous at* $x=0$ *and* $\frac{d}{dx}\left(\frac{1}{v'(0)}\right)<1$*, there exists a threshold* $x^*$ *in* $(0, \bar{x})$ *such that, for any* $x\in (0,x^*)$*,* $U(x,T)$ *is strictly convex in* $x$.

(c) *There exists an unit cost of attention adjustment* $\lambda^*$ *and an interval* $(x_1,x_2)$ *such that, if* $\lambda<\lambda^*$*, for any* $x\in(x_1,x_2)$*,* $U(x,T)$ *is strictly convex in* $x$*, where* $\lambda^*>0$ *and* $(x_1,x_2)\subset(0,\bar{x})$*.*

The proof of Proposition 4 is in Appendix D. Proposition 4 implies, if the derivative of $\frac{1}{v'(x)}$ converges to a small number when $x\rightarrow 0^+$, or the unit cost of attention adjustment $\lambda$ is small enough, the value function $U(x,T)$ will be an S-shaped in some interval of $x$. Figure \ref{fig:discount_value_function}(b) demonstrates examples of this S-shaped value function.

## Intertemporal Correlation Aversion

Consider a DM facing two lotteries. For one lottery, she can receive £100 today and £100 in 30 weeks with probability 1/2, and receive £3 today and £3 in 20 weeks with probability 1/2. For the other lottery, she can receive £3 today and £100 in 30 weeks with probability 1/2, and receive £100 today and £3 in 20 weeks with probability 1/2. In the former lottery, rewards delivered at different periods are positively correlated, whereas in the latter lottery, those rewards are negatively correlated. The expected discounted utility theory predicts the DM is indifferent between the two lotteries. However, reccent studies find the evidence of *intertemporal correlation aversion*[@andersen2018multiattribute; @rohde2023intertemporal]. That is, people often prefer the latter lottery than the former one.[^3]

[^3]: For theoretical analysis about intertemporal correlation aversion, please see @epstein1983stationary, @epstein1989substitution, @weil1990nonexpected, @bommier2005risk, and @bommier2017monotone. The AAD model takes a similar form to the class of models defined in @epstein1983stationary. A key feature of such models is that the discount factor for future utilities is dependent on the utility achieved in the current period.

For the above example, intertemporal correlation aversion can be generated by the AAD model as follows. The AAD model assumes the DM allocates decision weights within each specific reward sequence, which implies she would aggregate values over time in each state. For simplicity, suppose there are only two periods. In the state that the DM receives £3 in two periods, suppose the DM allocates decision weight $w$ to the first period and $1-w$ to the second period. Note in Definition 1, when $u(s_0)=u(s_1)=...=u(s_T)$, the decision weight $w_t$ for every period $t$ within the reward sequence $s_{0\rightarrow T}$ will remain the same as the reference weight $d_t$. So, in the state that the DM receives £100 in two periods, the allocation of decision weights is the same as $w$ and $1-w$. In the state that the DM can receive £100 in the first period and £3 in the second period, the reward of £100 can capture more attention so that its decision weight, say $w'$, is greater than $w$. Similarly, in the state that the DM receives £3 earlier and then £100, the decision weight for the reward of £100, say $1-w''$, is also greater than $1-w$. Therefore, the value of the lottery in which rewards are positively correlated, can be represented by $0.5\cdot u(3)+0.5\cdot u(100)$. By contrast, for the lottery in which rewards are negatively correlated, the value can be represented by $0.5(1-w'+w'')\cdot u(3)+0.5(1-w''+w')\cdot u(100)$. Given $(1-w'')+w'>1-w+w>1$, the decision weight assigned to $u(100)$, which is $0.5(1-w''+w')$, should be greater than 0.5. As a result, the DM prefer the latter lottery than the former lottery.

In a more general setting, whether the AAD model can continuously produce intertemporal correlation aversion is modulated by the unit cost of attention adjustment $\lambda$. To illustrate, we adopt the same theoretical setting as @bommier2005risk. Let $(s_1,s_2)$ denote the result of a lottery in which the DM can receive reward $s_1$ in period $t_1$ and then reward $s_2$ in period $t_2$, where $t_2>t_1\geq 0$. There are two lotteries, $L1$ and $L2$. The results of each lottery is of the same length of sequence. $L1$ generates $(x_s,y_s)$ and $(x_l,y_l)$ with equal chance, $L_2$ generates $(x_s,y_l)$ and $(x_l,y_s)$ with equal chance, $x_l>x_s>0$, $y_l>y_s>0$. By Proposition 5, we show that in this setting, we can always find a $\lambda$ that makes the DM intertemporal correlation averse.

\noindent \textbf{Proposition 5}: *Suppose* $U(L1), U(L2)$ *are the values of* $L1$ *and* $L2$*. For any* $x_l>x_s>0$*,* $y_l>y_s>0$*, any reference weights, and any time length of lottery results, there exists* *a threshold* $\lambda^{**}$ such that for any unit co*st of attention adjustment* $\lambda>\lambda^{**}$, *the DM would perform intertemporal correlation aversion, i.e.* $U(L1)<U(L2)$.

The proof of Proposition 5 is in Appendix E. The threshold $\lambda^{**}$ is jointly determined by $x_l$, $y_l$, $y_s$, as well as the reference weights for rewards delivered at $t_1$ and $t_2$. Notably, when $\lambda < \lambda^{**}$, the DM may be intertemporal correlation seeking under some conditions.[^4] This suggests a potentially new mechanism for intertemporal correlation aversion, that is, DM performs intertemporal correlation aversion perhaps because she attends more to larger rewards while attention adjustment is very costly.

[^4]: To validate, suppose $u(x_s)=5$, $u(x_l)=10$, $u(y_s)=1$, $u(y_l)=3$. The results of each lottery contain only two periods, $t_1$ and $t_2$. The reference weights are uniformly distributed, i.e. $d_{t_1}=d_{t_2}$. In this case, setting $\lambda=1$ would generate intertemporal correlation seeking, while setting $\lambda=100$ would generate intertemporal correlation aversion.

## Learning and Inconsistent Planning

Suppose a DM has budget $m$ ($m>0$) and is considering how to spend it over different time periods. We can use a reward sequence $x$ to represent this decision problem, where the DM's spending in period $t$ is $x_t$. In period 0, she wants to find a $x$ such that$$ \tag{3} \max_{x}\;\sum_{t=0}^T w_t u(x_t)\quad s.t. \;\sum_{t=0}^T x_t = m   $$

where $w_t$ is the attention-adjusted discounting factor in period $t$. I assume $w_t=\delta^t e^{u(x_t)/\lambda}/\sum_{t=\tau}^T \delta^{\tau} e^{u(x_\tau)/\lambda}$ and there is no risk under this setting.
