---
title: "Progress Report"
author: "Zijian Zark Wang"
date: "`r Sys.Date()`"
bibliography: reference.bib
biblio-style: apalike
header-includes: 
  \usepackage{setspace}
  \usepackage{amsmath}
  \usepackage{array}
  \usepackage{caption}
  \usepackage{longtable}
  \usepackage{booktabs}
  \renewcommand{\arraystretch}{1}
  \captionsetup[table]{skip=5pt}
  \setstretch{1.5} 
fontsize: 12pt
geometry: margin=1in
editor_options: 
  markdown: 
    wrap: 72
output:
  pdf_document:
    number_sections: true
    citation_package: natbib
  html_document:
    toc: true
    number_sections: true
---

# Current Progress

I develop a model of intertemporal choice, which I term
"*attention-adjusted discounting*". I postulate that the overall utility
a decision maker can obtain from a reward stream is the weighted sum of
instantaneous utilities of each time period. The initial weight
allocation is exponential, i.e. the decision maker is initially
time-stationary. However, when evaluating a given reward stream, she
tends to assign more weights (pay more attention) to the time periods
with larger rewards, in order to subjectively maximize her overall
utility. This attention adjustment process incurs a cognitive cost; and
the more the weight allocation deviates from the initial allocation, the
greater the cost is. The decision maker optimally re-allocates the
weights over time periods.

I term any discounted utility model of which the discounting factors
take an attention-adjusted form as ADU. I show that ADU can help explain
a series of empirical findings about intertemporal choice, including
common difference effect and magnitude effect
[@loewenstein_anomalies_1992], risk aversion over time lotteries
[@onay_intertemporal_2007; @dejarnette_time_2020], non-additive time
intervals [@read_is_2001; @scholten_discounting_2006], intertemporal
correlation aversion [@andersen_multiattribute_2018], and dynamic
inconsistency. The model can also offer insights on preferences for
sequences of outcomes [@loewenstein_preferences_1993] and the formation
of reference-dependent preferences [@koszegi_model_2006]. In empirical
analysis, I find ADU outperforms a set of time discounting models in
predicting human intertemporal choices.

The following example describes the underlying process of
attention-adjusted discounting: Suppose a decision maker needs to
estimate the value of "receiving £10 in 3 days". Set that Day 1's reward
and Day 2's reward are both 0, Day 3's reward is 10, and the decision
maker initially holds stationary time preferences. We can consider a
exponential discounting model where the discounting parameter is 0.8,
thus the value of this reward stream is 0 × 1 + 0 × 0.8 + 10 × 0.64. One
can also imagine that there is a black box with three types of balls,
1000 of which are initially labelled "Day 1's reward", 800 are initially
labelled "Day 2's reward", 640 are initially labelled "Day 3's reward".
The decision maker draws a random sample from the box, then exchange the
drawn balls for rewards.

The key hypothesis of ADU is, prior to the draw, there is an "attention
mechanism" that decides how many of each type of balls should be put
into the black box. Putting zero balls of a certain type into the box
implies the decision maker totally ignores that type. Given that "Day
3's reward" is the greatest, she wants to put more of this type of balls
into the box -- to exchange for more rewards afterwards. However, the
total number of balls in the box is fixed ("limited attention") and it
is extremely cognitively effortful to change all the balls initially in
the box to this type ("costly attention adjustment"). She can only
change a part of them. Suppose the final distribution of balls for Day
1, Day 2 and Day 3 is 970, 770 and 700, that is, the decision maker
successfully changes 60 balls to "Day 3's reward"; as a result, the
discounting factors corresponding to each time period should shift to 1,
0.79 and 0.72. Hence, her discounting factors will perform a
"hyperbolic" style. Besides, if we increase the magnitude of Day 3's
reward to £20, the benefit of increasing the weight on "Day 3's reward"
can offset a greater cost of attention adjustment. Thus, she performs
greater patience for a larger delayed reward.

While building the model, I was mainly inspired by the theories of
rational inattention [@matejka_rational_2015; @jung_discrete_2019;
@mackowiak_rational_2023]. In @matejka_rational_2015's theory of
rational inattention, the decision maker makes choices between discrete
alternatives; she evaluates each alternative via a costly information
acquisition process, then decides the optimal choice strategy. The
theory deduces that the probability of each alternative being chosen
follows a logistic-like distribution. In ADU, I assume the discounting
factors are generated by a similar process; hence, she subjectively
weights each time period according to a logistic-like distribution as
well.

The remaining part of this document is organized as follows. Section
\ref{model} outlines the model of attention-adjusted discounting.
Section \ref{behavioral} explains how the model can help explain the
empirical findings about choices and preferences. Section
\ref{empirical} performs an empirical test of the model, in comparison
with other intertemporal choice models. Section \ref{effort} introduces
an experiment design about how to test a key assumption of rational
inattention.

# Attention-Adjusted Discounting \label{model}

Consider a reward stream $x = [x_0,x_1,...,x_T]$ that yields reward
$x_t$ in time period $t$. The time length of this stream, denoted by
$T$, is finite. For any $t \in \{0,1,...,T\}$, the reward level $x_t$ is
a random variable defined on $R_{+}$. The support of $x$ is $X$, which
is a subset of $R_{+}^T$.

Suppose there is a decision maker wanting to evaluate the reward stream
$x$. She firstly randomly draws some potential realizations of $x$ from
$X$. Let $s=[s_0,s_1,...,s_T]$ be a potentially realized reward stream,
and $p(s)$ be the probability that $s$ is drawn. Then, for each drawn
realization of $x$, she randomly draws some time periods from the
stream, taking the reward of each drawn period into a sample. I denote
the decision maker's value function by $u(.)$, where $u(s_t)$ is the
utility obtained by reward $s_t$ ($t \in \{0,1,...,T\}$), and $u'>0$,
$u''<0$. Finally, she uses the mean value of sampled rewards as a value
representation of $x$. I denote the decision maker's sampling strategy
by a weighting function $w(.)$, where $w(s_t)$ is the probability that
the reward of the $t$-th period in a potentially realized stream $s$ is
sampled. She wants to find a function $w(.)$ that maximizes her overall
utility. In a given potentially realized stream $s$, the periods with
larger reward levels should be sampled more frequently. However, at the
very beginning, the decision maker does not know which period in $s$ has
a larger reward - she also learns such information via sampling. This
learning process triggers a cognitive cost. Therefore, her overall
utility is the mean value of sampled rewards minus the cognitive cost of
learning.

Suppose when having no information, the weight on period $t$ in each
potentially realized stream is equal ($\equiv w^0_t$). Let $W$ and $P$
be the minimal sets that contain all available function $w$ and function
$p$ respectively. We can use an optimization problem to represent the
above evaluation procedure: $$ 
\begin{aligned}
\max_{w\in W}  \quad & \sum_{s\in X}\sum_{t=0}^T w(s_t)u(s_t) - C(w;\theta) \\
s.t. \quad &  \sum_{s\in X}\sum_{t=0}^T w(s_t)=1 \\
& w(s_t)>0, \forall s\in X,t=0,1,…,T \\
\end{aligned}
$$

where $C(.)$ is a cognitive cost function with $\theta$ as its
parameters. To solve this optimization problem, I add two additional
assumptions. The first is that the weight updating process is consistent
with Bayes rule, that is, $w^0_t=\sum_{s\in X} w(s_t)$. The second is
that the cognitive cost function takes a form similar to Shannon mutual
information, that is$$
C(\textbf{w};\theta)= \lambda \sum_{s\in X}\sum_{t=0}^T w(s_t) \log\left(\frac{w(s_t)}{p(s)w_t^0}\right)
$$

where $p(s)w^0_t$ is the probability of $s_t$ being sampled when no
information is learned, $w(s_t)$ is the probability of that after
learning the information about $x$. Shannon mutual information
quantifies the information about which time period in each $s$ has a
larger reward, obtained by learning. Parameter $\lambda$ denotes unit
cost of information ($\lambda>0$).

Define $w(s_t|s) = \frac{w(s_t)}{p(s)}$. Then $w(s_t|s)$ can represent
the discounting factor of time period $t$ given that the stream $s$ is
drawn. As is shown in @matejka_rational_2015, the solution is$$ \tag{1}
w(s_t|s) =\frac{w_t^0e^{u(s_t)/\lambda}}{\sum_{t=0}^T w_\tau^0 e^{u(s_t)/\lambda}}
$$

Note the $w(s_t|s)$ is increasing in $s_t$, i.e. the decision maker
performs greater patience for a larger reward.

The reason why I use Shannon mutual information as the cognitive cost
function is twofold. First, note that
$w(s_t|s) \propto w^0_t e^{u(s_t)/\lambda}$. Given a certain stream $s$
and two time periods $t_1$ and $t_2$ ($t_2>t_1$), the relative weight
between them $\frac{w(s_{t_1}|s)}{w(s_{t_2}|s)}$ is only relevant to
$s_{t_1}$ and $s_{t_2}$. Therefore, changing the reward of a third
period has no impact on how the reward in $t_2$ should be discounted
relative to that in $t_1$. Second, under such settings, the objective
function can be rewritten as$$
\sum_{s\in X} p(s)[w(s_t|s)u(s_t) - \lambda D_{KL}]
$$

where $D_{KL}$ is the KL divergence between the initial weights over
time periods and the weights updated given the stream $s$ is drawn.
Clearly, the determination of $w(s_t|s)$ in each $s$ can be separated
from each other. In other words, given two potentially realized streams
$s$ and $s'$, the changes in $s'$ has no impact on the determination of
discounting factors in $s$. @matejka_rational_2015 show that the two
properties are satisfied if and only if the solution of $w(s_t|s)$
follows Equation (1).

Suppose $p(s)$ also equals to the true probability that $s$ is realized.
After decides the $w(s_t|s)$, the decision maker can obtain the
discounted utility (DU) of each potentially realized stream $s$. I
assume she wants to find a risky reward stream, denoted by $p$, that
maximizes her expected discounted utility. Therefore,$$
p = \arg\max_{p\in P} \left\{\sum_{s\in X} \sum_{t=0}^T p(s)w(s_t|s)u(s_t)\right\}
$$

Given that the discounting factor $w(s_t|s)$ is formed by an
attention-adjusted evaluation procedure. I term the model alike as
attention-adjusted DU (ADU).

In addition to ADU, there are other models that attempt to incorporate
attention mechanism into the formation of time preferences. For example,
@steiner_rational_2017 consider a decision maker adjusting the belief
$p(s)$ over time but holding the discounting factor $w(s_t|s)$ constant.
In each time period, given that her ability to learn new information is
limited, the upated belief cannot deviate from that in the previous
period by too much, which causes behavioral inertia. Instead, ADU
assumes the decision maker re-allocates $w(s_t|s)$ each time period.
This makes it convenient to analyze not only dynamic decision-making but
also choices in MEL tasks. Besides, @gabaix_myopia_2017 assume the
perception of future rewards is noisy and the decision maker estimates
the value of reward by sampling from a normal distribution. Furthermore,
@gershman_rationally_2020 optimally chooses the sample variance to
minimize the mean sample squared error. Such theories, together with a
certain specification on rate-distortion function, can lead to
magnitude-increasing patience and hyperbolic-like discounting.
Discounting factors in this style can be viewed as a special case of
attention-adjusted discounting. @noor_optimal_2022 and
@noor_constrained_2023 construct an optimization problem similar to ADU.
However, they use a different cognitive cost function. In an empirical
test, I compare the predictive performance of ADU with models of
@gershman_rationally_2020 and @noor_optimal_2022, as well as some other
models.

# Explaining Behavioral Biases \label{behavioral}

## Evaluating Delayed Rewards

Suppose a reward of level $c$ ($c>0$) is delivered at time period $T$.
This delayed reward can be represented by a reward stream
$x=[x_0,x_1,…,x_T]$, where $x_0=…=x_{T-1}=0$ and $x_T=c$. For
simplicity, I assume $u(0)=0$ and $w^0_t=\delta^t$, which implies the
decision maker initially hold stationary time preferences.
$\delta\in(0,1]$, where $\delta=1$ implies the initial attention is
uniformly distributed across time periods. Given that the reward is
deterministic, one can omit $s$ in $w(s_t|s)$ and directly represent the
weight on each time period $t$ by $w_t$. Therefore, the value of this
delayed reward is $w_Tu(x_T)$. By Equation (1),$$ \tag{2}
w_T  = \frac{\delta^Te^{u(x_T)/\lambda}}{\sum_{t=0}^T \delta^te^{u(x_t)/\lambda}}  =\frac{1}{1+G(T)\cdot e^{-u(x_T)/\lambda}}
$$ where$$
G(T) = \left\{\begin{split}
& T &, \delta=1\\
& \frac{1}{1-\delta}(\delta^{-T}-1) &,0<\delta<1
\end{split}\right.
$$

Equation (2) implies $w_T$ is increasing in $x_T$ and decreasing in $T$.
I show that Equation (2) can explain a series of experimental findings.
Due to the limitation on word number, I omit mathematical proof.

### Common Difference Effect

Suppose there are a large later reward $x_l$ arriving at period $t_l$
(denoted by LL) and a small sooner reward $x_s$ arriving at period $t_s$
(denoted by SS), where $x_l>x_s>0$, $t_l>t_s>0$. Assuming
$w_{t_l}(x_l)u(x_l)=w_{t_s}(x_s)u(x_s)$, common difference effect
implies $w_{t_l+\Delta t}(x_l)u(x_l)>w_{t_s+\Delta t}(x_s)u(x_s)$ for
any positive integer $\Delta t$, magnitude effect implies
$w_{t_l}(x_l)u(x_l+\Delta x)>w_{t_s}(x_s)u(x_s+\Delta x)$ for any
positive real number $\Delta x$ [@loewenstein_anomalies_1992].

When $\delta = 1$, ADU predicts that decision makers always perform
common difference effect. This is obvious because the discounting factor
$w_T$ takes a hyperbolic-like form. When $\delta<1$, decision makers
perform common difference effect only when the difference between $x_l$
and $x_s$ are much larger than the difference between $t_l$ and $t_s$.
Note that $w_t \propto \delta^t e^{u(x_t)/\lambda}$. At the intuitive
level, when omitting the constraint that the sum of weights on each time
period is fixed, the discounting factor of any period $t$ other than the
final period in LL or SS should be $\delta^t$, and
$w_{t_l+\Delta t}(x_l)/w_{t_s+\Delta t}(x_s)=\delta^{t_l-t_s} \exp\{\frac{u(x_l)-u(x_s)}{\lambda}\}$,
which is constant for any $\Delta t$. However, given that the decision
makers' attention is limited, when keeping time length fixed, increasing
the reward level in the final period (that is, $x_T$) can make it
naturally grab more attention from the previous periods; when extending
the time length, the average attention that can be allocated to each
period should shrink. Decision makers perform common difference effect
only when the former effect exceeds the latter effect.

### Magnitude Effect and Reference-Dependent Preferences

ADU predicts that the larger the unit cost of information $\lambda$ or
the smaller the magnitude of $x_l$ and $x_s$ is, the more likely it is
that decision makers perform the magnitude effect. First, note that
magnitude effect requires the decision makers' overall utility
$w_T(x_T)u(x_T)$ to be a convex function of $x_T$. Given that $u(.)$ is
concave, whether the magnitude effect holds should depend on $w_T$.
Then, set $z = u(x_T)-\lambda\log G(T)$. We can rewrite Equation (2) as
a logistic function of $z$, i.e. $w_T = 1/(1+e^{-z/\lambda})$. By the
shape of logistic function, $w_T$ is convex in $u(x_T)$ if and only if
$u(x_T)<\lambda \log G(T)$ (that is, when $x_T$ is small relative to $T$
or when $\lambda$ is large). Finally, it is notable that the given
condition is necessary but not sufficient to yield magnitude effect.

In summary, holding the others equal, the decision makers' overall
utility can be convex in a future reward when the level of reward is
under a certain threshold, and be concave when it is above the
threshold. This is also consistent to the theories about
reference-dependent preferences [@koszegi_model_2006].

### Risk Attitude over Time Lotteries and Non-additive Time Intervals

Both exponential and hyperbolic discounting models predict decision
makers are risk seeking over time lotteries. That is, suppose a reward
of level $c$ is delivered in period $t_s$ with probability $\pi$ and is
delivered in period $t_l$ with probability $1-\pi$, where $0<\pi<1$,
then decision makers should prefer this case to the case that the same
reward is delivered in a certain period $\pi t_s +(1-\pi) t_l$. However,
@onay_intertemporal_2007 find people are only risk seeking over time
lotteries when $\pi$ is small and they are risk averse over time
lotteries when $\pi$ is large. This finding can be explained by the
convexity of $w_T$.

Let $t_m = \pi t_s +(1-\pi) t_l$. Under ADU, the decision makers are
risk seeking over time lotteries when
$\pi w_{t_s}(c)+(1-\pi)w_{t_l}(c)>w_{t_m}(c)$. First, note the LHS
equals to the RHS when $\pi=0$ or $\pi=1$. Fixing $t_s$ and $t_l$, this
inequality implies $w_{t_m}(c)$ is convex in $t_m$. Second, it can be
proved that $w_T(c)$ is convex in $T$ if and only if $T$ is above a
certain threshold. Third, note $t_m$ is linearly decreasing with $\pi$,
thus decision makers are more likely to be risk seeking over time
lotteries when $\pi$ is small. The same can be applied to the risk
aversion case.

Now consider $T$ is small enough to make $w_T$ concave in $T$. In this
case, adding an extension to $T$ will increase the rate at which $w_T$
declines with $T$ -- the property is termed "super-additive time
intervals" by @read_is_2001. By contrast, in many models, including
exponential and hyperbolic discounting, discounting factors are
typically decided by a convex function of $T$. Moreover, ADU predicts
intervals are sub-additive when the total time length $T$ is large, and
are super-additive when $T$ is small, which is consistent with
@scholten_discounting_2006.

### Intertemporal Correlation Aversion

Let $x$ and $y$ denote two risky reward streams with 2 periods. For $x$,
the realized stream is [£100,£100] with probability 1/2, and is [£3,£3]
with probability 1/2. For $y$, the realized stream is [£3,£100] with
probability 1/2, and is [£100,£3] with probability 1/2.

intertemporal correlation aversion: people prefer $y$ to $x$.

For simplicity, suppose the initial attention is uniformly distributed
across the two periods.

each potentially realized stream, weight is equal

## Dynamic Inconsistency

There are two mechanisms that can lead to dynamic inconsistency.

updated initial attention

positive action-attention loop

# Comparing Models of Intertemporal Choice \label{empirical}

## Data

To test the capacity of attention-adjusted discounting factor in
explaining the findings of MEL experiments, I select two open datasets.
The first dataset is from @ericson_money_2015, containing 23,131
observations from 939 participants; the second is from
@chavez_hierarchical_2017, containing 34,515 choices from 1,284
participants. Hereafter, I term the first dataset as *Ericson* data, and
the second dataset as *Chávez* data. The experiments corresponding to
each dataset require the participants to answer a series of choice
questions. In each question, participants need to make a choice between
an small sooner reward (denoted by SS) and a large later reward (denoted
by LL). I denote the reward magnitude and delay by $x_s$ and $t_s$ for
option SS, and by $x_l$ and $t_l$ for option LL, where $x_l>x_s>0$,
$t_l>t_s$.

## Empirical Strategy

I mainly focus on out-of-sample model performance. For each dataset, I
randomly draw the responses from 20% of the participants as the test
sample, and set the rest as the train sample. To mitigate the
overfitting issue, I implement a 10-fold cross-validation procedure on
the train sample.

I test three types of intertemporal choice model: discounted utility
model, trade-off model, and heuristic model.

I use the maximum likelihood method to estimate the parameters, and
apply L-BFGS-B method for optimization. As the solutions of L-BFGS-B are
sensitive to initial points and often converge to local optima, I use
the basin-hopping algorithm to achieve global optimization. Finally, I
compare the goodness of fit and out-of-sample performance of 20
discounted utility models, 2 trade-off models, and 1 heuristic model on
the two datasets.

## Results

For *Chávez* data, I use MXD as reward unit and days of delay as $t_l$.
Note the rewards range from 11 MXD to 85 MXD, and for either SS or LL,
the distribution of rewards approximates a uniform distribution.
Therefore, Table \ref{tab:chavez_kf_result} shows the goodness of fit
for each model in cross-validation. Similar with Table
\ref{tab:ericson_kf_result}, the *trade* model with power utility
performs the lowest MSE, and the heuristic model performs the best in
MAE, log loss, and accuracy. Though, the goodness of fit of
attention-adjusted models with power utility are close to the *trade*
model in all evaluation metrics. For example, the log loss for
*attention* with power utility (0.486) is only 0.003 higher than that
for *trade* with power utility (0.483). Table
\ref{tab:chavez_test_result} shows the out-of-sample performance of each
model. The heuristic model performs the best in all evaluation metrics,
following *trade*, *attention*, and *attention_uni* with power utility.
The performance between these models are also close. For example, the
accuracy rate of heuristic model is 76.7%, while those of *trade*,
*attention*, and *attention_uni* with power utility are 76.6%, 76.3%,
76.3%. Setting the predicted choices by heuristic model as the labels
and letting the other models to predicting them, we obtain an accuracy
of 96.2% for *trade* with power utililty (the highest), and an accuracy
of 95.9% for attention-adjusted models with power utility (the second
highest).

In summary, I find the heuristic model outperforms than the other models
in predicting human choices on Ericson and *Chávez* data. Both
Intertemporal trade-off and attention adjustment (given that *hbmd* is a
special case of attention-adjusted model) can be viewed as good
candidates of mechansim for explaining the choices predicted by the
heuristic model. Remarkably, apart from the heuristic model, the *trade*
model has the largest number of parameters, which has 6. The
*attention_uni* and *attention* model have only 3 and 4 parameters to be
fitted, and the minimum number of parameters among the fitted models is
3. Thus, for people who want to save parameters or draw the predictions
on a normative theory of intertemporal choice, the attention adjustment
mechanism can be a choice.

# Testing Rational Inattention via Real-Effort Experiments \label{effort}

During the time around my last annual review, which took place in 2021,
I was working on an experiment that tests people's attitude towards
exerting efforts. After that, I modified the original experimental
design to make it a test for "rational inattention" hypothesis. In the
new design, the participants need to randomly draw a lottery from
different lotteries, and prior to the draw, they can change the
probability of each lottery being drawn through a real-effort task.

# Time Schedule \label{schedule}

# Reference
