---
title: "Appendix"
author: "Zark Wang"
date: "`r Sys.Date()`"
output: html_document
---

# Math Proofs

herefore, the value of this delayed reward is $w_Tu(x_T)$. By Equation (1),$$ \tag{2} w_T  =\frac{1}{1+G(T)\cdot e^{-u(x_T)/\lambda}} $$ where$$ G(T) = \left\{\begin{aligned} & T \;, & \delta=1\\ & \frac{1}{1-\delta}(\delta^{-T}-1) \;,&0<\delta<1 \end{aligned}\right. $$

Equation (2) implies $w_T$ is increasing in $x_T$ and decreasing in $T$. Hereafter, I use $w_T(x_T)$ to represent $w_T$. I will show that Equation (2) can explain a series of experimental findings.

SS: "getting $x_s$ in period $t_s$"

LL: "getting $x_l$ in period $t_l$"

The discounted utility of a delayed reward is $w_t u(x_t)$, where

$$
w_t = \delta^t \frac{1}{1+G(T) e^{-u(x_t)/\lambda}} 
$$

and

$$
G(T) = \frac{\delta}{1-\delta}(1-\delta^T)
$$

given $T\geq t$

## Proof of common difference effect

Suppose the instantaneous utilities of LL and SS are $v_l$ and $v_s$, and the delays for LL and SS are $t_l$ and $t_s$. Under ADU, the common difference effect implies that, if

$$ \tag{A.1}
\frac{1+G(t_s)e^{-v_s}}{v_s} = \frac{1+G(t_l)e^{-v_l}}{v_l}
$$

then for any $\Delta t \geq 0$,

$$ \tag{A.2}
\frac{1+G(t_s+\Delta t)e^{-v_s}}{v_s} > \frac{1+G(t_l+\Delta t)e^{-v_l}}{v_l}
$$

If $G(T)=T$, we have $G(t+\Delta t) = G(t) + \Delta t$. Thus, after combining with Equation (A.1), Equation (A.2) is valid if and only if

$$ \tag{A.3}
\frac{\Delta t e^{-v_s}}{v_s} > \frac{\Delta t e^{-v_l}}{v_l}
$$

Given that function $\psi(v) = e^{-v}/v$ is decreasing in $v$ so long as $v>0$, then Equation (A.3) must be valid.

If $G(T) = \frac{1}{1-\delta}(\delta^{-T}-1)$, we have

$$
1+G(t+\Delta t)e^{-v} = \delta^{-\Delta t}[1+G(t)e^{-v}]+(\delta^{-\Delta t}-1)(\frac{e^{-v}}{1-\delta}-1)
$$

Thus, after combining Equation (A.1), Equation (A.2) is valid if and only if

$$\tag{A.4}
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_s}}{1-\delta}-1}{v_s} >
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_l}}{1-\delta}-1}{v_l}
$$

Given that $0<\delta<1$, we have $\delta^{-\Delta t}>1$, thus Equation (A.4) is valid if and only if

$$\tag{A.5}
\frac{1}{v_s}-\frac{1}{v_l}<\frac{1}{1-\delta}(\frac{e^{-v_s}}{v_s}-\frac{e^{-v_l}}{v_l})
$$

From Equation (A.1) we know that

$$\tag{A.6}
\frac{1}{v_s}-\frac{1}{v_l}=\frac{1}{1-\delta}\left[\frac{(\delta^{-t_l}-1)e^{-v_l}}{v_l} -\frac{(\delta^{-t_s}-1)e^{-v_s}}{v_s}\right]
$$

Combine Equation (A.5) and (A.6), we have

$$
\delta^{-t_l}\frac{e^{-v_l}}{v_l}<\delta^{-t_s}\frac{e^{-v_s}}{v_s} \Longrightarrow v_l - v_s + \ln \left(\frac{v_l}{v_s}\right)>-(t_l-t_s)\ln\delta
$$

## Proof of magnitude effect

Given that $G(t_1)e^{-v(x_1)/\lambda}=v(x_1)/(1+b)-1$, $$
\frac{\partial V}{\partial x_1}x_1
=(1+b)(v(x_1)+b) \frac{v'(x_1)}{v(x_1)}x_1
$$ Observing that $x_2\cdot \partial V/\partial x_2$ admits a similar representation, we can define a function $\psi(x)$: $$
\psi(x) = (v(x)+b) \frac{v'(x)}{v(x)}x=xv'(x)+bE_{vx}
$$

(Lama 1) for any $x_1<x_2<\infty$, there always exists $t_1<t_2<\infty$ that makes **eq. (2)** holds.

Thus, if $\psi'(x)>0$, then $\psi(x_2)>\psi(x_1)$ for any , $x_1<x_2<\infty$. The inequality can be derived from $\psi'(x)>0$.

## Proof of inconsistent planning

Given that $w_t(s) \propto \delta^t e^{u_t(s)/\lambda}$, we have $E_s[w_tu_t] \propto \delta^t \xi(x_t,\sigma_t)$, where $$
\xi(x_t,\sigma_t) \equiv (\frac{v(x_t)}{\lambda}+\sigma_t^2)\exp\left\{\frac{v(x_t)}{\lambda}+\frac{\sigma_t^2}{2}\right\}
$$

Note that if $z \sim N(\mu,\sigma)$, then $E[ze^z] = (\mu+\sigma^2)\exp\{\mu+\sigma^2/2\}$

$\xi(.)$ should be a concave function to $x_t$; therefore, $$
v'(x_t)\left[1+\sigma_t^2+\frac{v(x_t)}{\lambda}\right]
\exp\left\{\frac{v(x_t)}{\lambda}+\frac{\sigma_t^2}{2}\right\}
$$ is decreasing with $x_t$.

At period 0, the decision maker allocate her total rewards to solve the optimization problem $$
\max_\textbf{x} \sum_{t=0}^T \delta^t\cdot\xi(x_t,\sigma_t)\quad s.t. \sum_{t=0}^T x_t = m
$$ by FOC, we have $$
\frac{\partial \xi}{\partial x_t} = \delta\frac{\partial \xi}{\partial x_{t+1}}
$$ i.e. $$
\frac{v'(x_t)}{v'(x_{t+1})} = \rho\delta
$$ where $$
\rho = \frac{1+\sigma_{t+1}+v(x_{t+1})/\lambda}{1+\sigma_t+v(x_t)/\lambda}
\exp\left\{ \frac{v(x_{t+1})-v(x_t)}{\lambda} + \frac{\sigma_{t+1} - \sigma_t}{2}\right\}
$$ When $\rho$ is constant with $t$, the decision maker performs no planning fallacy. When $\beta$ is (weakly) increasing with $t$ and there exists an interval $[0,\bar{t}]$ such that $\beta$ is strictly increasing with $t$, the decision maker performs planning fallacy and is present-biased.

## Proof of intertemporal correlation aversion

## 

Meanwhile, by definition, $D_{KL}$ is increasing in $w(s_t|s)$ and is convex. To solve the model, one can simply set a Lagrange multiplier $\gamma$, then construct the FOC condition that, for any $s \in X$ and $t \in \{0,1,â€¦,T\}$, $\frac{\partial D_{KL}}{\partial w(s_t|s)} = u(s_t) + \gamma$. Given that $u'>0$ and the convexity of $D_{KL}$ ensures its first derivative increasing in $w(s_t|s)$, the solution of $w(s_t|s)$ should be increasing with $s_t$. This enables the decision maker to perform greater patience for a larger reward.

While building the model, I was mainly inspired by the theories of rational inattention [@matejka_rational_2015; @jung_discrete_2019; @mackowiak_rational_2023]. In @matejka_rational_2015's theory of rational inattention, the decision maker makes choices between discrete alternatives; she evaluates each alternative via a costly information acquisition process, then decides the optimal choice strategy. The theory deduces the probability of each alternative being chosen should follow a logistic-like distribution. In ADU, I assume the discounting factors are generated by a similar process; hence, she subjectively weights each time period according to a logistic-like distribution as well.

The reason why I use Shannon mutual information as the cognitive cost function is twofold. First, note that $w(s_t|s) \propto w^0_t e^{u(s_t)/\lambda}$. Given a certain stream $s$ and two time periods $t_1$ and $t_2$ ($t_2>t_1$), the relative weight between them $\frac{w(s_{t_1}|s)}{w(s_{t_2}|s)}$ is only relevant to $s_{t_1}$ and $s_{t_2}$. Therefore, changing the reward of a third period has no impact on how the reward in $t_2$ should be discounted relative to that in $t_1$. Second, under such settings, the objective function can be rewritten as$$
\sum_{s\in X} p(s)[w(s_t|s)u(s_t) - \lambda D_{KL}]
$$

where $D_{KL}$ is the KL divergence between the initial weights over time periods and the weights updated given the stream $s$ is drawn. Clearly, the determination of $w(s_t|s)$ in each $s$ can be separated from each other. In other words, given two potentially realized streams $s$ and $s'$, the changes in $s'$ has no impact on the determination of discounting factors in $s$. This property is consistent with many forms of optimal sequential learning [@caplin_rationally_2022]. @matejka_rational_2015 show that the two properties are jointly satisfied if and only if the solution of $w(s_t|s)$ follows Equation (1).

Suppose $p(s)$ also equals to the true probability that $s$ is realized. After decides the $w(s_t|s)$, the decision maker can obtain the discounted utility (DU) of each potentially realized stream $s$. I assume she wants to find a risky reward stream, denoted by $p$, that maximizes her expected discounted utility. Therefore,$$
p = \arg\max_{p\in P} \left\{\sum_{s\in X} \sum_{t=0}^T p(s)w(s_t|s)u(s_t)\right\}
$$

Given that the discounting factor $w(s_t|s)$ is formed by an attention-adjusted evaluation procedure. I term the model alike as attention-adjusted DU (ADU). ADU suggests that the so-called discounting factors are the attention weights that decision makers assign to each time period.

The limited attention and costly attention adjustment can be characterized by intertemporal correlation aversion and magnitude-increasing patience. The rationale for using Shannon mutual information as the cognitive cost function is that, under a certain state, the discounting factor in a certain period should be independent from irrelevant periods and irrelevant states.
