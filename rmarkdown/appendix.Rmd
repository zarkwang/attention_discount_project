---
title: "Appendix"
author: "Zark Wang"
date: "`r Sys.Date()`"
output: html_document
---

## Proof of common difference effect

From the definition of $\alpha$, $$
\alpha \cdot(1+G(t_1)\cdot e^{-u}) = 1+G(t_2)\cdot e^{-\alpha u} \tag{1}
$$ Set up a function $$
f(\Delta t) = \alpha\cdot(1+G(t_1+\Delta t)\cdot e^{-u}) - (1+G(t_2+\Delta t)\cdot e^{-\alpha u})
$$ We know that $f(0)=0$

common difference effect implies $f(\Delta t)>0$ when $\Delta t>0$

if $f'(\Delta t) >0$ then $$
\frac{G'(t_2+\Delta t)}{G'(t_1+\Delta t)} < \alpha e^{(\alpha-1)u}
$$ when $\delta=1$, the right hand is 1, the common difference effect always holds

when $0<\delta<1$, rewrite $f(\Delta t)>0$: $$
\delta^{-\Delta t}(\delta^{-t_1}\alpha e^{-u}-\delta^{-t_2}e^{-\alpha u}) >
\alpha e^{-u}- e^{-\alpha u} -(1-\delta)(\alpha-1)$$ from eq. (1) we know the the left hand equals $\delta^{-t_1}\alpha e^{-u}-\delta^{-t_2}e^{-\alpha u}$. Therefore, the inequality always holds when $\Delta t>0$

## Proof of magnitude effect

Given that $G(t_1)e^{-v(x_1)/\lambda}=v(x_1)/(1+b)-1$, $$
\frac{\partial V}{\partial x_1}x_1
=(1+b)(v(x_1)+b) \frac{v'(x_1)}{v(x_1)}x_1
$$ Observing that $x_2\cdot \partial V/\partial x_2$ admits a similar representation, we can define a function $\psi(x)$: $$
\psi(x) = (v(x)+b) \frac{v'(x)}{v(x)}x=xv'(x)+bE_{vx}
$$

(Lama 1) for any $x_1<x_2<\infty$, there always exists $t_1<t_2<\infty$ that makes **eq. (2)** holds.

Thus, if $\psi'(x)>0$, then $\psi(x_2)>\psi(x_1)$ for any , $x_1<x_2<\infty$. The inequality can be derived from $\psi'(x)>0$.

## Proof of inconsistent planning

Given that $w_t(s) \propto \delta^t e^{u_t(s)/\lambda}$, we have $E_s[w_tu_t] \propto \delta^t \xi(x_t,\sigma_t)$, where $$
\xi(x_t,\sigma_t) \equiv (\frac{v(x_t)}{\lambda}+\sigma_t^2)\exp\left\{\frac{v(x_t)}{\lambda}+\frac{\sigma_t^2}{2}\right\}
$$

Note that if $z \sim N(\mu,\sigma)$, then $E[ze^z] = (\mu+\sigma^2)\exp\{\mu+\sigma^2/2\}$

$\xi(.)$ should be a concave function to $x_t$; therefore, $$
v'(x_t)\left[1+\sigma_t^2+\frac{v(x_t)}{\lambda}\right]
\exp\left\{\frac{v(x_t)}{\lambda}+\frac{\sigma_t^2}{2}\right\}
$$ is decreasing with $x_t$.

At period 0, the decision maker allocate her total rewards to solve the optimization problem $$
\max_\textbf{x} \sum_{t=0}^T \delta^t\cdot\xi(x_t,\sigma_t)\quad s.t. \sum_{t=0}^T x_t = m
$$ by FOC, we have $$
\frac{\partial \xi}{\partial x_t} = \delta\frac{\partial \xi}{\partial x_{t+1}}
$$ i.e. $$
\frac{v'(x_t)}{v'(x_{t+1})} = \rho\delta
$$ where $$
\rho = \frac{1+\sigma_{t+1}+v(x_{t+1})/\lambda}{1+\sigma_t+v(x_t)/\lambda}
\exp\left\{ \frac{v(x_{t+1})-v(x_t)}{\lambda} + \frac{\sigma_{t+1} - \sigma_t}{2}\right\}
$$ When $\rho$ is constant with $t$, the decision maker performs no planning fallacy. When $\beta$ is (weakly) increasing with $t$ and there exists an interval $[0,\bar{t}]$ such that $\beta$ is strictly increasing with $t$, the decision maker performs planning fallacy and is present-biased.

## Proof of intertemporal correlation aversion

## 

Meanwhile, by definition, $D_{KL}$ is increasing in $w(s_t|s)$ and is convex. To solve the model, one can simply set a Lagrange multiplier $\gamma$, then construct the FOC condition that, for any $s \in X$ and $t \in \{0,1,â€¦,T\}$, $\frac{\partial D_{KL}}{\partial w(s_t|s)} = u(s_t) + \gamma$. Given that $u'>0$ and the convexity of $D_{KL}$ ensures its first derivative increasing in $w(s_t|s)$, the solution of $w(s_t|s)$ should be increasing with $s_t$. This enables the decision maker to perform greater patience for a larger reward.

I compare the predictive performance of ADU model with other 10 intertemporal choice models on two datasets. The datasets are originally used in two previous studies: @ericson_money_2015 and @chavez_hierarchical_2017. Each of the studies recruits \~1,000 participants to do the classical "Money Eariler or Later" (MEL, choosing between a small sooner reward and a large later reward) tasks. I randomly draw the responses from 20% of the participants as the test sample, and set the rest as the train sample. After fitting the train sample, I use these models to predict the responses on the test sample. A machine learning algorithm, namely XGBoost [@chen_xgboost_2016], outperforms the other models in out-of-sample predictive accuracy. The ADU model and the intertemporal trade-off model [@scholten_weighing_2014] rank either the second or the third (dependent on the dataset and empirical strategy), and their performance is very close. Each of the two models can accurately predict about 95% of the simulated responses generated by XGBoost. Given that ADU has only one additional parameter compared with the standard exponential discounting, while the intertemporal trade-off model has three additional parameters, researchers who want to save the number of parameters can take ADU as a candidate in model fitting.
