{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from mpl import cross_valid\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "itch_dt = pd.read_csv('data/ericson_data.csv')\n",
    "itch_dt = itch_dt.rename(columns={\"Subject\":\"person_id\",\n",
    "                        \"Condition\":\"condition\",\n",
    "                        \"Question\":\"question_id\",\n",
    "                        \"X1\":\"ss_x\",\n",
    "                        \"T1\":\"ss_t\",\n",
    "                        \"X2\":\"ll_x\",\n",
    "                        \"T2\":\"ll_t\",\n",
    "                        \"LaterOptionChosen\": \"choice\"}).drop(['R','G','D'],axis=1)\n",
    "\n",
    "dataset = cross_valid.generate_sample(itch_dt)\n",
    "\n",
    "# Define features and label\n",
    "features = ['ss_x', 'ss_t', 'll_x', 'll_t',\n",
    "            'abs_diff_x', 'abs_diff_t', 'rel_diff_x','rel_diff_t','growth_x']\n",
    "label = 'choice'\n",
    "X = dataset[features]\n",
    "y = dataset[label]\n",
    "\n",
    "# Split the data into train sample and test sample \n",
    "# Train sample containts 80% of the participants, test sample contains the rest \n",
    "groups = dataset['person_id']\n",
    "train_index,test_index = list(model_selection.GroupShuffleSplit(n_splits=1,train_size=.8,random_state=2023).\n",
    "                              split(X,y,groups))[0]\n",
    "train_sample = dataset[dataset.index.isin(train_index)]\n",
    "test_sample = dataset[dataset.index.isin(test_index)]\n",
    "\n",
    "# Split the train sample into K folds (K=10) for cross-validation\n",
    "sgkf = model_selection.StratifiedGroupKFold(n_splits=10,shuffle=True,random_state=2023)\n",
    "cv = list(sgkf.split(X=train_sample[features],\n",
    "                y=train_sample[label],\n",
    "                groups=train_sample['person_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dataset['person_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=[(array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   50,    51,    52, ..., 18085, 18086, 18087])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  167,   168,   169, ..., 17828, 17829, 17830])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  192,   193,   194, ..., 18127, 18128, 18129])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   74,    75,    76, ..., 17729, 17730, 17731])),\n",
       "                 (array([    0,     1...\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [1.0], &#x27;gamma&#x27;: [0.3],\n",
       "                         &#x27;learning_rate&#x27;: [0.1], &#x27;max_depth&#x27;: [3],\n",
       "                         &#x27;n_estimators&#x27;: [40], &#x27;reg_lambda&#x27;: [0.7],\n",
       "                         &#x27;subsample&#x27;: [0.55]},\n",
       "             refit=&#x27;neg_log_loss&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;neg_log_loss&#x27;, &#x27;neg_mean_absolute_error&#x27;,\n",
       "                      &#x27;neg_mean_squared_error&#x27;],\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=[(array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   50,    51,    52, ..., 18085, 18086, 18087])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  167,   168,   169, ..., 17828, 17829, 17830])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  192,   193,   194, ..., 18127, 18128, 18129])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   74,    75,    76, ..., 17729, 17730, 17731])),\n",
       "                 (array([    0,     1...\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [1.0], &#x27;gamma&#x27;: [0.3],\n",
       "                         &#x27;learning_rate&#x27;: [0.1], &#x27;max_depth&#x27;: [3],\n",
       "                         &#x27;n_estimators&#x27;: [40], &#x27;reg_lambda&#x27;: [0.7],\n",
       "                         &#x27;subsample&#x27;: [0.55]},\n",
       "             refit=&#x27;neg_log_loss&#x27;,\n",
       "             scoring=[&#x27;accuracy&#x27;, &#x27;neg_log_loss&#x27;, &#x27;neg_mean_absolute_error&#x27;,\n",
       "                      &#x27;neg_mean_squared_error&#x27;],\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=[(array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   50,    51,    52, ..., 18085, 18086, 18087])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  167,   168,   169, ..., 17828, 17829, 17830])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([  192,   193,   194, ..., 18127, 18128, 18129])),\n",
       "                 (array([    0,     1,     2, ..., 18182, 18183, 18184]),\n",
       "                  array([   74,    75,    76, ..., 17729, 17730, 17731])),\n",
       "                 (array([    0,     1...\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [1.0], 'gamma': [0.3],\n",
       "                         'learning_rate': [0.1], 'max_depth': [3],\n",
       "                         'n_estimators': [40], 'reg_lambda': [0.7],\n",
       "                         'subsample': [0.55]},\n",
       "             refit='neg_log_loss',\n",
       "             scoring=['accuracy', 'neg_log_loss', 'neg_mean_absolute_error',\n",
       "                      'neg_mean_squared_error'],\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use XGBoost to fit the data\n",
    "# Tune the hyer-parameters by grid search \n",
    "param_grid = {'n_estimators': [40],\n",
    "              'max_depth': [3],\n",
    "              'learning_rate': [.1],\n",
    "              'gamma': [.3],\n",
    "              'reg_lambda': [.7],\n",
    "              'subsample': [.55],\n",
    "              'colsample_bytree': [1.0]\n",
    "            }\n",
    "\n",
    "model = xgb.XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "grid_search = model_selection.GridSearchCV(model, param_grid, cv=cv, \n",
    "                                           scoring=[\"accuracy\",\"neg_log_loss\",'neg_mean_absolute_error','neg_mean_squared_error'], \n",
    "                                           refit=\"neg_log_loss\",\n",
    "                                           n_jobs=-1,verbose=3)\n",
    "\n",
    "grid_search.fit(X=train_sample[features], \n",
    "                y=train_sample[label], \n",
    "                groups=train_sample['person_id'])\n",
    "\n",
    "# model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "#                           max_depth=3,\n",
    "#                           learning_rate=.1,\n",
    "#                           gamma=.3,\n",
    "#                           reg_lambda=.7,\n",
    "#                           subsample=.6,\n",
    "#                           colsample_bytree=1.0,\n",
    "#                           eval_metric=['error','logloss'],\n",
    "#                           early_stopping_rounds=30)\n",
    "# eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "# bst = model.fit(X=X_train,\n",
    "#                 y=y_train,\n",
    "#                 eval_set=eval_set,\n",
    "#                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 1.0, 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 40, 'reg_lambda': 0.7, 'subsample': 0.55}\n",
      "{'dstyle': 'gbdt', 'ustyle': 'gbdt', 'params': None, 'accuracy': 0.7011608722299743, 'log_loss': 0.5811911011008668, 'mse': 0.2988391277700258, 'mae': 0.2988391277700258}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the XGBoost model and check the feature importance\n",
    "gbdt_result = {'dstyle': 'gbdt',\n",
    "            'ustyle': 'gbdt',\n",
    "            'params': None,\n",
    "            'accuracy': grid_search.cv_results_['mean_test_accuracy'][0],\n",
    "            'log_loss': -grid_search.cv_results_['mean_test_neg_log_loss'][0],\n",
    "            'mse':-grid_search.cv_results_['mean_test_neg_mean_squared_error'][0],\n",
    "            'mae':-grid_search.cv_results_['mean_test_neg_mean_absolute_error'][0]\n",
    "            }\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(gbdt_result)\n",
    "\n",
    "bst_model = grid_search.best_estimator_\n",
    "xgb.plot_importance(bst_model)\n",
    "with open('my_model.pkl', 'wb') as f:\n",
    "    pickle.dump(bst_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a decision tree\n",
    "# Change num_trees from 0 to 39, check different trees\n",
    "xgb.to_graphviz(bst_model, num_trees=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.29965080750763856,\n",
       " 'mae': 0.29965080750763856,\n",
       " 'log_loss': 10.800509843574387,\n",
       " 'accuracy': 0.7003491924923614,\n",
       " 'pred_ll': 0.26473155827149714}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The out-of-sample performance of XGBoost model\n",
    "with open('my_model.pkl', 'rb') as f:\n",
    "    heuristic_model = pickle.load(f)\n",
    "\n",
    "preds = heuristic_model.predict(test_sample[features])\n",
    "trues = test_sample[label]\n",
    "\n",
    "pred_binary = (preds > .5)\n",
    "\n",
    "test_heuristic_dict = {'mse': metrics.mean_squared_error(trues, preds),\n",
    "                       'mae': metrics.mean_absolute_error(trues, preds),\n",
    "                       'log_loss': metrics.log_loss(trues, preds),\n",
    "                       'accuracy': metrics.accuracy_score(trues, pred_binary),\n",
    "                       'pred_ll':sum(pred_binary)/len(pred_binary)\n",
    "                       }\n",
    "test_heuristic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [42:35<00:00, 11.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fit data by distounted utility model and trade-off model\n",
    "dstyle_list = list(cross_valid.estimation.config_param['discount_func'].keys())\n",
    "ustyle_list = list(cross_valid.estimation.config_param['utility_func'].keys())\n",
    "style_list = [{\"dstyle\":dstyle_list[i],\n",
    "               \"ustyle\":ustyle_list[j],\n",
    "               \"method\":'logit',\n",
    "               \"intercept\":False} \n",
    "              for i in range(len(dstyle_list)) for j in range(len(ustyle_list))]\n",
    "\n",
    "kf = cross_valid.KFvalidation(style=style_list,data=train_sample,cv=cv,n_jobs=4)\n",
    "kf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dstyle</th>\n",
       "      <th>ustyle</th>\n",
       "      <th>params</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>gbdt</td>\n",
       "      <td>None</td>\n",
       "      <td>0.298839</td>\n",
       "      <td>0.298839</td>\n",
       "      <td>0.581191</td>\n",
       "      <td>0.701161</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>trade</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.773, 1.128, 6.401, 0.409, 0.092, 0.079]</td>\n",
       "      <td>0.201905</td>\n",
       "      <td>0.403546</td>\n",
       "      <td>0.591444</td>\n",
       "      <td>0.694859</td>\n",
       "      <td>0.228946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>power</td>\n",
       "      <td>[6.751, 0.149, 0.182]</td>\n",
       "      <td>0.204222</td>\n",
       "      <td>0.409320</td>\n",
       "      <td>0.597593</td>\n",
       "      <td>0.694343</td>\n",
       "      <td>0.228360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.996, 0.997, 0.904, 0.009, 0.007]</td>\n",
       "      <td>0.207360</td>\n",
       "      <td>0.415097</td>\n",
       "      <td>0.603642</td>\n",
       "      <td>0.688867</td>\n",
       "      <td>0.239352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.997, 0.997, 0.007, 0.005]</td>\n",
       "      <td>0.207397</td>\n",
       "      <td>0.414831</td>\n",
       "      <td>0.603718</td>\n",
       "      <td>0.688867</td>\n",
       "      <td>0.239352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expo2</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.913, 0.752, 0.646, 0.01, 0.008]</td>\n",
       "      <td>0.207383</td>\n",
       "      <td>0.415668</td>\n",
       "      <td>0.603775</td>\n",
       "      <td>0.685805</td>\n",
       "      <td>0.244356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>power</td>\n",
       "      <td>[1.298, 0.176, 0.326]</td>\n",
       "      <td>0.208835</td>\n",
       "      <td>0.418370</td>\n",
       "      <td>0.607155</td>\n",
       "      <td>0.683527</td>\n",
       "      <td>0.152511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hb2</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.152, 2.614, 0.015, 0.013]</td>\n",
       "      <td>0.209411</td>\n",
       "      <td>0.418613</td>\n",
       "      <td>0.608559</td>\n",
       "      <td>0.684611</td>\n",
       "      <td>0.249073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hb</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.012, 0.027, 0.024]</td>\n",
       "      <td>0.209737</td>\n",
       "      <td>0.419905</td>\n",
       "      <td>0.609231</td>\n",
       "      <td>0.683284</td>\n",
       "      <td>0.251767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hce</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.993, 2.117, 0.01, 0.013]</td>\n",
       "      <td>0.210006</td>\n",
       "      <td>0.420338</td>\n",
       "      <td>0.609677</td>\n",
       "      <td>0.683284</td>\n",
       "      <td>0.251767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expo</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.993, 0.015, 0.013]</td>\n",
       "      <td>0.210007</td>\n",
       "      <td>0.420338</td>\n",
       "      <td>0.609678</td>\n",
       "      <td>0.683284</td>\n",
       "      <td>0.251767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>power</td>\n",
       "      <td>[0.993, 5.233, 0.302, 0.995]</td>\n",
       "      <td>0.213170</td>\n",
       "      <td>0.427823</td>\n",
       "      <td>0.617921</td>\n",
       "      <td>0.678237</td>\n",
       "      <td>0.164364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>trade</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.732, 2.2, 88.333, 5.634, 0.003, 0.184]</td>\n",
       "      <td>0.216043</td>\n",
       "      <td>0.432103</td>\n",
       "      <td>0.623241</td>\n",
       "      <td>0.672878</td>\n",
       "      <td>0.116715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.83, 0.738, 2.276, 0.464]</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.454947</td>\n",
       "      <td>0.647333</td>\n",
       "      <td>0.634828</td>\n",
       "      <td>0.072882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.793, 2.886, 0.395]</td>\n",
       "      <td>0.227811</td>\n",
       "      <td>0.455306</td>\n",
       "      <td>0.647701</td>\n",
       "      <td>0.633779</td>\n",
       "      <td>0.081843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>cara</td>\n",
       "      <td>[3.242, 2.767, 0.409]</td>\n",
       "      <td>0.227841</td>\n",
       "      <td>0.455367</td>\n",
       "      <td>0.647780</td>\n",
       "      <td>0.634131</td>\n",
       "      <td>0.080581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expo2</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.316, 0.47, 0.413, 2.767, 0.484]</td>\n",
       "      <td>0.227970</td>\n",
       "      <td>0.455689</td>\n",
       "      <td>0.648017</td>\n",
       "      <td>0.632251</td>\n",
       "      <td>0.067494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.928, 0.78, 2.753, 0.486]</td>\n",
       "      <td>0.227976</td>\n",
       "      <td>0.455673</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>0.631976</td>\n",
       "      <td>0.067510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.928, 0.78, 1.44, 2.753, 0.486]</td>\n",
       "      <td>0.227976</td>\n",
       "      <td>0.455673</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>0.631976</td>\n",
       "      <td>0.067510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expo</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.725, 2.516, 0.544]</td>\n",
       "      <td>0.228005</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.648082</td>\n",
       "      <td>0.632555</td>\n",
       "      <td>0.049998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hce</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.725, 95.419, 2.54, 0.545]</td>\n",
       "      <td>0.228011</td>\n",
       "      <td>0.455791</td>\n",
       "      <td>0.648093</td>\n",
       "      <td>0.632555</td>\n",
       "      <td>0.049998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hb2</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.077, 4.897, 2.676, 0.503]</td>\n",
       "      <td>0.228058</td>\n",
       "      <td>0.455817</td>\n",
       "      <td>0.648200</td>\n",
       "      <td>0.632517</td>\n",
       "      <td>0.062955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hb</td>\n",
       "      <td>cara</td>\n",
       "      <td>[0.317, 3.258, 0.419]</td>\n",
       "      <td>0.228190</td>\n",
       "      <td>0.456115</td>\n",
       "      <td>0.648485</td>\n",
       "      <td>0.632283</td>\n",
       "      <td>0.073486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dstyle ustyle                                      params  \\\n",
       "99           gbdt   gbdt                                        None   \n",
       "21          trade  power  [0.773, 1.128, 6.401, 0.409, 0.092, 0.079]   \n",
       "13           hbmd  power                       [6.751, 0.149, 0.182]   \n",
       "19     quasihb_fc  power         [0.996, 0.997, 0.904, 0.009, 0.007]   \n",
       "17        quasihb  power                [0.997, 0.997, 0.007, 0.005]   \n",
       "7           expo2  power          [0.913, 0.752, 0.646, 0.01, 0.008]   \n",
       "3   attention_uni  power                       [1.298, 0.176, 0.326]   \n",
       "11            hb2  power                [0.152, 2.614, 0.015, 0.013]   \n",
       "9              hb  power                       [0.012, 0.027, 0.024]   \n",
       "15            hce  power                 [0.993, 2.117, 0.01, 0.013]   \n",
       "5            expo  power                       [0.993, 0.015, 0.013]   \n",
       "1       attention  power                [0.993, 5.233, 0.302, 0.995]   \n",
       "20          trade   cara   [0.732, 2.2, 88.333, 5.634, 0.003, 0.184]   \n",
       "0       attention   cara                 [0.83, 0.738, 2.276, 0.464]   \n",
       "2   attention_uni   cara                       [0.793, 2.886, 0.395]   \n",
       "12           hbmd   cara                       [3.242, 2.767, 0.409]   \n",
       "6           expo2   cara          [0.316, 0.47, 0.413, 2.767, 0.484]   \n",
       "16        quasihb   cara                 [0.928, 0.78, 2.753, 0.486]   \n",
       "18     quasihb_fc   cara           [0.928, 0.78, 1.44, 2.753, 0.486]   \n",
       "4            expo   cara                       [0.725, 2.516, 0.544]   \n",
       "14            hce   cara                [0.725, 95.419, 2.54, 0.545]   \n",
       "10            hb2   cara                [0.077, 4.897, 2.676, 0.503]   \n",
       "8              hb   cara                       [0.317, 3.258, 0.419]   \n",
       "\n",
       "         mse       mae  log_loss  accuracy   pred_ll  \n",
       "99  0.298839  0.298839  0.581191  0.701161       NaN  \n",
       "21  0.201905  0.403546  0.591444  0.694859  0.228946  \n",
       "13  0.204222  0.409320  0.597593  0.694343  0.228360  \n",
       "19  0.207360  0.415097  0.603642  0.688867  0.239352  \n",
       "17  0.207397  0.414831  0.603718  0.688867  0.239352  \n",
       "7   0.207383  0.415668  0.603775  0.685805  0.244356  \n",
       "3   0.208835  0.418370  0.607155  0.683527  0.152511  \n",
       "11  0.209411  0.418613  0.608559  0.684611  0.249073  \n",
       "9   0.209737  0.419905  0.609231  0.683284  0.251767  \n",
       "15  0.210006  0.420338  0.609677  0.683284  0.251767  \n",
       "5   0.210007  0.420338  0.609678  0.683284  0.251767  \n",
       "1   0.213170  0.427823  0.617921  0.678237  0.164364  \n",
       "20  0.216043  0.432103  0.623241  0.672878  0.116715  \n",
       "0   0.227636  0.454947  0.647333  0.634828  0.072882  \n",
       "2   0.227811  0.455306  0.647701  0.633779  0.081843  \n",
       "12  0.227841  0.455367  0.647780  0.634131  0.080581  \n",
       "6   0.227970  0.455689  0.648017  0.632251  0.067494  \n",
       "16  0.227976  0.455673  0.648034  0.631976  0.067510  \n",
       "18  0.227976  0.455673  0.648034  0.631976  0.067510  \n",
       "4   0.228005  0.455782  0.648082  0.632555  0.049998  \n",
       "14  0.228011  0.455791  0.648093  0.632555  0.049998  \n",
       "10  0.228058  0.455817  0.648200  0.632517  0.062955  \n",
       "8   0.228190  0.456115  0.648485  0.632283  0.073486  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare models: Cross-validation result\n",
    "kf_result_df = kf.summary()\n",
    "kf_result_df['dstyle'] = kf_result_df['style'].apply(lambda x:x['dstyle'])\n",
    "kf_result_df['ustyle'] = kf_result_df['style'].apply(lambda x:x['ustyle'])\n",
    "df_cols = kf_result_df.columns.tolist()\n",
    "df_cols = df_cols[-2:] + df_cols[:-2]\n",
    "kf_result = kf_result_df.reindex(columns=df_cols).drop('style',axis=1)\n",
    "kf_result = pd.concat([kf_result,pd.DataFrame(gbdt_result,index=[99])]).sort_values('log_loss')\n",
    "kf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dstyle</th>\n",
       "      <th>ustyle</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>power</td>\n",
       "      <td>0.204391</td>\n",
       "      <td>0.407377</td>\n",
       "      <td>0.598332</td>\n",
       "      <td>0.700786</td>\n",
       "      <td>0.213226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>gbdt</td>\n",
       "      <td>gbdt</td>\n",
       "      <td>0.299651</td>\n",
       "      <td>0.299651</td>\n",
       "      <td>10.800510</td>\n",
       "      <td>0.700349</td>\n",
       "      <td>0.264732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>trade</td>\n",
       "      <td>power</td>\n",
       "      <td>0.202046</td>\n",
       "      <td>0.402179</td>\n",
       "      <td>0.591884</td>\n",
       "      <td>0.697730</td>\n",
       "      <td>0.210607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.208359</td>\n",
       "      <td>0.418450</td>\n",
       "      <td>0.606683</td>\n",
       "      <td>0.690310</td>\n",
       "      <td>0.245089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>power</td>\n",
       "      <td>0.206735</td>\n",
       "      <td>0.416692</td>\n",
       "      <td>0.602913</td>\n",
       "      <td>0.689873</td>\n",
       "      <td>0.283937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.209022</td>\n",
       "      <td>0.400055</td>\n",
       "      <td>0.610947</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.205587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expo</td>\n",
       "      <td>power</td>\n",
       "      <td>0.208985</td>\n",
       "      <td>0.415752</td>\n",
       "      <td>0.608228</td>\n",
       "      <td>0.681362</td>\n",
       "      <td>0.205587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hce</td>\n",
       "      <td>power</td>\n",
       "      <td>0.209189</td>\n",
       "      <td>0.416087</td>\n",
       "      <td>0.608668</td>\n",
       "      <td>0.681362</td>\n",
       "      <td>0.205587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>power</td>\n",
       "      <td>0.210806</td>\n",
       "      <td>0.419025</td>\n",
       "      <td>0.611358</td>\n",
       "      <td>0.679834</td>\n",
       "      <td>0.149062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>power</td>\n",
       "      <td>0.215893</td>\n",
       "      <td>0.428781</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.668485</td>\n",
       "      <td>0.124618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>trade</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.219191</td>\n",
       "      <td>0.432637</td>\n",
       "      <td>0.629829</td>\n",
       "      <td>0.663029</td>\n",
       "      <td>0.107813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.230188</td>\n",
       "      <td>0.457046</td>\n",
       "      <td>0.652845</td>\n",
       "      <td>0.631820</td>\n",
       "      <td>0.070930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.230261</td>\n",
       "      <td>0.457356</td>\n",
       "      <td>0.653074</td>\n",
       "      <td>0.630729</td>\n",
       "      <td>0.076386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.230354</td>\n",
       "      <td>0.457459</td>\n",
       "      <td>0.653297</td>\n",
       "      <td>0.630074</td>\n",
       "      <td>0.076168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.230921</td>\n",
       "      <td>0.458206</td>\n",
       "      <td>0.654390</td>\n",
       "      <td>0.628110</td>\n",
       "      <td>0.054561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.230921</td>\n",
       "      <td>0.458206</td>\n",
       "      <td>0.654390</td>\n",
       "      <td>0.628110</td>\n",
       "      <td>0.054561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.231016</td>\n",
       "      <td>0.458451</td>\n",
       "      <td>0.654648</td>\n",
       "      <td>0.627455</td>\n",
       "      <td>0.065692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hb2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.231680</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>0.656225</td>\n",
       "      <td>0.624836</td>\n",
       "      <td>0.039939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hce</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.231008</td>\n",
       "      <td>0.458273</td>\n",
       "      <td>0.654512</td>\n",
       "      <td>0.624182</td>\n",
       "      <td>0.041030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expo</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.231010</td>\n",
       "      <td>0.458207</td>\n",
       "      <td>0.654517</td>\n",
       "      <td>0.624182</td>\n",
       "      <td>0.041030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expo2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.240588</td>\n",
       "      <td>0.448390</td>\n",
       "      <td>0.681933</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.008293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hb2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.383232</td>\n",
       "      <td>0.383238</td>\n",
       "      <td>8.854142</td>\n",
       "      <td>0.616761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expo2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.383238</td>\n",
       "      <td>0.383239</td>\n",
       "      <td>9.356921</td>\n",
       "      <td>0.616761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dstyle ustyle       mse       mae   log_loss  accuracy   pred_ll\n",
       "13           hbmd  power  0.204391  0.407377   0.598332  0.700786  0.213226\n",
       "99           gbdt   gbdt  0.299651  0.299651  10.800510  0.700349  0.264732\n",
       "21          trade  power  0.202046  0.402179   0.591884  0.697730  0.210607\n",
       "9              hb  power  0.208359  0.418450   0.606683  0.690310  0.245089\n",
       "19     quasihb_fc  power  0.206735  0.416692   0.602913  0.689873  0.283937\n",
       "17        quasihb  power  0.209022  0.400055   0.610947  0.689655  0.205587\n",
       "5            expo  power  0.208985  0.415752   0.608228  0.681362  0.205587\n",
       "15            hce  power  0.209189  0.416087   0.608668  0.681362  0.205587\n",
       "3   attention_uni  power  0.210806  0.419025   0.611358  0.679834  0.149062\n",
       "1       attention  power  0.215893  0.428781   0.624156  0.668485  0.124618\n",
       "20          trade   cara  0.219191  0.432637   0.629829  0.663029  0.107813\n",
       "0       attention   cara  0.230188  0.457046   0.652845  0.631820  0.070930\n",
       "2   attention_uni   cara  0.230261  0.457356   0.653074  0.630729  0.076386\n",
       "12           hbmd   cara  0.230354  0.457459   0.653297  0.630074  0.076168\n",
       "18     quasihb_fc   cara  0.230921  0.458206   0.654390  0.628110  0.054561\n",
       "16        quasihb   cara  0.230921  0.458206   0.654390  0.628110  0.054561\n",
       "8              hb   cara  0.231016  0.458451   0.654648  0.627455  0.065692\n",
       "10            hb2   cara  0.231680  0.455400   0.656225  0.624836  0.039939\n",
       "14            hce   cara  0.231008  0.458273   0.654512  0.624182  0.041030\n",
       "4            expo   cara  0.231010  0.458207   0.654517  0.624182  0.041030\n",
       "6           expo2   cara  0.240588  0.448390   0.681933  0.620253  0.008293\n",
       "11            hb2  power  0.383232  0.383238   8.854142  0.616761  0.000000\n",
       "7           expo2  power  0.383238  0.383239   9.356921  0.616761  0.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare models: Out-of-sample performance\n",
    "test_result = []\n",
    "\n",
    "for i in range(len(kf_result_df)):\n",
    "\n",
    "    test_style = kf_result_df['style'][i]\n",
    "    test_params = kf_result_df['params'][i]\n",
    "\n",
    "    test_scores = cross_valid.test_model(test_sample=test_sample,style=test_style,params=test_params)\n",
    "    test_scores['dstyle'] = test_style['dstyle'] \n",
    "    test_scores['ustyle'] = test_style['ustyle']\n",
    "    test_result.append(test_scores)\n",
    "\n",
    "test_result = pd.DataFrame(test_result)\n",
    "test_result = test_result.reindex(columns=df_cols).sort_values('log_loss').drop(['style','params'],axis=1)\n",
    "\n",
    "test_heuristic_dict['dstyle'] = 'gbdt'\n",
    "test_heuristic_dict['ustyle'] = 'gbdt'\n",
    "\n",
    "test_result = pd.concat([test_result,pd.DataFrame(test_heuristic_dict,index=[99])]).sort_values('accuracy',ascending=False)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dstyle</th>\n",
       "      <th>ustyle</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>trade</td>\n",
       "      <td>power</td>\n",
       "      <td>0.116259</td>\n",
       "      <td>0.319982</td>\n",
       "      <td>0.402297</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>power</td>\n",
       "      <td>0.123578</td>\n",
       "      <td>0.330407</td>\n",
       "      <td>0.418045</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>power</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.449562</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.124530</td>\n",
       "      <td>0.320258</td>\n",
       "      <td>0.414451</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.145127</td>\n",
       "      <td>0.358655</td>\n",
       "      <td>0.467321</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>power</td>\n",
       "      <td>0.141021</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expo</td>\n",
       "      <td>power</td>\n",
       "      <td>0.143615</td>\n",
       "      <td>0.354174</td>\n",
       "      <td>0.462649</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hce</td>\n",
       "      <td>power</td>\n",
       "      <td>0.144140</td>\n",
       "      <td>0.354858</td>\n",
       "      <td>0.463764</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>power</td>\n",
       "      <td>0.161015</td>\n",
       "      <td>0.375705</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>trade</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.159773</td>\n",
       "      <td>0.374307</td>\n",
       "      <td>0.500225</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207006</td>\n",
       "      <td>0.435422</td>\n",
       "      <td>0.603495</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hb2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.206323</td>\n",
       "      <td>0.431784</td>\n",
       "      <td>0.601487</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hce</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.208035</td>\n",
       "      <td>0.436811</td>\n",
       "      <td>0.605607</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.436206</td>\n",
       "      <td>0.604549</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.436206</td>\n",
       "      <td>0.604549</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expo</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207989</td>\n",
       "      <td>0.436703</td>\n",
       "      <td>0.605504</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expo2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.308999</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>6.569844</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hb2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.308991</td>\n",
       "      <td>0.308997</td>\n",
       "      <td>6.132345</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expo2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.206042</td>\n",
       "      <td>0.416151</td>\n",
       "      <td>0.599752</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207245</td>\n",
       "      <td>0.435829</td>\n",
       "      <td>0.603899</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.207411</td>\n",
       "      <td>0.435977</td>\n",
       "      <td>0.604252</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.208322</td>\n",
       "      <td>0.437190</td>\n",
       "      <td>0.606112</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dstyle ustyle       mse       mae  log_loss  accuracy  pred_ll\n",
       "21          trade  power  0.116259  0.319982  0.402297     0.905    0.238\n",
       "13           hbmd  power  0.123578  0.330407  0.418045     0.896    0.239\n",
       "19     quasihb_fc  power  0.136944  0.349498  0.449562     0.880    0.321\n",
       "17        quasihb  power  0.124530  0.320258  0.414451     0.877    0.230\n",
       "9              hb  power  0.145127  0.358655  0.467321     0.860    0.283\n",
       "3   attention_uni  power  0.141021  0.351220  0.455357     0.853    0.174\n",
       "5            expo  power  0.143615  0.354174  0.462649     0.842    0.239\n",
       "15            hce  power  0.144140  0.354858  0.463764     0.842    0.239\n",
       "1       attention  power  0.161015  0.375705  0.497726     0.833    0.150\n",
       "20          trade   cara  0.159773  0.374307  0.500225     0.809    0.132\n",
       "0       attention   cara  0.207006  0.435422  0.603495     0.694    0.083\n",
       "10            hb2   cara  0.206323  0.431784  0.601487     0.694    0.045\n",
       "14            hce   cara  0.208035  0.436811  0.605607     0.693    0.046\n",
       "16        quasihb   cara  0.207547  0.436206  0.604549     0.693    0.060\n",
       "18     quasihb_fc   cara  0.207547  0.436206  0.604549     0.693    0.060\n",
       "4            expo   cara  0.207989  0.436703  0.605504     0.693    0.046\n",
       "7           expo2  power  0.308999  0.309000  6.569844     0.691    0.000\n",
       "11            hb2  power  0.308991  0.308997  6.132345     0.691    0.000\n",
       "6           expo2   cara  0.206042  0.416151  0.599752     0.690    0.009\n",
       "12           hbmd   cara  0.207245  0.435829  0.603899     0.686    0.091\n",
       "2   attention_uni   cara  0.207411  0.435977  0.604252     0.682    0.093\n",
       "8              hb   cara  0.208322  0.437190  0.606112     0.681    0.078"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly draw 1000 questions from the dataset \n",
    "# Use the prediction value by XGBoost as the label\n",
    "# Examine which model can explain the XGBoost's prediction the best\n",
    "rda_sample = itch_dt[itch_dt.index.isin(np.random.choice(itch_dt.index,size=1000,replace=False))][['ss_x','ss_t','ll_x','ll_t']]\n",
    "rda_sample = cross_valid.generate_sample(rda_sample)\n",
    "rda_sample['choice'] = heuristic_model.predict(rda_sample[features])\n",
    "\n",
    "rda_result = []\n",
    "\n",
    "for i in range(len(kf_result_df)):\n",
    "\n",
    "    rda_style = kf_result_df['style'][i]\n",
    "    rda_params = kf_result_df['params'][i]\n",
    "\n",
    "    rda_scores = cross_valid.test_model(test_sample=rda_sample,style=rda_style,params=rda_params)\n",
    "    rda_scores['dstyle'] = rda_style['dstyle'] \n",
    "    rda_scores['ustyle'] = rda_style['ustyle']\n",
    "    rda_result.append(rda_scores)\n",
    "\n",
    "rda_result = pd.DataFrame(rda_result)\n",
    "rda_result = rda_result.reindex(columns=df_cols).sort_values('accuracy',ascending=False).drop(['style','params'],axis=1)\n",
    "rda_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dstyle</th>\n",
       "      <th>ustyle</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>power</td>\n",
       "      <td>0.119289</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>0.409297</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.114866</td>\n",
       "      <td>0.310595</td>\n",
       "      <td>0.393456</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hb</td>\n",
       "      <td>power</td>\n",
       "      <td>0.137929</td>\n",
       "      <td>0.351457</td>\n",
       "      <td>0.451721</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>power</td>\n",
       "      <td>0.136919</td>\n",
       "      <td>0.347119</td>\n",
       "      <td>0.447106</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>power</td>\n",
       "      <td>0.152359</td>\n",
       "      <td>0.367049</td>\n",
       "      <td>0.479605</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expo</td>\n",
       "      <td>power</td>\n",
       "      <td>0.134611</td>\n",
       "      <td>0.345170</td>\n",
       "      <td>0.443175</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hce</td>\n",
       "      <td>power</td>\n",
       "      <td>0.134754</td>\n",
       "      <td>0.345473</td>\n",
       "      <td>0.443532</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>power</td>\n",
       "      <td>0.134172</td>\n",
       "      <td>0.346726</td>\n",
       "      <td>0.443693</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>trade</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.147103</td>\n",
       "      <td>0.361636</td>\n",
       "      <td>0.473816</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expo2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.187767</td>\n",
       "      <td>0.397876</td>\n",
       "      <td>0.560264</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hb2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.237993</td>\n",
       "      <td>0.237999</td>\n",
       "      <td>4.765810</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expo2</td>\n",
       "      <td>power</td>\n",
       "      <td>0.237999</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>5.118842</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hb2</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.191294</td>\n",
       "      <td>0.416756</td>\n",
       "      <td>0.570438</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hce</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194076</td>\n",
       "      <td>0.422853</td>\n",
       "      <td>0.576921</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expo</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194005</td>\n",
       "      <td>0.422719</td>\n",
       "      <td>0.576764</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quasihb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194278</td>\n",
       "      <td>0.422937</td>\n",
       "      <td>0.577245</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>quasihb_fc</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194278</td>\n",
       "      <td>0.422937</td>\n",
       "      <td>0.577245</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.193766</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>0.576226</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hb</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.195291</td>\n",
       "      <td>0.424158</td>\n",
       "      <td>0.579254</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hbmd</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194717</td>\n",
       "      <td>0.423301</td>\n",
       "      <td>0.578037</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_uni</td>\n",
       "      <td>cara</td>\n",
       "      <td>0.194698</td>\n",
       "      <td>0.423264</td>\n",
       "      <td>0.578021</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dstyle ustyle       mse       mae  log_loss  accuracy  pred_ll\n",
       "13           hbmd  power  0.119289  0.326118  0.409297     0.945    0.239\n",
       "17        quasihb  power  0.114866  0.310595  0.393456     0.920    0.230\n",
       "9              hb  power  0.137929  0.351457  0.451721     0.897    0.283\n",
       "3   attention_uni  power  0.136919  0.347119  0.447106     0.884    0.174\n",
       "1       attention  power  0.152359  0.367049  0.479605     0.882    0.150\n",
       "5            expo  power  0.134611  0.345170  0.443175     0.881    0.239\n",
       "15            hce  power  0.134754  0.345473  0.443532     0.881    0.239\n",
       "19     quasihb_fc  power  0.134172  0.346726  0.443693     0.875    0.321\n",
       "20          trade   cara  0.147103  0.361636  0.473816     0.854    0.132\n",
       "6           expo2   cara  0.187767  0.397876  0.560264     0.765    0.009\n",
       "11            hb2  power  0.237993  0.237999  4.765810     0.762    0.000\n",
       "7           expo2  power  0.237999  0.238000  5.118842     0.762    0.000\n",
       "10            hb2   cara  0.191294  0.416756  0.570438     0.755    0.045\n",
       "14            hce   cara  0.194076  0.422853  0.576921     0.754    0.046\n",
       "4            expo   cara  0.194005  0.422719  0.576764     0.754    0.046\n",
       "16        quasihb   cara  0.194278  0.422937  0.577245     0.746    0.060\n",
       "18     quasihb_fc   cara  0.194278  0.422937  0.577245     0.746    0.060\n",
       "0       attention   cara  0.193766  0.422182  0.576226     0.741    0.083\n",
       "8              hb   cara  0.195291  0.424158  0.579254     0.734    0.078\n",
       "12           hbmd   cara  0.194717  0.423301  0.578037     0.733    0.091\n",
       "2   attention_uni   cara  0.194698  0.423264  0.578021     0.729    0.093"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the prediction value by magnitude-dependent hyperbolic (hbmd) with power utillity as the label\n",
    "# Examine which model can explain the hbmd's prediction the best\n",
    "ss_t = rda_sample['ss_t'].values\n",
    "ss_x = rda_sample['ss_x'].values\n",
    "ll_t = rda_sample['ll_t'].values\n",
    "ll_x = rda_sample['ll_x'].values\n",
    "kf_init_row = kf_result_df[(kf_result_df['dstyle']=='trade') & (kf_result_df['ustyle']=='power')]\n",
    "init_style = kf_init_row['style'].values[0]\n",
    "init_params = kf_init_row['params'].values[0]\n",
    "\n",
    "rda_sample['init_choice'] = cross_valid.choice_rule.choice_prob(ss_x, ss_t, ll_x, ll_t, \n",
    "                                                            dstyle = init_style['dstyle'], \n",
    "                                                            ustyle = init_style['ustyle'], \n",
    "                                                            method = init_style['method'],\n",
    "                                                            intercept= init_style['intercept'],\n",
    "                                                            params = init_params[:-1], \n",
    "                                                            temper = init_params[-1]) \n",
    "\n",
    "rda_sample['choice'] = (rda_sample['init_choice']>.5)\n",
    "\n",
    "rda_result = []\n",
    "\n",
    "for i in range(len(kf_result_df)):\n",
    "\n",
    "    rda_style = kf_result_df['style'][i]\n",
    "    rda_params = kf_result_df['params'][i]\n",
    "\n",
    "    rda_scores = cross_valid.test_model(test_sample=rda_sample,style=rda_style,params=rda_params)\n",
    "    rda_scores['dstyle'] = rda_style['dstyle'] \n",
    "    rda_scores['ustyle'] = rda_style['ustyle']\n",
    "    rda_result.append(rda_scores)\n",
    "\n",
    "rda_result = pd.DataFrame(rda_result)\n",
    "rda_result = rda_result.reindex(columns=df_cols).sort_values('accuracy',ascending=False).drop(['style','params'],axis=1)\n",
    "rda_result.iloc[1:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "kf_result.to_csv(\"itch_result_kf.csv\",index=False)\n",
    "test_result.to_csv(\"itch_result_test.csv\",index=False)\n",
    "rda_result.to_csv(\"itch_result_rda.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(kf.success==False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47febc5ef3103743ffe554ef26604df7fc0e56c4a1892b2f8bd68368c7fcdbd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
