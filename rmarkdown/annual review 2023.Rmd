---
title: "Progress Report for Annual Review 22/23"
author: "Zijian Zark Wang"
bibliography: reference.bib
biblio-style: apalike
header-includes: 
  \usepackage{setspace}
  \usepackage{amsmath}
  \usepackage{array}
  \usepackage{caption}
  \usepackage{longtable}
  \usepackage{booktabs}
  \renewcommand{\arraystretch}{1}
  \captionsetup[table]{skip=5pt}
  \setstretch{1.5} 
fontsize: 12pt
geometry: margin=1in
editor_options: 
  markdown: 
    wrap: 72
output:
  pdf_document:
    number_sections: true
    citation_package: natbib
  html_document:
    toc: true
    number_sections: true
---

# Current Progress

I develop a model of intertemporal choice, which I term
"*attention-adjusted discounting*". I postulate that the overall utility
a decision maker can obtain from a reward stream is decided by the
weighted sum of utilities that can be obtained in each time period. The
initial weight allocation is exponential, i.e. the decision maker is
initially time-stationary. However, when evaluating the given reward
stream, she tends to assign more weights (pay more attention) to the
time periods with larger rewards, in order to subjectively maximize her
overall utility. This attention adjustment process incurs a cognitive
cost; and the more the weight allocation deviates from the initial
allocation, the greater the cost is. The decision maker optimally
re-allocates the weights across time periods.

I term any discounted utility model of which the discounting factors are
formed by such an attention adjustment process as ADU. I show that ADU
can help explain a series of empirical findings about intertemporal
choice, including common difference effect and magnitude effect
[@loewenstein_anomalies_1992], risk aversion over time lotteries
[@onay_intertemporal_2007; @dejarnette_time_2020], non-additive time
intervals [@read_is_2001; @scholten_discounting_2006], intertemporal
correlation aversion [@andersen_multiattribute_2018], and dynamic
inconsistency. The model can also offer insights on the preferences for
sequences of outcomes [@loewenstein_preferences_1993] and the formation
of reference-dependent preferences [@koszegi_model_2006]. In an
empirical test, I find ADU outperforms a set of time discounting models
in predicting human intertemporal choices.

The following example describes the underlying process of
attention-adjusted discounting: Suppose a decision maker wants to
estimate the value of "receiving £10 in 3 days" (and receiving no reward
in Day 1 and Day 2). Set that the value of both Day 1's reward and Day
2's reward is 0, and that of Day 3's reward is 10. Assuming the decision
maker initially holds stationary time preferences, we can consider a
exponential discounting model where the discounting parameter is 0.8.
Then, the value of this reward stream is 0 × 1 + 0 × 0.8 + 10 × 0.64.
One can also imagine that there is a black box with three types of
balls, 1000 of which are initially labelled "Day 1's reward", 800 are
initially labelled "Day 2's reward", 640 are initially labelled "Day 3's
reward". The decision maker draws a random sample from the box, then
exchange the drawn balls for rewards, to evaluate the given stream.

The key hypothesis of ADU is, prior to the draw, there is an "attention
mechanism" that decides how many of each type of balls should be put
into the black box. Putting zero balls of a certain type into the box
implies the decision maker totally ignores that type. Given that "Day
3's reward" is the greatest, she wants to put more of this type of balls
into the box -- to exchange for more rewards afterwards. However, the
total number of balls in the box is fixed ("limited attention") and it
is extremely cognitively effortful to change all balls initially in the
box to this type ("costly attention adjustment"). She is only able to
change a part of them. Suppose the final distribution of balls for Day
1, Day 2 and Day 3 is 970, 770 and 700, that is, the decision maker
successfully changes 60 balls to "Day 3's reward"; as a result, the
discounting factors corresponding to each time period should shift to 1,
0.79 and 0.72. Hence, her discounting factors will perform a
"hyperbolic" style. Besides, if we increase the magnitude of Day 3's
reward to £20, the benefit of increasing the weight on "Day 3's reward"
can offset a greater cost of attention adjustment. Thus, she should
perform greater patience for a larger delayed reward.

The remaining part of this document is organized as follows. Section
\ref{model} outlines the model of attention-adjusted discounting.
Section \ref{behavioral} explains how the model can help explain the
empirical findings about choices and preferences. Section
\ref{empirical} performs an empirical test of the model, in comparison
with other intertemporal choice models. Section \ref{scedule} introduces
my plan for the next step.

# Attention-Adjusted Discounting \label{model}

Consider a reward stream $x = [x_0,x_1,...,x_T]$ that yields reward
$x_t$ in time period $t$. The time length of this stream, denoted by
$T$, is finite. For any $t \in \{0,1,...,T\}$, the reward level $x_t$ is
a random variable defined on $R_{+}$. The support of $x$ is $X$, which
is a subset of $R_{+}^T$.

Suppose there is a decision maker wanting to evaluate the reward stream
$x$. She firstly randomly draws some potential realizations of $x$ from
$X$. Let $s=[s_0,s_1,...,s_T]$ be a potentially realized reward stream,
and $p(s)$ be the probability that $s$ is drawn. Then, for each drawn
realization of $x$, she randomly draws some time periods from the
stream, taking the rewards of such periods into a sample. I denote the
decision maker's value function by $u(.)$, where $u(s_t)$ is the utility
obtained by reward $s_t$ ($t \in \{0,1,...,T\}$), and $u'>0$, $u''<0$.
Finally, she uses the mean value of sampled rewards as a value
representation of $x$. I denote the decision maker's sampling strategy
by a weighting function $w(.)$, where $w(s_t)$ is the probability that
the reward of the $t$-th period in a potentially realized stream $s$ is
sampled. She wants to find a function $w(.)$ that maximizes her overall
utility. The sampling process is sequential. In a given potentially
realized stream $s$, the periods with larger reward levels should be
sampled more frequently. However, at the very beginning, the decision
maker has no information about which period in $s$ has a larger reward;
she learns such information gradually in the process of sampling. This
learning process triggers a cognitive cost. Therefore, her overall
utility is the mean value of sampled rewards minus the cognitive cost of
learning.

Suppose when having no information, the weight on period $t$ in each
potentially realized stream is equal ($\equiv w^0_t$). Let $W$ and $P$
be the minimal sets that contain all available function $w$ and function
$p$ respectively. We can use an optimization problem to represent the
above evaluation procedure: $$ 
\begin{aligned}
\max_{w\in W}  \quad & \sum_{s\in X}\sum_{t=0}^T w(s_t)u(s_t) - C(w;\theta) \\
s.t. \quad &  \sum_{s\in X}\sum_{t=0}^T w(s_t)=1 \\
& w(s_t)>0, \forall s\in X,t=0,1,…,T \\
\end{aligned}
$$

where $C(.)$ is a cognitive cost function with $\theta$ as its
parameters. To solve this optimization problem, I add two additional
assumptions. The first is that the weight updating process is consistent
with Bayes rule, that is, $w^0_t=\sum_{s\in X} w(s_t)$. The second is
that the cognitive cost function takes a form similar to Shannon mutual
information, that is$$
C(\textbf{w};\theta)= \lambda \sum_{s\in X}\sum_{t=0}^T w(s_t) \log\left(\frac{w(s_t)}{p(s)w_t^0}\right)
$$

where $p(s)w^0_t$ is the probability of $s_t$ being sampled when no
information is learned, $w(s_t)$ is the probability of that after
learning the information about $x$. Shannon mutual information
quantifies the information about which time period in each $s$ has a
larger reward, obtained by learning. Parameter $\lambda$ denotes unit
cost of information ($\lambda>0$).

Define $w(s_t|s) = \frac{w(s_t)}{p(s)}$. Then $w(s_t|s)$ can represent
the discounting factor of time period $t$ given that the stream $s$ is
drawn. As is shown in @matejka_rational_2015, the solution is$$ \tag{1}
w(s_t|s) =\frac{w_t^0e^{u(s_t)/\lambda}}{\sum_{t=0}^T w_\tau^0 e^{u(s_t)/\lambda}}
$$

Note the $w(s_t|s)$ is increasing in $s_t$, i.e. the decision maker
performs greater patience for a larger reward.

In addition to ADU, there are other models that attempt to incorporate
attention mechanism into the formation of time preferences. For example,
@steiner_rational_2017 consider a decision maker adjusting the belief
$p(s)$ over time but holding the discounting factor $w(s_t|s)$ constant.
In each time period, given that her ability to learn new information is
limited, the upated belief cannot deviate from that in the previous
period by too much, which causes behavioral inertia. Instead, ADU
assumes the decision maker re-allocates $w(s_t|s)$ each time period.
This makes it convenient to analyze not only dynamic decision-making but
also choices in MEL tasks. Besides, @gabaix_myopia_2017 assume the
perception of future rewards is noisy and the decision maker estimates
the value of reward by sampling from a normal distribution. Furthermore,
@gershman_rationally_2020 optimally chooses the sample variance to
minimize the mean sample squared error. Such theories, together with a
certain specification on rate-distortion function, can lead to
magnitude-increasing patience and hyperbolic-like discounting.
Discounting factors in this style can be viewed as a special case of
attention-adjusted discounting. @noor_optimal_2022 and
@noor_constrained_2023 construct an optimization problem similar to ADU.
However, they use a different cognitive cost function. In an empirical
test, I compare the predictive performance of ADU with models of
@gershman_rationally_2020 and @noor_optimal_2022, as well as some other
models.

# Explaining Behavioral Biases \label{behavioral}

## Evaluating Delayed Rewards

Suppose a reward of level $c$ ($c>0$) is delivered at time period $T$.
This delayed reward can be represented by a reward stream
$x=[x_0,x_1,…,x_T]$, where $x_0=…=x_{T-1}=0$ and $x_T=c$. For
simplicity, I assume $u(0)=0$ and $w^0_t=\delta^t$, which implies the
decision maker initially holds stationary time preferences.
$\delta\in(0,1]$, where $\delta=1$ implies the initial attention is
uniformly distributed across time periods. Given that the reward is
deterministic, one can omit $s$ in $w(s_t|s)$ and directly represent the
weight on each time period $t$ by $w_t$. Therefore, the value of this
delayed reward is $w_Tu(x_T)$. By Equation (1),$$ \tag{2}
w_T  = \frac{\delta^Te^{u(x_T)/\lambda}}{\sum_{t=0}^T \delta^te^{u(x_t)/\lambda}}  =\frac{1}{1+G(T)\cdot e^{-u(x_T)/\lambda}}
$$ where$$
G(T) = \left\{\begin{split}
& T \;, & \delta=1\\
& \frac{1}{1-\delta}(\delta^{-T}-1) \;,&0<\delta<1
\end{split}\right.
$$

Equation (2) implies $w_T$ is increasing in $x_T$ and decreasing in $T$.
I show that Equation (2) can explain a series of experimental findings.
Due to the limitation on word number, I omit mathematical proof.

### Common Difference Effect

Suppose there are a large later reward $x_l$ arriving at period $t_l$
(denoted by LL) and a small sooner reward $x_s$ arriving at period $t_s$
(denoted by SS), where $x_l>x_s>0$, $t_l>t_s>0$. Assuming
$w_{t_l}(x_l)u(x_l)=w_{t_s}(x_s)u(x_s)$, common difference effect
implies $w_{t_l+\Delta t}(x_l)u(x_l)>w_{t_s+\Delta t}(x_s)u(x_s)$ for
any positive integer $\Delta t$, magnitude effect implies
$w_{t_l}(x_l)u(x_l+\Delta x)>w_{t_s}(x_s)u(x_s+\Delta x)$ for any
positive real number $\Delta x$ [@loewenstein_anomalies_1992].

When $\delta = 1$, ADU predicts that decision makers always perform
common difference effect. This is obvious because the discounting factor
$w_T$ takes a hyperbolic-like form. When $\delta<1$, decision makers
perform common difference effect only when the difference between $x_l$
and $x_s$ are much larger than the difference between $t_l$ and $t_s$.
Note that $w_t \propto \delta^t e^{u(x_t)/\lambda}$. At the intuitive
level, when omitting the constraint that the sum of weights on each time
period is fixed, the discounting factor of any period $t$ other than the
final period in LL or SS should be $\delta^t$, and
$w_{t_l+\Delta t}(x_l)/w_{t_s+\Delta t}(x_s)=\delta^{t_l-t_s} \exp\{\frac{u(x_l)-u(x_s)}{\lambda}\}$,
which is constant for any $\Delta t$. However, given that the decision
makers' attention is limited, when keeping time length fixed, increasing
the reward level in the final period (that is, $x_T$) can make it
naturally grab more attention from the previous periods; when extending
the time length, the average attention that can be allocated to each
period should shrink. Decision makers perform common difference effect
only when the former effect exceeds the latter effect.

### Magnitude Effect and Reference-Dependent Preferences

ADU predicts that the larger the unit cost of information $\lambda$ or
the smaller the magnitude of $x_l$ and $x_s$ is, the more likely it is
that decision makers perform the magnitude effect. First, note that
magnitude effect requires the decision makers' overall utility
$w_T(x_T)u(x_T)$ to be a convex function of $x_T$. Given that $u(.)$ is
concave, whether the magnitude effect holds should depend on $w_T$.
Then, set $z = u(x_T)-\lambda\log G(T)$. We can rewrite Equation (2) as
a logistic function of $z$, i.e. $w_T = 1/(1+e^{-z/\lambda})$. By the
shape of logistic function, $w_T$ is convex in $u(x_T)$ if and only if
$u(x_T)<\lambda \log G(T)$ (that is, when $x_T$ is small relative to $T$
or when $\lambda$ is large). Finally, it is notable that the given
condition is necessary but not sufficient to yield magnitude effect.

In summary, holding the others equal, the decision makers' overall
utility can be convex in a future reward when the level of reward is
under a certain threshold, and be concave when it is above the
threshold. This is also consistent to the theories about
reference-dependent preferences [@koszegi_model_2006].

### Risk Attitude over Time Lotteries and Non-additive Time Intervals

Both exponential and hyperbolic discounting models predict decision
makers are risk seeking over time lotteries. That is, suppose a reward
of level $c$ is delivered in period $t_s$ with probability $\pi$ and is
delivered in period $t_l$ with probability $1-\pi$, where $0<\pi<1$,
then decision makers should prefer this case to the case that the same
reward is delivered in a certain period $\pi t_s +(1-\pi) t_l$. However,
@onay_intertemporal_2007 find people are only risk seeking over time
lotteries when $\pi$ is small and they are risk averse over time
lotteries when $\pi$ is large. This finding can be explained by the
convexity of $w_T$.

Let $t_m = \pi t_s +(1-\pi) t_l$. Under ADU, the decision makers are
risk seeking over time lotteries when
$\pi w_{t_s}(c)+(1-\pi)w_{t_l}(c)>w_{t_m}(c)$. First, note the LHS
equals to the RHS when $\pi=0$ or $\pi=1$. Fixing $t_s$ and $t_l$, this
inequality implies $w_{t_m}(c)$ is convex in $t_m$. Second, it can be
proved that $w_T(c)$ is convex in $T$ if and only if $T$ is above a
certain threshold. This is also consistent with
@takeuchi_non-parametric_2011 that suggests the discount function should
be inverse S-shaped with respect to time. Third, note $t_m$ is linearly
decreasing with $\pi$, thus decision makers are more likely to be risk
seeking over time lotteries when $\pi$ is small. The same can be applied
to the risk aversion case.

Now consider $T$ is small enough to make $w_T$ concave in $T$. In this
case, adding an extension to $T$ will increase the rate at which $w_T$
declines with $T$ -- the property is termed "super-additive time
intervals" by @read_is_2001. By contrast, in many models, including
exponential and hyperbolic discounting, discounting factors are
typically decided by a convex function of $T$. Moreover, ADU predicts
intervals are sub-additive when the total time length $T$ is large, and
are super-additive when $T$ is small, which is consistent with
@scholten_discounting_2006.

## Intertemporal Correlation Aversion

Let $x$ and $y$ denote two 2-period risky reward streams. For $x$, the
realized stream is [£100,£100] with probability 1/2, and is [£3,£3] with
probability 1/2. For $y$, the realized stream is [£3,£100] with
probability 1/2, and is [£100,£3] with probability 1/2. Classical models
of intertemporal choice, such as @fishburn_time_1982, typically assume
the separability of potentially realized streams. This implies that
decision makers are indifferent between $x$ and $y$. However,
@andersen_multiattribute_2018 find evidence of intertemporal correlation
aversion, that is, people often prefer $y$ to $x$. Such a property is
also termed "weak separability" in @noor_constrained_2023.

ADU can naturally yield intertemporal correlation aversion. For
simplicity, suppose the initial attention is uniformly distributed
across the two periods. For $x$, under each potentially realized stream,
decision makers equally weight each period. For $y$, decision makers
tend to assign more weight to the period with a reward of £100 (suppose
that weight is $w$). Then the value of $x$ is
$\frac{1}{2} u(100) + \frac{1}{2} u(3)$ and the value of $y$ is
$w\cdot u(100) +(1-w) \cdot u(3)$. Given that $x>\frac{1}{2}$, the
decision makers should strictly prefer $y$ to $x$.

## Dynamic Inconsistency

Suppose a decision maker has budget $m$ and is considering how to spend
it over different time periods. We can use a reward stream $x$ to
represent this decision problem, where the decision maker's spending in
period $t$ is $x_t$. In period 0, she wants to find a $x$ such
that$$ \tag{3}
\max_{x}\;\sum_{t=0}^T w_t u(x_t)\quad s.t. \;\sum_{t=0}^T x_t = m  
$$

where $w_t$ is the attention-adjusted discounting factor in period $t$.
I assume
$w_t=\delta^t e^{u(x_t)/\lambda}/\sum_{t=\tau}^T \delta^{\tau} e^{u(x_\tau)/\lambda}$
and there is no risk under this setting.

In models like exponential and hyperbolic discounting, the discounting
factor in a future period is consistently smaller than that in the
present period. Thus, the decision maker should spend more at the
present than in the future. By contrast, in ADU, when increasing the
spending in a certain period, the discounting factor corresponding to
that should also increase. So it is possible that the decision maker
spends more in the future and a future period can have greater
discounting factor than the present period. This is consistent with the
findings in @loewenstein_preferences_1993 that people sometimes prefer
improving sequences to declining sequences.

ADU suggests there are two mechanisms that can help explain why people
usually perform dynamically inconsistent behavior. The first is
*attention-grabbing effect*, that is, keeping the others equal, when we
increase $x_t$ (which lead to an increase in $w_t$), the discounting
factor in any other period should decrease due to limited attention.
After omitting a previous period from the decision problem in Equation
(3), the decision maker can assign more weights to remaining periods;
thus, the attention-grabbing effect is enhanced. The increased
attention-grabbing effect will offset some benefit of increasing
spending toward a certain period. Therefore, when the decision maker
prefers improving sequences, the attention-grabbing effect will make her
perform a present bias-like behavior (always feeling that she should
spend more at the present than the original plan); when the decision
maker prefers declining sequences, this effect will maker her perform a
future bias-like behavior (always feeling she should spend more in the
future).

The second mechanism is *initial attention updating*. As is assumed
above, in period 0, prior to evaluating each reward stream, the decision
maker's initial weight on period $t$ is proportional to $\delta^t$;
after evaluation, the weight becomes being proportional to
$\delta^t e^{u(x_t)/\lambda}$. In period 1, if she implements the
evaluation based on the information attained in period 0, the initial
weight should be updated to being proportional
$\delta^t e^{u(x_t)/\lambda}$; thus, the weight after evaluation should
become being proportional to $\delta e^{2u(x_t)/\lambda}$. As a result,
the benefit of increasing spending toward a certain period gets
strengthened. The updated initial attention can make those who prefer
improving sequences perform present bias and those who prefer declining
sequences perform future bias.

Both the attention-grabbing effect and initial attention updating are
affected by the curvature of utility function. They jointly decide which
behavior pattern that people should perform in dynamics.

# Comparing Models of Intertemporal Choice \label{empirical}

## Data

I test the ability of ADU in predicting human intertemporal choices on
two open datasets. The sources of data are @ericson_money_2015 and
@chavez_hierarchical_2017. In each study, participants are recruited to
do the classical "Money Eariler or Later" (MEL, choosing between a SS
reward and a LL reward) tasks. Thus, I term each dataset as *Ericson*
data and *Chávez* data. *Ericson* data contains 23,131 observations from
939 participants; *Chávez* data contains 34,515 choices from 1,284
participants.

## Empirical Strategy

I mainly focus on out-of-sample model performance. For each dataset, I
randomly draw the responses from 20% of the participants as the test
sample, and set the rest as the train sample. To mitigate the
overfitting issue, I implement a 10-fold cross-validation procedure on
the train sample.

I fit the train sample with ADU and 8 other time discounting models, the
intertemporal trade-off model [@scholten_weighing_2014], and a
decision-tree heuristic model, namely XGboost [@chen_xgboost_2016].
Table \ref{tab:models} describes the models used for comparison. For
each time discounting model and the intertemporal trade-off model, I use
Logit model to predict the probability of participants choosing LL. A
temperature parameter is added to control the entropy of the probability
distribution. Besides, I test each model with two types of utility
functions: CARA utility, $u(x)=1-e^{-\gamma x}$; power utility,
$u(x) = x^{\gamma}$ ($\gamma$ is a parameter). For the heuristic model,
I use the same features as @read_drift_2013 and @ericson_money_2015.

```{=tex}
\begin{longtable}{p{0.14\textwidth} p{0.4\textwidth} p{0.13\textwidth} p{0.23\textwidth}}
\caption{Models for Comparison}
\label{tab:models}\\
\toprule
\textbf{model} & \textbf{description} & \textbf{\#parameter} & \textbf{  source} \\* \midrule
\endhead
%
\bottomrule
\endfoot
%
\endlastfoot
%
attention & ADU & 2 & - \\
attention\_uni & ADU with uniformly allocated initial attention & 1 &  - \\
expo & exponential discounting & 1 & - \\
hb & hyperbolic discounting & 1 & - \\
expo2 & double-exponential discounting & 3 & \citet{van_den_bos_towards_2013} \\
hb2 & dual-parameter hyperbolic discounting & 2 & \citet{loewenstein_anomalies_1992} \\
hbmd & magnitude-dependent hyperbolic discounting & 1 & \citet{gershman_rationally_2020} \\
quasihb & quasi-hyperbolic discounting & 2 &  \citet{laibson_golden_1997} \\
quasihb\_fc & quasi-hyperbolic discounting plus a fixed delay cost & 
3 & \citet{benhabib_present-bias_2010} \\
hce & homogeneous costly empathy model of discounting & 2 & \citet{noor_optimal_2022}\\
trade & intertemporal trade-off & 4 & \citet{scholten_weighing_2014}\\
heuristic & a decision-tree learning algorithm & by tunning & \citet{chen_xgboost_2016}\\* 
\bottomrule
\end{longtable}
```
I use the maximum likelihood method to estimate the parameters, and
apply L-BFGS-B method for optimization. As the solution of L-BFGS-B is
sensitive to initial points, I use the basin-hopping algorithm for
global optimization. For details about empirical analysis, see
[https://github.com/zarkwang/attention_discount_project](https://github.com/zarkwang/attention_discount_project.)

## Results

In summary, I find the heuristic model outperforms the other models in
predictive accuracy. ADU and the intertemporal trade-off model rank
either the second or the third (dependent on the dataset and empirical
strategy), and their performance is very close. Each of the two models
can accurately predict about 95% of the simulated responses generated by
XGBoost. Note that ADU has only one additional parameter compared with
the exponential or hyperbolic discounting, whereas the intertemporal
trade-off model has three additional parameters, researchers who want to
save the number of parameters can consider ADU as a candidate in model
fitting. Table \ref{tab:chavez_test_result} present the out-of-sample
test results on Chávez Data.

```{=tex}
\begin{longtable}{@{}lllllll@{}}
\caption{Out-of-Sample Test Results on Chávez Data}
\label{tab:chavez_test_result}\\
\toprule
\textbf{model} & \textbf{utility} & \textbf{mse} & \textbf{mae} & \textbf{logLoss} & \textbf{accuracy} & \textbf{predLL} \\* \midrule
\endhead
%
\bottomrule
\endfoot
%
\endlastfoot
%
heurstic       & --    & 0.163 & 0.322 & 0.500 & 0.767 & 0.295 \\
trade          & power & 0.165 & 0.316 & 0.504 & 0.766 & 0.258 \\
attention      & power & 0.165 & 0.326 & 0.505 & 0.763 & 0.332 \\
attention\_uni & power & 0.165 & 0.325 & 0.506 & 0.763 & 0.332 \\
hb             & power & 0.167 & 0.327 & 0.508 & 0.757 & 0.332 \\
quasihb\_fc    & power & 0.206 & 0.448 & 0.604 & 0.757 & 0.332 \\
quasihb        & power & 0.180 & 0.397 & 0.543 & 0.757 & 0.332 \\
hce            & power & 0.167 & 0.335 & 0.510 & 0.757 & 0.332 \\
hbmd           & power & 0.165 & 0.327 & 0.505 & 0.757 & 0.332 \\
hbmd           & cara  & 0.167 & 0.329 & 0.509 & 0.757 & 0.332 \\
hb2            & power & 0.167 & 0.341 & 0.509 & 0.757 & 0.332 \\
expo           & power & 0.167 & 0.334 & 0.510 & 0.757 & 0.332 \\
trade          & cara  & 0.248 & 0.498 & 0.689 & 0.700 & 0.222 \\
attention      & cara  & 0.179 & 0.356 & 0.535 & 0.685 & 0.037 \\
attention\_uni & cara  & 0.191 & 0.414 & 0.567 & 0.685 & 0.037 \\
hb2            & cara  & 0.184 & 0.344 & 0.545 & 0.667 & 0.000 \\
hb             & cara  & 0.207 & 0.322 & 0.622 & 0.667 & 0.000 \\
expo2          & power & 0.333 & 0.333 & 3.764 & 0.667 & 0.000 \\
expo2          & cara  & 0.326 & 0.333 & 1.605 & 0.667 & 0.000 \\
hce            & cara  & 0.202 & 0.316 & 0.634 & 0.667 & 0.000 \\
quasihb        & cara  & 0.222 & 0.333 & 0.650 & 0.667 & 0.000 \\
expo           & cara  & 0.203 & 0.314 & 0.653 & 0.667 & 0.000 \\
quasihb\_fc    & cara  & 0.198 & 0.336 & 0.582 & 0.667 & 0.000 \\* \bottomrule
\multicolumn{7}{p{0.8\textwidth}}{
\setstretch{1}
\textit{Note}: \textbf{mae} denotes mean absolute error, \textbf{mse} denotes mean squared error, \textbf{predLL} denotes the ratio of LL in predicted choices.
}
\end{longtable}
```
# Next Step \label{schedule}

The model of attention-adjusted discounting borrows insights from
rational inattention theories [@matejka_rational_2015;
@jung_discrete_2019; @mackowiak_rational_2023]. I am designing an
experiment to test the key assumptions of "rational inattention". In
that design, participants randomly draw a lottery from two given
lotteries, and prior to the draw, they can change the probability of
each lottery being drawn through a real-effort task.

I am also trying to develop an axiomatic characterization of ADU.
@matejka_rational_2015, @caplin_rationally_2022 and
@noor_constrained_2023 provide some clues in this regard. Moreover, more
empirical analyses are needed for validating the model. Evidence in the
field of dynamic inconsistency is still limited. Table \ref{tab:next}
shows my plan for the next six months.

```{=tex}
    \begin{longtable}{@{}l@{}}
    \caption{Plan for the Next Step}
    \label{tab:next}\\
    \toprule
    \textbf{Timeline} \\* \midrule
    \endhead
    %
    \bottomrule
    \endfoot
    %
    \endlastfoot
    %
    \textbf{June 2023} \\
      \quad - Refine the theory of ADU \\
      \quad - Find proper ways to further validate the ADU model \\
      \quad - Finalize the experimental design for testing rational inattention \\
    \textbf{July-Aug 2023} \\
      \quad - Obtain necessary approvals and run a pilot test \\
      \quad - Conduct the experiment and collect data \\
      \quad - Start analyzing the data \\
    \textbf{Sep-Nov 2023} \\
      \quad - Complete the paper about ADU \\
      \quad - Begin drafting a paper on the experiment tests of rational inattention \\
      \quad - Plan for the final chapter of thesis \\* \bottomrule
    \end{longtable}
```
# Reference
