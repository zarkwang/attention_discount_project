\newpage

\hypertarget{appendix}{%
\section*{Appendix}\label{appendix}}
\addcontentsline{toc}{section}{Appendix}

\hypertarget{a.-proof-of-proposition-1}{%
\subsection*{A. Proof of Proposition
1}\label{a.-proof-of-proposition-1}}
\addcontentsline{toc}{subsection}{A. Proof of Proposition 1}

We present the proof of sufficiency here. That is, if \(\succsim\) has
an optimal discounting representation and satisfies Axiom 1-4, then it
has an AMD representation.

\noindent \textbf{Lemma 1}: \emph{If Axiom 1 and 3 hold, for any}
\(s_{0\rightarrow T}\)\emph{, there exist} \(w_0, w_1, …, w_T > 0\)
\emph{such that}
\(s_{0\rightarrow T} \sim w_0 \cdot s_0 + ...+w_T\cdot s_T\)\emph{,
where} \(\sum_{t=0}^T w_t=1\)\emph{.}

\noindent \emph{Proof}: If \(T=1\), Lemma 1 is a direct application of
Axiom 3. If \(T\geq 2\), for any \(2\leq t\leq T\), there should exist
\(\alpha_t\in(0,1)\) such that
\(s_{0\rightarrow t}\sim \alpha_t\cdot s_{0\rightarrow t-1}+(1-\alpha_t)\cdot s_{t}\).
By state-independence and reduction of compound alternatives, we can
recursively apply such equivalence relations as follows:\[
\begin{aligned}
s_{0\rightarrow T} &\sim \alpha_{T-1}\cdot s_{0\rightarrow T-1} + (1-\alpha_{T-1})\cdot s_T \\
&\sim  \alpha_{T-1}\alpha_{T-2}\cdot s_{0\rightarrow T-2} + \alpha_{T-1}(1-\alpha_{T-2})\cdot s_{T-1} + (1-\alpha_{T-1})\cdot s_T \\
& \sim ...\\
& \sim w_0 \cdot s_0 + w_1\cdot s_1 +... +w_T\cdot s_T
\end{aligned}
\]where \(w_0=\prod_{t=0}^{T-1}\alpha_t\), \(w_T = 1-\alpha_{T-1}\), and
for \(0<t<T\),
\(w_t=(1-\alpha_{t-1})\prod_{\tau=t}^{T-1}\alpha_{\tau}\). It is easy to
show the sum of \(w_0,…,w_T\) is equal to 1. \emph{QED}.

Therefore, if Axiom 1 and 3 hold, for any reward sequence
\(s_{0\rightarrow T}\), we can always find a convex combination of all
its elements, such that the DM is indifferent between the reward
sequence and this convex combination. If \(s_{0\rightarrow T}\) is a
constant sequence, i.e.~all its elements are constant, then we can
directly assume \(\mathcal{W}\) is AMD-style. So henceforth, we discuss
whether AMD can apply to non-constant sequences.

By Lemma 2, we show adding a new reward to the end of
\(s_{0\rightarrow T}\) has no impact on the relative decision weights of
rewards in the original reward sequence.

\noindent \textbf{Lemma 2}: \emph{For any}
\(s_{0\rightarrow T+1}\)\emph{, if}
\(s_{0\rightarrow T}\sim \sum_{t=0}^T w_t \cdot s_t\) \emph{and}
\(s_{0\rightarrow T+1} \sim \sum_{t=0}^{T+1} w'_t\cdot s_t\)\emph{,
where} \(w_t, w'_t>0\) and \(\sum_{t=0}^Tw_t=1\)\emph{,}
\(\sum_{t=0}^{T+1}w'_t=1\)\emph{, then when Axiom 1-4 hold, we can
obtain} \(\frac{w'_0}{w_0}=\frac{w'_1}{w_1}=…=\frac{w'_T}{w_T}\)\emph{.}

\noindent \emph{Proof}: According to Axiom 3, for any
\(s_{0\rightarrow T+1}\), there exist \(\alpha,\zeta \in (0,1)\) such
that\[\tag{A1}
\begin{aligned}
s_{0 \rightarrow T}\sim\alpha\cdot s_{0 \rightarrow T-1} + (1-\alpha)\cdot s_T \\
s_{0\rightarrow T+1} \sim \zeta\cdot s_{0\rightarrow T} + (1-\zeta)\cdot s_{T+1}
\end{aligned}
\]On the other hand, we drawn on Lemma 1 and set\[\tag{A2}
s_{0\rightarrow T+1} \sim \beta_0\cdot s_{0 \rightarrow T-1} + \beta_1\cdot s_T + (1-\beta_0-\beta_1)\cdot s_{T+1}
\]where \(\beta_0, \beta_1 > 0\). According to Axiom 4,
\(1-\zeta=1-\beta_0-\beta_1\). So, \(\beta_1=\zeta-\beta_0\). This also
implies \(\zeta > \beta_0\).

According to Axiom 2, we suppose there exists a reward sequence \(s\)
such that
\(s \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_T\).
By Equation (A2) and reduction of compound alternatives, we have
\(s_{0\rightarrow T+1}\sim \zeta \cdot s + (1-\zeta)\cdot s_{T+1}\).
Combining Equation (A2) with the second line of Equation (A1) and
applying transitivity and state-independence, we obtain
\(s_{0\rightarrow T} \sim \frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1\).

We aim to prove that for any \(s_{0\rightarrow T+1}\), we can obtain
\(\alpha=\frac{\beta_0}{\zeta}\). We show this by contradiction.

Given the symmetry of \(\alpha\) and \(\frac{\beta_0}{\zeta}\), we can
assume that \(\alpha > \frac{\beta_0}{\zeta}\). Consider the case that
\(s_{0 \rightarrow T-1} \succ s_T\). By state-independence, for any
\(c\in \mathbb{R}_{\geq 0}\), we have
\((\alpha - \frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c \succ (\alpha - \frac{\beta_0}{\zeta})\cdot s_T + (1-\alpha+\frac{\beta_0}{\zeta})\cdot c\).
By Axiom 2, there exists \(z\in \mathbb{R}_{\geq 0}\) such that
\((1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}\sim z\).
Given \(c\) is arbitrary, we can set
\((1-\alpha+\frac{\beta_0}{\zeta})\cdot c \sim z\). By reduction of
compound alternatives, we can derive that\[
(\alpha-\frac{\beta_0}{\zeta})\cdot s_{0\rightarrow T-1} +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1} \succ (\alpha-\frac{\beta_0}{\zeta})\cdot s_T +(1-\alpha)\cdot s_T + \frac{\beta_0}{\zeta}\cdot s_{0\rightarrow T-1}
\]where the LHS can be rearranged to
\(\alpha\cdot s_{0\rightarrow T-1} + (1-\alpha)\cdot s_T\), and the RHS
can be rearranged to
\(\frac{\beta_0}{\zeta}\cdot s_{0 \rightarrow T-1} + (1-\frac{\beta_0}{\zeta})\cdot s_1\).
They both should be indifferent from \(s_{0\rightarrow T}\). This
results in a contradiction.

Similarly, in the case that \(s_T \succ s_{0 \rightarrow T-1}\), we can
also derive such a contradiction. Meanwhile, when
\(s_{0\rightarrow T}\sim s_T\), \(\alpha\) and \(\frac{\beta_0}{\zeta}\)
can be any number within \((0,1)\). In that case, we can directly set
\(\alpha = \frac{\beta_0}{\zeta}\).

Thus, we have \(\alpha = \frac{\beta_0}{\zeta}\) for any
\(s_{0\rightarrow T+1}\), which indicates
\(\frac{\beta_0}{\alpha}=\frac{\beta_1}{1-\alpha}=\zeta\). We can
recursively apply this equality to any sub-sequence
\(s_{0\rightarrow t}\) (\(t\leq T\)) of \(s_{0\rightarrow T+1}\), so
that the lemma will be proved. \emph{QED}.

Now we move on to prove Proposition 1. The proof contains six steps.

First, we add the constraints \(\sum_{t=0}^T w_t=1\) and \(w_t>0\) to
the optimal discounting problem for \(s_{0\rightarrow T}\) so that the
problem can accommodate Lemma 1. According to the first-order condition
(FOC) of its solution, for all \(t=0,1,….,T\), we have\[\tag{A3}
f_t'(w_t)=u(s_t)+\theta
\]where \(\theta\) is the Lagrange multiplier. Given that \(f'_t(w_t)\)
is strictly increasing, \(w_t\) is increasing with \(u(s_t)+\theta\). We
define the solution as \(w_t =\phi_t(u(s_t)+\theta)\).

Second, we add a new reward \(s_{T+1}\) to the end of
\(s_{0\rightarrow T}\) and apply Lemma 2 as a constraint on optimal
discounting problem. Look at the optimal discounting problem for
\(s_{0\rightarrow T+1}\). For all \(t\leq T\), its FOC should take the
same form as Equation (A3). Hence, if the introduction of \(s_{T+1}\)
changes some \(w_t\) to \(w'_t\) (\(w'_t \neq w_t\), where \(w_t\) is
the solution to optimal discounting problem for \(s_{0\rightarrow T}\)),
the only way is through changing the multiplier \(\theta\). Suppose
introducing \(s_{T+1}\) changes \(\theta\) to \(\theta-\Delta \theta\),
we have \(w'_t = \phi_t(u(s_t)+\theta-\Delta \theta)\).

By Lemma 2, we know
\(\frac{w_0}{w'_0}=\frac{w_1}{w'_1}=…=\frac{w_T}{w'_T}\). In other
words, for \(t=0,1,…,T\), we have
\(w_t \propto \phi_t(u(s_t)+\theta-\Delta \theta)\). We can rewrite
\(w_t\) as \[\tag{A4}
w_t = \frac{\phi_t(u(s_t)+\theta-\Delta \theta)}{\sum_{\tau=0}^{T}\phi_\tau(u(s_\tau)+\theta-\Delta \theta)}
\]

Third, we show that in \(s_{0\rightarrow T}\), if we change each \(s_t\)
to \(z_t\) such that \(u(z_t)=u(s_t)+\Delta u\), the decision weights
\(w_0,…,w_T\) will remain the same. Note
\(\sum_{t=0}^T \phi_t(u(s_t)+\theta)=1\). It is clear that
\(\sum_{t=0}^T \phi_t(u(z_t)+\theta-\Delta u)=1\). Suppose changing
every \(s_t\) to \(z_t\) moves \(\theta\) to \(\theta'\) and
\(\theta'<\theta-\Delta u\). Then, we must have
\(\phi_t(u(z_t)+\theta')<\phi_t(u(z_t)+\theta-\Delta u)\) since
\(\phi_t(.)\) is strictly increasing. Summing all such decision weights
up will result in \(\sum_{t=0}^T \phi_t(u(z_t)+\theta')<1\), which
contradicts with the constraint that the sum of decision weights is 1.
The same contradiction can apply to the case that
\(\theta'>\theta-\Delta u\). Therefore, changing every \(s_t\) to
\(z_t\) must move \(\theta\) to \(\theta - \Delta u\), and each \(w_t\)
can only be moved to \(\phi_t(u(z_t)+\theta -\Delta u)\), which is
exactly the same as the original decision weight.

A natural corollary of this step is that, subtracting or adding a common
number to all instantaneous utilities within a reward sequence has no
effect on decision weights. What actually matters for determining the
decision weights is the difference between these instantaneous
utilities. This indicates, for convenience, we can subtract or add an
arbitrary number to the utility function.

In other words, for a given \(s_{0\rightarrow T}\) and \(s_{T+1}\), we
can define a new utility function \(v(.)\) such that
\(v(s_t) = u(s_t) +\theta-\Delta \theta\). So, Equation (A4) can be
rewritten as\[\tag{A5}
w_t = \frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))}
\]If \(w_t\) takes the AMD form under the utility function \(v(.)\),
i.e.~\(w_t \propto d_t e^{v(s_t)/\lambda}\), then it should also take
the AMD form under the original utility function \(u(.)\).

Fourth, we show that in Equation (A4), \(\Delta \theta\) has two
properties: (i) \(\Delta \theta\) is strictly increasing with
\(u(s_{T+1})\); (ii) suppose \(\Delta \theta = \underline{\theta}\) when
\(u(s_{T+1})=\underline{u}\) and \(\Delta\theta=\bar{\theta}\) when
\(u(s_{T+1})=\bar{u}\), where \(\underline{u}<\bar{u}\), then for any
\(l \in(\underline{\theta},\bar{\theta})\), there exists
\(u(s_{T+1})\in(\underline{u},\bar{u})\) such that
\(\Delta \theta = l\).

The property (i) can be shown by contradiction. Let
\(\{w'_t\}_{t=0}^{T+1}\) denote a sequence of decision weights for
\(s_{0\rightarrow T+1}\). Suppose \(u(s_{T+1})\) is increased but
\(\Delta \theta\) is constant. In this case, each of \(w'_0,…,w'_T\)
should also be constant. However, \(w'_{T+1}\) should increase as it is
strictly increasing with \(u(s_{T+1})+\theta-\Delta \theta\) (as
\(\theta\) is determined only by the optimal discounting problem for
\(s_{0\rightarrow T}\), any operations on \(s_{T+1}\) should have no
effect on \(\theta\)). This contradicts with the constraint that
\(\sum_{t=0}^{T+1} w'_t =1\). The only way to avoid such contradictions
is to set \(\Delta \theta\) strictly increasing with \(s_{T+1}\), so
that \(w'_0,…,w'_T\) are decreasing with \(u(s_{T+1})\).

For property (ii), note that given \(s_{0\rightarrow T+1}\) and
\(\theta\), \(\Delta\theta\) is defined as the solution to
\(\sum_{t=0}^{T+1} \phi_t(u(s_t)+\theta-\Delta\theta)=1\). For any
arbitrary number \(l\in(\underline{\theta},\bar{\theta})\), the proof of
property (ii) consists of two stages. First, for period \(t=0,1,…,T\),
we need to show \(u(s_t)+\theta-l\) is in the domain of \(\phi_t(.)\).
Second, for period \(T+1\), we need to show given any
\(\omega\in(0,1)\), there exists \(u(s_{T+1})\in \mathbb{R}\) such that
\(\phi_{T+1}(u(s_{T+1})+\theta-l)=\omega\).

For the first stage, note \(\phi_t(.)\) is the inverse function of
\(f'_t(.)\). Suppose when \(\Delta\theta=\bar{\theta}\), we have
\(f'_t(w^{a}_t)=u(s_t)+\theta-\bar{\theta}\), and when
\(\Delta\theta=\underline{\theta}\), we have
\(f'_t(w^{b}_t)=u(s_t)+\theta-\underline{\theta}\). For any
\(l\in(\underline{\theta},\bar{\theta})\), we have
\(u(s_t)+\theta-l \in (f'_t(w^a_t),f'_t(w^b_t))\). Given that
\(f'_t(.)\) is continuous and strictly increasing, there must be
\(w_t\in(w^a_t,w^b_t)\) such that \(f'_t(w_t)=u(s_t)+\theta-l\). So,
\(u(s_t)+\theta-l\) is in the domain of \(\theta_t(.)\). For the second
stage, given an arbitrary \(\omega\in(0,1)\), we can directly set
\(u(s_{T+1})=f'(\omega)-\theta+l\), so that the target condition is
satisfied.

A corollary of this step is that we can manipulate \(\Delta \theta\) in
Equation (A4) at any level between \([\underline{\theta},\bar{\theta}]\)
by changing a hypothetical \(s_{T+1}\).

Fifth, we show \(\ln \phi_t(.)\) is linear under some condition. To do
this, let us add a hypothetical \(s_{T+1}\) to the end of \(s_T\) and
let \(w'_t=\phi_t(v(s_t))\) denote the decision weights for
\(s_{0\rightarrow T+1}\). We can change the hypothetical \(s_{T+1}\)
within the set \(\{s_{T+1}|v(s_{T+1})\in[\underline{v},\bar{v}]\}\) and
see what will happen to the decision weights from period 0 to period
\(T\). Suppose this changes each \(w'_t\) to \(\phi_t(v(s_t)-\eta)\).
Set \(\eta=\underline{\eta}\) when \(u(s_{T+1})=\underline{v}\) and
\(\eta=\bar{\eta}\) when \(u(s_{T+1})=\bar{v}\). By Equation (A5), we
have\[\tag{A6}
\frac{\phi_t(v(s_t))}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau))} = \frac{\phi_t(v(s_t)-\eta)}{\sum_{\tau=0}^{T}\phi_\tau(v(s_\tau)-\eta)}
\]For each \(t=0,1,...,T\), we can rewrite \(\phi_t(v(s_t))\) as
\(e^{\ln \phi_t(v(s_t))}\). For the LHS of Equation (A6), multiplying
both the numerator and the denominator by a same number will not affect
the value. Therefore, Equation (A6) can be rewritten as \[
\frac{e^{\ln\phi_t(v(s_t))-\kappa\eta}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau))-\kappa\eta}} = \frac{e^{\ln\phi_t(v(s_t)-\eta)}}{\sum_{\tau=0}^{T}e^{\ln\phi_\tau(v(s_\tau)-\eta)}}
\]where \(\kappa\) can be any constant number. By properly selecting
\(\kappa\), for all \(t=0,1,...,T\), we can obtain\[\tag{A7}
\ln \phi_t(v(s_t))-\kappa\eta=\ln \phi_t(v(s_t)-\eta)
\]as long as \(\eta \in [\underline{\eta},\bar{\eta}]\). Since
\(\ln\phi_t(.)\) is strictly increasing, for any \(\eta\neq 0\), we have
\(\kappa>0\).

Finally, we denote the maximum and minimum of \(\{v(s_t)\}_{t=0}^T\) by
\(v_{\max}\) and \(v_{\min}\), and show that Equation (A7) can hold if
\(\eta = v_{\max} - v_{\min}\). That implies
\(v_{\max}-v_{\min}\in [\underline{\eta},\bar{\eta}]\), where
\(\underline{\eta}, \bar{\eta}\) are the realizations of \(\eta\) at the
points of \(v(s_{T+1})=\underline{v}\) and \(v(s_{T+1})=\bar{v}\).
Obviously, \(\underline{\eta}\) can take the value
\(\underline{\eta}=0\). Thus, we focus on whether \(\bar{\eta}\) can
take a value \(\bar{\eta}\geq v_{\max}-v_{\min}\).

The proof is similar with the fourth step and consists of two stages.
First, for \(t=0,1,…,T\), we show \(v(s_t)-v_{\max}+v_{\min}\) is in the
domain of \(\phi_t(.)\). That is, under some \(w_t\), we have
\(f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}\). Note in a non-constant reward
sequence, \(v_{\max}-v_{\min}\in(0,+\infty)\). On the one hand, Equation
(A5) indicates that the equation \(f'_t(\omega)=v(s_t)\) has a solution
\(\omega\). On the other hand, by Definition 2, we know
\(\lim_{w_t\rightarrow 0}f'_t(w_t)=-\infty\). Given \(f'_t(w_t)\) is
continuous and strictly increasing, there must be a solution \(w_t\)
lying in \((0,\omega)\) for equation
\(f'_t(w_t)=v(s_t)-v_{\max}+v_{\min}\). Second, we show that for any
\(\omega'\in(0,1)\), there exists some \(v(s_{T+1})\) such that
\(\phi_{T+1}(v(s_{T+1})-v_{\max}+v_{\min})=\omega'\). This can be
achieved by setting \(v(s_{T+1})=f'_{T+1}(\omega')+v_{\max}-v_{\min}\).

As a result, for any period \(t\) in \(s_{0\rightarrow T}\), by Equation
(A7), we have \(\ln \phi_t(v(s_t))=\ln\phi_t(v(s_t)-\eta)+\kappa\eta\)
so long as \(\eta\in[0,v_{\max}-v_{\min}]\), where \(\kappa>0\). We can
rewrite each \(\ln \phi_t(v(s_t))\) as
\(\ln \phi_t(v_{\min})+\kappa[v(s_t)-v_{\min}]\). Therefore, we
have\[\tag{A8}
w_t \propto \phi_t(v_{\min})\cdot e^{\kappa[v(s_t)-v_{\min}]}
\]and \(\sum_{t=0}^T w_t=1\). In Equation (A8), setting
\(\phi_t(v_{\min})=d_t\), \(\lambda = 1/\kappa\), and apply the
corollary of the third step, we can conclude that
\(w_t\propto d_t e^{u(s_t)/\lambda}\), which is of the AMD form.

\hypertarget{b.-proof-of-proposition-2}{%
\subsection*{B. Proof of Proposition
2}\label{b.-proof-of-proposition-2}}
\addcontentsline{toc}{subsection}{B. Proof of Proposition 2}

Note the instantaneous utilities of LL and SS are \(v_l\) and \(v_s\),
and the delays for LL and SS are \(t_l\) and \(t_s\). According to
Equation (4), the common difference effect implies that, if\[ \tag{B1}
\frac{v_s}{1+G(t_s)e^{-v_s}} = \frac{v_l}{1+G(t_l)e^{-v_l}}
\]then for any \(\Delta t \geq 0\), we have \[ \tag{B2}
\frac{v_s}{1+G(t_s+\Delta t)e^{-v_s}} < \frac{v_l}{1+G(t_l+\Delta t)e^{-v_l}}
\]If \(G(T)=T\), we have \(G(t+\Delta t) = G(t) + \Delta t\). In this
case, combining Equation (B1) and (B2), we can obtain\[ \tag{B3}
\frac{\Delta t e^{-v_s}}{v_s} > \frac{\Delta t e^{-v_l}}{v_l}
\]Given that function \(\psi(v) = e^{-v}/v\) is decreasing with \(v\) so
long as \(v>0\), Equation (B3) is valid.

If \(G(T) = \frac{1}{1-\delta}(\delta^{-T}-1)\), we have\[\tag{B4}
1+G(t+\Delta t)e^{-v} = \delta^{-\Delta t}[1+G(t)e^{-v}]+(\delta^{-\Delta t}-1)(\frac{e^{-v}}{1-\delta}-1)
\] Thus, combining Equation (B1) and (B2), we can obtain\[\tag{B5}
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_s}}{1-\delta}-1}{v_s} >
(\delta^{-\Delta t}-1)\frac{\frac{e^{-v_l}}{1-\delta}-1}{v_l}
\] Given that \(0<\delta<1\), we have \(\delta^{-\Delta t}>1\). So,
Equation (B5) is valid if and only if\[\tag{B6}
\frac{1}{v_s}-\frac{1}{v_l}<\frac{1}{1-\delta}(\frac{e^{-v_s}}{v_s}-\frac{e^{-v_l}}{v_l})
\] By Equation (B1), we know that\[\tag{B7}
\frac{1}{v_s}-\frac{1}{v_l}=\frac{1}{1-\delta}\left[\frac{(\delta^{-t_l}-1)e^{-v_l}}{v_l} -\frac{(\delta^{-t_s}-1)e^{-v_s}}{v_s}\right]
\] Combining Equation (B6) and (B7), we have\[
\delta^{-t_l}\frac{e^{-v_l}}{v_l}<\delta^{-t_s}\frac{e^{-v_s}}{v_s} \Longleftrightarrow v_l - v_s + \ln \left(\frac{v_l}{v_s}\right)>-(t_l-t_s)\ln\delta
\]

\hypertarget{c.-proof-of-proposition-3}{%
\subsection*{C. Proof of Proposition
3}\label{c.-proof-of-proposition-3}}
\addcontentsline{toc}{subsection}{C. Proof of Proposition 3}

Suppose a positive reward \(x\) is delivered at period \(T\). By
Equation (4), if \(w_T\) is convex in \(T\), we should have
\(\frac{\partial^2 w_T}{\partial T^2}\geq 0\). This
implies\[\tag{C1} 2G'(T)^2\geq(G(T)+e^{v(x)})G''(T) \]

If \(\delta=1\), then \(G(T)=T\). We have \(G'(T)=1\), \(G''(T)=0\).
Thus, Equation (C1) is always valid.

If \(0<\delta<1\), then \(G(T)=(1-\delta)^{-1}(\delta^{-T}-1)\). We have
\(G'(T)=(1-\delta)^{-1}(-\ln\delta)\delta^{-T}\),
\(G''(t)=(-\ln\delta)G'(T)\). Thus, Equation (C1) is valid when
\[\tag{C2} \delta^{-T}\geq(1-\delta)e^{v(x)}-1 \]Given \(T>0\), Equation
(C2) holds true in two cases. The first case is
\(1\geq (1-\delta)e^{v(x)}-1\), which implies that \(v(x)\) is no
greater than a certain threshold \(v(\underline{x})\), where
\(v(\underline{x})=\ln(\frac{2}{1-\delta})\). The second case is that
\(v(x)\) is above \(v(\underline{x})\) and \(T\) is above a threshold
\(\underline{t}\). In the second case, we can take the logarithm on both
sides of Equation (C2). It yields
\(\underline{t}=\frac{\ln[(1-\delta)\exp\{v(x)\}-1]}{\ln(1/\delta)}\).

\hypertarget{d.-proof-of-proposition-4}{%
\subsection*{D. Proof of Proposition
4}\label{d.-proof-of-proposition-4}}
\addcontentsline{toc}{subsection}{D. Proof of Proposition 4}

For convenience, we use \(v\) to represent \(v(x)\equiv u(x)/\lambda\),
and use \(U\) to represent \(U(x,T)\). Set \(g= G(T)\). The first-order
derivative of \(U\) with respect to \(x\) can be written as\[\tag{D1}
\frac{\partial U}{\partial x}=v'\frac{e^v+U}{e^v+g}
\]If \(U\) is strictly concave in \(x\), we should have
\(\frac{\partial^2 U}{\partial x^2}<0\). By Equation (D1), we calculate
the second-order derivative of \(U\) with respect to \(x\), and
rearrange this second-order condition to\[\tag{D2}
2\zeta(v)+\frac{1}{1+v\zeta(v)}-1<\frac{-v''}{(v')^2}\equiv\frac{d}{dx}\left(\frac{1}{v'}\right)
\]

where \(\zeta(v)=g/(g+e^v)\). Since \(v''<0\), the RHS of Equation (D2)
is clearly positive.

To prove the first part of Proposition 4, we can show that when \(x\) is
large enough, the LHS of Equation (D2) will be non-positive. To make the
LHS non-positive, we require\[\tag{D3}
\zeta(v)+\frac{1}{v}\leq\frac{1}{2}
\]hold true. Note that \(\zeta(v)\) is decreasing in \(v\), and \(v\) is
increasing in \(x\). Hence, \(\zeta(v)+\frac{1}{v}\) is decreasing in
\(x\). Besides, it approaches \(+\infty\) when \(x\rightarrow0\) and
approaches 0 when \(x\rightarrow +\infty\). When
\(\frac{d}{dx}\left(\frac{1}{v'(x)}\right)\) is continuous, there must
be a unique realization of \(x\) in \((0,+\infty)\), say \(\bar{x}\),
making the equality in Equation (D3) valid. Moreover, when
\(x\geq\bar{x}\), Equation (D3) is always valid. In such cases,
\(U(x,T)\) is concave in \(x\).

To prove the second part, first note that when \(x=0\), the LHS of
Equation (D2) will become \(\frac{2g}{g+1}\). If
\(\frac{d}{dx}\left(\frac{1}{v'(0)}\right)\) is smaller than this
number, then the LHS of Equation (D2) should be greater than the RHS at
the point of \(x=0\). Meanwhile, from the first part of the current
proposition, we know the LHS is smaller than the RHS at the point of
\(x=\bar{x}\). Thus, given \(\frac{d}{dx}\left(\frac{1}{v'(x)}\right)\)
is continuous in \([0,\bar{x}]\), there must also be a point within
\([0,\bar{x}]\), such that the LHS equals the RHS. Let \(x^*\) denote
the minimum of \(x\) that makes the equality valid. Then, for any
\(x\in(0,x^*)\), we must have that the LHS of Equation (D2) is greater
than the RHS, which implies \(U(x,T)\) is convex in \(x\). Given that
\(T\geq1\), we have \(g\geq1\) and thus \(\frac{2g}{g+1}\geq 1\).
Therefore, when \(\frac{d}{dx}\left(\frac{1}{v'(0)}\right)<1\),
\(U(x,t)\) can be convex in \(x\) for any \(x\in(0,x^{*})\), regardless
of \(g\).

The prove the third part, note \(v(x)=u(x)/\lambda\). So, \[
\frac{d}{dx}\left(\frac{1}{v'}\right)=\lambda\frac{d}{dx}\left(\frac{1}{u'}\right)
\]We arbitrarily draw a point from \((0,\bar{x})\) and derive the range
\(\lambda\) relative to this point. For simplicity, we choose
\(x=\ln g\). In this case, the LHS of Equation (D2) becomes
\(\frac{2}{2+\ln g}\). Define a function \(\xi(x)\), where \(\xi\) is
the value of the LHS of Equation (D2) minus its RHS. Note \(\xi(x)\) is
continuous at \(x=\ln g\). Therefore, for any positive real number
\(b\), there must exist a positive real number \(c\) such that, when
\(x\in(\ln g-c,\ln g+c)\), we have\[\tag{D4}
\xi(\ln g)-b<\xi(x)<\xi(\ln g)+b
\]If \(\xi(\ln g)-b\geq 0\), then \(\xi(x)\) will keep positive for all
\(x\in(\ln g-c,\ln g+c)\), which implies the LHS of Equation (D2) is
always greater than its RHS.

Now we derive the condition for \(\xi(\ln g)-b\geq 0\). Suppose when
\(x=\ln g\), \(\frac{d}{dx}\left(\frac{1}{u'}\right)=a\) (note at this
point we have \(\frac{d}{dx}\left(\frac{1}{u'}\right)<+\infty\)).
Combining with Equation (D3), we know that
\(\xi(\ln g)-b =\frac{2}{2+\ln g}-\lambda a-b\). Letting this value be
non-negative, we obtain\[\tag{D5}
\lambda \leq \frac{2}{a(2+\ln g)}-\frac{b}{a}
\]Given that \(T\geq1\), we have \(g\geq 1\) and thus
\(\frac{2}{2+\ln g}\) should be positive. Meanwhile, given that \(u'>0\)
and \(u''<0\), \(a\) should also be positive. Since \(b\) can be any
positive number, Equation (D5) holds if
\(\lambda <\frac{2}{a(2+\ln g)}\). That is, when \(\lambda\) is positive
but smaller than a certain threshold, there must be an interval
\((\ln g-c,\ln g+c)\) such that the LHS of Equation (D2) is greater than
the RHS. Set \(x_1 = \max\{0,\ln g-c\}\),
\(x_2=\min\{\bar{x}, \ln g +c\}\). When \(x\in (x_1,x_2)\), function
\(U(x,T)\) must be convex in \(x\).

\hypertarget{e.-proof-of-proposition-5}{%
\subsection*{E. Proof of Proposition
5}\label{e.-proof-of-proposition-5}}
\addcontentsline{toc}{subsection}{E. Proof of Proposition 5}

The proof consists of four steps. First, we write the expressions for
\(U(L1)\) and \(U(L2)\). Suppose the time length of each lottery result
is \(T\). For a period \(\tau\) at which no reward is delivered, the
instantaneous utility is zero. Let \(\Omega\) denote the set of all such
period \(\tau\), then
\(\Omega=\{\tau\,|\,0\leq\tau\leq T,\;\tau \neq t_1,t_2\}\). For any
\(j,k\in\{s,l\}\), we define \(\phi_j=d_{t_1}e^{v(x_j)}\) and
\(\eta_k=d_{t_2}e^{v(y_k)}\), where \(v(s)=u(s)/\lambda\), and \(d_t\)
represents the default discount factor for reward delivered at period
\(t\).

For a given lottery result \((s_1,s_2)\), we denote the decision weight
of each positive reward by \(w_{t_1}\) and \(w_{t_2}\). By the
definition of AMD, we have\[
w_{t_1} = \frac{\phi_j}{\phi_j + \eta_k +D} \quad ,\quad
w_{t_2} = \frac{\eta_k}{\phi_j + \eta_k +D}
\]where \(j,k\in\{s,l\}\), \(D=\sum_{\tau\in\Omega} d_{\tau}\geq 0\).
The value of a lottery \(L\) can be written as
\(U(L)=w_{t_1}u(s_1)+w_{t_2}u(s_2)\). Hence, \[\tag{E1}
\begin{aligned}
U(L1)=0.5\frac{\phi_s u(x_s)+\eta_s u(y_s)}{\phi_s+\eta_s+D} + 0.5\frac{\phi_l u(x_l)+\eta_l u(y_l)}{\phi_l+\eta_l+D} \\
U(L2)=0.5\frac{\phi_s u(x_s)+\eta_l u(y_l)}{\phi_s+\eta_l+D} + 0.5\frac{\phi_l u(x_l)+\eta_s u(y_s)}{\phi_l+\eta_s+D}
\end{aligned}
\]We observe that, when \(x_l=x_s\), we have \(U(L1)=U(L2)\).

Second, suppose we increase \(x_l\) from \(x_s\) by an increment. This
increases both \(U(L1)\) and \(U(L2)\) (either by a positive or a
negative number). To make \(U(L1)<U(L2)\), this increment should
increase \(U(L2)\) by a greater number than \(U(L1)\). Specifically, we
assume \(U(L2)\) is increasing faster than \(U(L1)\) at any level of
\(x_l\). That is, the partial derivative of \(U(L2)\) in terms of
\(x_l\) is always greater than that of \(U(L1)\). Given \(\phi_l\) is
increasing in \(x_l\), to see this, we can take partial derivatives in
terms of \(\phi_l\).

In each line of Equation (E1), note only the second term contains
\(x_l\). Thus, we focus on the difference between the second terms. The
second term of the \(U(L1)\) is influenced by \(y_l\), while that of the
\(U(L2)\) is influenced by \(y_s\), where \(y_l>y_s\). Thus, we can
construct a function \(\xi\) such that\[
\xi(\phi_l,\eta) = \frac{\phi_l \cdot v(x_l)+\eta\cdot v(y)}{\phi_l+\eta+D}
\]where \(\eta=d_{t_2}e^{v(y)}\). In reverse, we can define
\(v(x_l)=\ln(\phi_l/d_{t_1})\) and \(v(y)=\ln(\eta/d_{t_2})\). The
function \(\xi\) is similar to the second term of each line, but note we
replace \(u(.)\) by \(v(.)\). When \(y=y_l\), \(\xi\) is proportional to
the second term of \(U(L1)\). When \(y=y_s\), \(\xi\) is proportional to
the second term of \(U(L2)\) (by the same proportion). Thus, to show
that the partial derivative of \(U(L2)\) in terms of \(x_l\) is greater
than that of \(U(L1)\), we just need to show
\(\partial \xi/\partial \phi_l\) is decreasing with \(y\) (or \(\eta\)).

Third, we take the first- and second-order partial derivatives of
\(\xi(\phi_l,\eta)\). The partial derivative of \(\xi\) in terms of
\(\phi_l\) is\[
\frac{\partial \xi}{\partial \phi_l}=\frac{(v(x_l)+1)\eta-v(y)\eta+\phi_l+D(v(x_l)+1)}{(\phi_l+\eta+D)^2}
\]We need to show that for \(y\in[y_s,y_l]\), we can obtain
\(\partial^2 \xi/\partial \phi_l\partial \eta<0\). This
implies\[\tag{E2}
(v(x_l)+v(y)+2)D-(\phi_l-\eta)(v(x_l)-v(y))+2(\phi_l+\eta)>0
\]We want Equation (E2) to hold for any \(D\geq 0\). Given the LHS is
increasing with \(D\), this can only be achieved when\[\tag{E3}
2(\phi_l+\eta)>(\phi_l-\eta)(v(x_l)-v(y))
\]Define \(\kappa=d_{t_2}/d_{t_1}\), \(\alpha=v(x_l)-v(y)\). Note
\(\kappa\in \mathbb{R}_{>0}\), \(\alpha\in\mathbb{R}\). Equation (E3)
can be rewritten as\[\tag{E4}
(\alpha-2)\kappa^{-1} e^{\alpha}-\alpha-2<0
\]

Fourth, based on Equation (E4), we construct a function
\(h(\alpha)=(\alpha-2)\kappa^{-1} e^\alpha-\alpha-2\). We aim to examine
whether there exists some \(\alpha\in\mathbb{R}\) that makes \(h(a)<0\).
Obviously, \(\alpha=-2\) and \(\alpha=2\) satisfy this condition.
Moreover, note \(h(\alpha)\) is decreasing in \(\alpha\) when
\((\alpha-1)e^{\alpha}\leq \kappa\) and is increasing in \(\alpha\)
otherwise. And when either \(\alpha\rightarrow -\infty\) or
\(\alpha \rightarrow +\infty\), we have
\(h(\alpha)\rightarrow +\infty\). Thus, there must be a limited interval
\((\alpha_1,\alpha_2)\) such that \(h(a)<0\) so long as
\(\alpha\in(\alpha_1,\alpha_2)\), and obviously
\([-2,2]\subset(\alpha_1,\alpha_2)\). Since \(v(s)=u(s)/\lambda\), this
implies \(\frac{u(x_l)-u(y)}{\lambda}\in(\alpha_1,\alpha_2)\).

For a given positive number \(\kappa\), the points \(\alpha_1,\alpha_2\)
are determined by the solution to
\(\frac{\alpha-2}{\alpha+2}e^{\alpha}=\kappa\). In other words, for any
\(x_l\) and \(y\in[y_s,y_l]\), we can always achieve \(U(L1)<U(L2)\) as
long as \(u(x_l)-u(y_l)\geq \lambda\alpha_1\) and
\(u(x_l)-u(y_s)\leq\lambda\alpha_2\). So, we can conclude that for any
\(x_l>x_s>0\), \(y_l>y_s>0\), any time length of lottery results and
default discount factor (which determines \(D\) and \(\kappa\)), there
exists some \(\lambda\) that makes DM intertemporal correlation averse.
Specifically, all
\(\lambda>\lambda^{**} ={\max}\{\frac{u(x_l)-u(y_l)}{\alpha_1},\frac{u(x_l)-u(y_s)}{\alpha_2}\}\)
satisfy the target condition.

Notably, if \(\lambda\leq\lambda^{**}\), we have \(h(a)\geq0\), which by
Equation (E2)(E3), indicates that under some conditions such as \(D=0\),
there will be \(\partial^2 \xi/\partial \phi_l\partial \eta\geq0\) for
all \(y\in[y_s,y_l]\). In that case, at each level of \(x_l\), the
partial derivative of \(U(L1)\) in terms of \(x_l\) is greater than that
of \(U(L2)\). So, increasing \(x_l\) by an increment from \(x_s\) can
induce a greater increase in \(U(L1)\) than in \(U(L2)\). This makes it
possible that \(U(L1)>U(L2)\). In short, DM may perform intertemporal
correlation seeking when \(\lambda\leq\lambda^{**}\).

\hypertarget{f.-proof-of-proposition-6}{%
\subsection*{F. Proof of Proposition
6}\label{f.-proof-of-proposition-6}}
\addcontentsline{toc}{subsection}{F. Proof of Proposition 6}

Before proving the proposition, we first show that in the DM's optimal
consumption plan \(s_{0\rightarrow T}\), the largest consumption must be
\(s_0\). To show this, suppose the largest consumption is \(s_\tau\)
(\(\tau>0\)). By Lemma 3 below, we can obtain that if we exchange the
consumption planned in \(\tau\) with the consumption planned in period
0, the total value of consumption will be non-decreasing. So, the
largest consumption must be placed in period 0.

For convenience, henceforth we use \(u_t\) to represent \(u(s_t)\).

\noindent \textbf{Lemma 3}: \emph{Suppose in}
\(s_{0\rightarrow T}\)\emph{, we have}
\(s_\tau= \max\{s_0,s_1,...,s_T\}\) \emph{and} \(\tau>0\). \emph{Set}
\(u_0/\lambda=v_1\) \emph{and} \(u_{\tau}/\lambda=v_2\)\emph{. If we
change} \(u_0/\lambda\) \emph{to} \(v_2\) \emph{and}
\(u_{\tau}/\lambda\) \emph{to} \(v_1\)\emph{,} \(U(s_{0\rightarrow T})\)
\emph{will be non-decreasing.}

\noindent \emph{Proof}: Let \(V/\lambda\) denote the total value of
consumption before we exchange consumption between period 0 and
\(\tau\), where\[
V = \frac{\sum_{t=0}^T (u_t/\lambda)\cdot d_t e^{u_t/\lambda}}{\sum_{t=0}^T d_te^{u_t/\lambda}}
\]Set \(d_0=\delta_1\), \(d_\tau=\delta_2\). We denote the numinator of
\(V\) by \(v_1\delta_1e^{v_1}+v_2\delta_2e^{v_2}+P\) and denote its
denominator by \(\delta_1e^{v_1}+\delta_2e^{v_2}+Q\).

Note \(v_2\geq V\). If changing \(u_t/\lambda\) to \(v_2\) as well as
\(u_{t+\tau}/\lambda\) to \(v_1\) does not decrease \(V\), we should
have
\[\tag{F1} \frac{v_1\delta_1e^{v_1}+ v_2\delta_2e^{v_2}+P}{\delta_1e^{v_1}+\delta_2e^{v_2}+Q} \leq  \frac{v_2\delta_1e^{v_2}+ v_1\delta_2e^{v_1}+P}{\delta_1e^{v_2}+\delta_2e^{v_1}+Q} \]where
\(\delta_1>\delta_2>0\), \(v_2\geq v_1>0\). By rearranging Equation
(F1), we can
obtain\[\tag{F2} -(\delta_1+\delta_2)e^{v_1+v_2}(v_2-v_1)\leq e^{v_2}(Qv_2-P)-e^{v_1}(Qv_1-P) \]Clearly,
Equation (F2) holds if \(e^v(Qv-P)\) is increasing with \(v\) when
\(v\in[v_1,v_2]\), and the latter implies \(v_1\geq \frac{P}{Q}-1\).

Notably, \(V\) is a weighted mean of \(v_1\), \(v_2\) and
\(\frac{P}{Q}\), and we have \(v_2\geq \max\{v_1,\frac{P}{Q}\}\). If
\(v_1\geq V\), we must have \(v_1\geq \frac{P}{Q}\). In this case,
Equation (F2) clearly holds. If \(v_1<V\), note that
\(\frac{\partial U}{\partial d_t}\propto u_t-U\). So, we will have
\(\frac{\partial U}{\partial d_0}<0\) and
\(\frac{\partial U}{\partial d_\tau}>0\). Both decreasing \(d_0\) to
\(\delta_2\) and increasing \(d_\tau\) to \(\delta_1\) would increase
the total value of consumption. In summary, for either case, after
changing \(u_t/\lambda\) to \(v_2\) and \(u_{t+\tau}/\lambda\) to
\(v_1\), \(V\) should be non-decreasing. \emph{QED}.

Denote the value of \(s_{0\rightarrow T}\) by
\(U=\sum_{t=0}^T w_t u(s_t)\). Notably, if the optimal consumption plan
is an interior solution, then the solution must satisfy
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=1\).
Suppose
\(\frac{\partial U}{\partial s_t}>\frac{\partial U}{\partial s_{t+1}}>0\),
then the DM can transfer an incremental consumption from \(s_{t+1}\) to
\(s_t\), as increasing \(s_t\) by an increment will lead to a greater
improvement in \(U\) compared to the decrease in \(U\) caused by
reducing \(s_{t+1}\) by the same amount. The DM will keep transfer
consumption between periods until
\(\frac{\partial U}{\partial s_t}=\frac{\partial U}{\partial s_{t+1}}\).
Nevertheless, if the DM keeps reducing \(s_{t+1}\) and still has
\(\frac{\partial U}{\partial s_t}>\frac{\partial U}{\partial s_{t+1}}>0\)
even when \(s_{t+1}\) is reduced to 0, this optimization problem will
reach a corner solution. We aim to show that when \(\lambda\) is small
enough, for all \(t>0\), the DM would tend to reduce \(s_t\) to zero.

The partial derivatives of \(U\) in terms of \(s_t\) is
\(\frac{\partial U}{\partial s_t}=w_tu'_t(u_t+\lambda-U)\). Therefore,
we have \[\tag{F3}
\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_t} = 
\frac{d_{t+1}}{d_t} \exp\{\frac{u_{t+1}-u_t}{\lambda}\}
\cdot\frac{u'_{t+1}}{u'_t}
\cdot\frac{u_{t+1}+\lambda-U}{u_t+\lambda-U}
\]Drawing on Equation (F3), we construct a function
\(\rho(s_t;\mathcal{U})=e^{u_t/\lambda}u'_t(u_t+\lambda-\mathcal{U})\).
Its partial derivative in terms of \(s_t\) is\[\tag{F4}
\frac{\partial \rho(s_t;\mathcal{U})}{\partial s_t} = 
e^{u_t/\lambda}[
(u_t+\lambda-\mathcal{U})(\frac{(u'_t)^2}{\lambda}+u''_t)
+(u'_t)^2]
\]To prove Proposition 6, note that if\[\tag{F5}
\lambda\leq \min_{0 \leq t \leq T} \;\inf_{s_{0\rightarrow T}\in A}\{-(u'_t)^2/u''_t\}
\]we will always have \(\frac{(u'_t)^2}{\lambda}+u''_t \geq 0\). Set
\(\underline{\lambda}\) as the RHS of Equation (F5). Note by Lemma 3, we
know that \(s_0\) must be the largest consumption in the DM's
consumption plan. It can be proved that if
\(\lambda\leq \underline{\lambda}\), in an arbitrary sequence
\(s_{0\rightarrow T}\) that satisfies this condition, it is always
beneficial for the DM to transfer consumption from the future periods to
the current period, until all consumption is concentrated at the current
period. We discuss this in two cases.

First, suppose for all \(t>0\), we have \(u_t+\lambda -U>0\). As \(s_0\)
is the largest consumption, we must have \(u_0>U\); so, we also have
\(u_0+\lambda-U>0\). In this case, if
\(\lambda\leq \underline{\lambda}\), according to Equation (F4), we can
obtain \(\frac{\partial \rho(s_t;U)}{\partial s_t}>0\). By Equation
(F3), for all \(t>0\), we have
\(\frac{\partial U}{\partial s_t}/\frac{\partial U}{\partial s_0}=\frac{d_{t}}{d_0} \frac{\rho(s_t;U)}{\rho(s_0;U)}\).
Since \(d_0>d_t\) and \(s_0>s_t\), we can obtain
\(\frac{\partial U}{\partial s_0}>\frac{\partial U}{\partial s_t}>0\).
Therefore, it is beneficial for the DM to transfer an incremental
consumption from \(s_t\) to \(s_0\).

Second, suppose for some period \(\tau>0\), we have
\(u_\tau+\lambda-U< 0\). For this period \(\tau\), there must be
\(\frac{\partial U}{\partial s_\tau}<0\). A reduction in \(s_\tau\) will
increase \(U\). So, it is also beneficial to reduce it and transfer the
consumption to the current period.

In conclusion, in any sequence, as long as \(s_0\) is the largest
consumption, transferring consumption from the future to the present
must improve \(U\). If in such a sequence, a future period has a
positive consumption, the DM will keep transfer it to the current period
until she concentrate all consumption into the current period.

\hypertarget{g.-proof-of-proposition-7}{%
\subsection*{G. Proof of Proposition
7}\label{g.-proof-of-proposition-7}}
\addcontentsline{toc}{subsection}{G. Proof of Proposition 7}

We use the same notation as in the proof of Proposition 6. Before
proving Proposition 7, we list a set of conditions that, taken together,
are sufficient to derive the resulting behavior in the proposition. We
then prove that all these conditions are satisfied as long as
\(\lambda\) is large enough.

First, suppose the consumption planing problem has an interior solution.
Again, let \(U\) denote the value of a consumption sequence. Then, for
any period \(t\) in the optimal plan, we should have
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=1\)
and \(\frac{\partial^2 U}{\partial s_t^2}<0\). The second-order
condition
implies\[\tag{G1} (1-2w_t)\frac{1}{\lambda} + \frac{1}{u_t+\lambda-U} < -\frac{u''_t}{(u'_t)^2}  \]Second,
to analyze how a change in \(s_t\) will affect
\(\frac{\partial U}{\partial s_t}\), we construct a function
\(\tilde{\rho}_t(s_0,s_1,...,s_T)\equiv\rho(s_t;U)\). By calculating the
partial derivative of \(\tilde{\rho}_t\) in terms of \(s_t\), when
\(u_t+\lambda-U>0\), we can
obtain\[\tag{G2} \frac{\partial \tilde{\rho}_t}{\partial s_t} <0 \;\Longleftrightarrow\; (1-w_t)\frac{1}{\lambda}+\frac{1}{u_t+\lambda-U}<-\frac{u''_t}{(u'_t)^2} \]Third,
when \(u_t+\lambda-U>0\), according to Equation (F4), we can also obtain
that \(\rho(s_t;U)\) decreases in \(s_t\) if and only
if\[\tag{G3} \frac{1}{\lambda}+\frac{1}{u_t+\lambda-U}<-\frac{u''_t}{(u'_t)^2} \]Clearly,
if Equation (G3) holds, both Equation (G1) and (G2) will also hold. Note
\(u(m)>U\); so, if \(\lambda - u(m)\geq 0\), we will always have
\(u_t+\lambda-U>0\). Under this interval of \(\lambda\), the LHS of
Equation (G3) is continuous and decreasing in \(\lambda\), and it
converges to 0 with \(\lambda \rightarrow 0\). Hence, there must be some
\(\bar{\lambda}_1 \geq u(m)\) such that, when
\(\lambda\geq \bar{\lambda}_1\), for any given consumption sequence,
Equation (G3) is valid.

Fourth, we construct a new function
\(\Gamma(s_t;\mathcal{U})=e^{u_t/\lambda}\cdot\left(\frac{u_t+\lambda}{u_t+\lambda -\mathcal{U}}\right)\).
By calculating the partial derivative of \(\Gamma\) in terms of \(s_t\),
when \(u_t+\lambda-U>0\), we can obtain\[
\tag{G4} \frac{\partial\Gamma(s_t;U)}{\partial s_t}>0 \Longleftrightarrow (\lambda +u_t-U)^2+U(u_t-U)>0
\]If \(\lambda\) is large enough, Equation (G4) must hold. Again, we can
conclude that there is some \(\bar{\lambda}_2\geq u(m)\), such that when
\(\lambda \geq \bar{\lambda}_2\), for any given sequence, Equation (G4)
is valid.

We define \(\bar{\lambda}\) as
\(\max\{\bar{\lambda}_1,\bar{\lambda}_2\}\). It can be shown that any
\(\lambda\in[\bar{\lambda},+\infty)\) satisfies the proposition.

For the part (a) of Proposition 7, note by Equation (F3), we have
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_t}=\frac{d_{t+1}}{d_t} \frac{\rho(s_{t+1};U)}{\rho(s_t;U)}\).
Given an arbitrary sequence in the set
\(\{s_{0\rightarrow T}|s_{0\rightarrow T}\in A, \forall t<T:s_t>s_{t+1}>0\}\)
(the sequence is decreasing and available for choice, and all rewards
are positive), when \(\lambda \geq \bar{\lambda}\), for each period
\(t<T\) in the sequence, we have \(\rho(s_{t+1};U)>\rho(s_t;U)\), as
\(\rho(s_t;U)\) is decreasing in \(s_t\).

Denote the given sequence by \(s_{0\rightarrow T}^*\). To make
\(s_{0\rightarrow T}^*\) become the interior solution for the planning
problem in period 0, according to the FOC, we need
\(\frac{d_{t+1}^0}{d_t^0}=\frac{\rho(s_t;U)}{\rho(s_{t+1};U)}\). Thus,
we can define \(d_t^0\) as\[
d_t^0 = \frac{\rho(s_t;U)^{-1}}{\rho(s_0;U)^{-1}+...+\rho(s_T;U)^{-1}}
\]According to this definition, it can be easily validated that
\(\frac{d_{t+1}^0}{d_t^0}=\frac {\rho(s_t;U)}{\rho(s_{t+1};U)}\) and
\(d_t^0>d_{t+1}^0>0\). Under such \(\{d_t^0\}_{t=0}^T\), the given
sequence \(s_{0\rightarrow T}^*\) is the optimal consumption plan.

For the part (b), we prove it in two steps. First, we show if
\(d_t^1=d_t^0\), the DM will under-consume in period 1. In this case,
for any \(t\geq 1\) we set \(d_t^0=d_t^1=d_t\). Given
\(s_{0\rightarrow T}^*\) as the period 0's optimal consumption plan,
when moving to period 1, the DM will need to allocate budget over period
\(t=1,…,T\), and the largest consumption \(s_0^*\) will be moved out
from the sequence. Keeping everything equal, this will reduce the
sequence value \(U\), thereby (under the condition
\(\lambda\geq \bar{\lambda}\)) increasing
\(\frac{u_{t+1}+\lambda-U}{u_t+\lambda-U}\). Thus, according to Equation
(F3),
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}\)
will increase to greater than 1. To optimize the consumption plan, the
DM needs to adjust \(s_t\) and \(s_{t+1}\) toward re-achieving
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=\frac{d_{t+1}}{d_t}\frac{\tilde{\rho}_{t+1}}{\tilde{\rho}_t}=1\).
Specifically, she is required to reduce
\(\frac{\tilde{\rho}_{t+1}}{\tilde{\rho}_t}\). According to Equation
(G2), when \(\lambda\geq\bar{\lambda}\), \(\tilde{\rho}_t\) is
decreasing in \(s_t\), which implies that the DM needs to increase
\(s_{t+1}\) in relative to \(s_t\). In other words, she needs to
transfer consumption from an earlier period to a later period. So, for
the optimal consumption plan in period 1, we have \(s_1^{**}<s_1^*\).

Finally, we discuss the case that \(d_t^1=w_t^0\). When moving to period
1, how the DM will adjust consumption is affected by two mechanisms. On
the one hand, as
\(\frac{d_{t+1}^1}{d_t^1}=\frac{d_{t+1}^0}{d_t^0}\exp\{\frac{u_{t+1}-u_t}{\lambda}\}\),
the DM will initially pay more attention to an earlier period than a
later period; on the other hand, as the largest consumption \(s_0^*\) is
moved out, keeping everything equal, the sequence value \(U\) will
decrease. We suppose it will decrease from \(U_0\) to \(U_1\). The
former mechanism drives the DM to transfer consumption from a later
period to an earlier period, while the latter mechanism drives her to do
the opposite. We can show that the former mechanism overrides the
latter.

To show this, note for the period 0's optimal consumption plan, we have
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=\frac{d_{t+1}^0}{d_t^0}\frac{\rho(s_{t+1}^*;U_0)}{\rho(s_t^*;U_0)}=1\).
Substituting it into the equation of
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}\)
in period 1 (keeping everything equal), we have\[
\tag{G5}
\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}
= \exp\{\frac{u(s_{t+1}^*)-u(s_t^*)}{\lambda}\}\frac{u_{t+1}+\lambda-U_1}{u_t+\lambda-U_1}
\frac{u_t+\lambda-U_0}{u_{t+1}+\lambda-U_0}<\frac{\Gamma(s_{t+1}^*;U_0)}{\Gamma(s_{t}^*;U_0)}
\]Equation (G5) implies that, uner this situation, if we reduce \(U_1\)
to 0, the value of
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}\)
will be increased to \(\Gamma(s_{t+1}^*;U_0)/\Gamma(s_t^*;U_0)\). Since
(under the condition \(\lambda \geq \bar{\lambda}\)) \(s_t^*>s_{t+1}^*\)
and \(\Gamma(s_t;U_0)\) is increasing in \(s_t\), we can obtain that
\(\Gamma(s_{t+1}^*;U_0)/\Gamma(s_t^*;U_0)<1\). As a result, to optimize
the consumption plan in period 1, the DM will need to increase
\(\frac{\partial U}{\partial s_{t+1}}/\frac{\partial U}{\partial s_{t}}=\frac{d_{t+1}^1}{d_t^1}\frac{\tilde{\rho}_{t+1}}{\tilde{\rho}_t}\).
Specifically, she is required to increase
\(\frac{\tilde{\rho}_{t+1}}{\tilde{\rho}_t}\). Thus, she has to transfer
consumption from a later period to a later period; that is, reducing
\(s_{t+1}\) in relative to \(s_t\). For period 1, we will have
\(s_1^{**}>s_1^*\).

\hypertarget{h.-impact-of-adding-a-zero-to-the-sequence-end}{%
\subsection*{H. Impact of Adding a Zero to the Sequence
End}\label{h.-impact-of-adding-a-zero-to-the-sequence-end}}
\addcontentsline{toc}{subsection}{H. Impact of Adding a Zero to the
Sequence End}

We discuss the impact of adding a period of zero reward to the end of
each sequence here. This technical trick primarily affects the contexts
involving choices between sequences of different lengths. For
intertemporal correlation aversion, S-shaped value function, and the
anomalies related to allocating resources over a certain time
(e.g.~concentration bias, present bias), it doesn't require special
consideration. For example, in our proof about intertemporal correlation
aversion (see Appendix E), we import a parameter \(D\) to capture the
effect incurred by all periods with zero reward delivered, and assume
that \(D\) is arbitrary. Changing the scale of \(D\) has no effect on
our proposition. Besides, the reason why this does not affect the hidden
zero effect is immediately apparent. We thus focus on the following
anomalies: (1) common difference effect; (2) concavity of discount
function.

Given a sequence \(s_{0\rightarrow T+1}=[0,0,...,0,s_T,0]\), where
\(s_T>0\) and all the other periods have no reward delivered, we can
obtain an equation similar to Equation (4). The discount function for
\(s_T\) will be \(w_T=\frac{1}{1+G(T)e^{-v_T}}\), where
\(v_T=u(s_T)/\lambda\) and \[\tag{H1}
G(T)=\left\{
\begin{aligned}
& \frac{\delta^{-T}-1}{1-\delta}+\delta \;,&\; 0<\delta<1 \\
& T+1 \;,&\; \delta=1
\end{aligned}
\right.
\]

For common difference effect, we follow the same notation and the
process of proof in Appendix B. If \(G(T)=T+1\), Equation (B3) is still
valid. Thus, the common difference effect must hold. If
\(G(T)=\frac{1}{1-\delta}(\delta^{-T}-1)+\delta\), we will
have\[\tag{H2}
1+G(t+\Delta t)e^{-v}=
\delta^{-\Delta t}(1+G(t)e^{-v})+(\delta^{-\Delta t}-1)[(\frac{1}{1-\delta}-\delta)e^{-v}-1]
\]which is similar to Equation (B4). Combining it with Equation
(B1)(B2), we can obtain that the common difference effect holds when and
only when\[
v_l-v_s+\ln\left(\frac{v_l}{v_s}\right)>
\ln\left(\frac{\delta^{-t_l}-\delta(1-\delta)}{\delta^{-t_s}-\delta(1-\delta)}\right)
\]That is to say, to observe the common difference effect, the absolute
and relative differences in reward utility should be significantly
larger than the difference in reward delay (the implication of
Proposition 2).

For concavity of discount function, we follow Appendix C. Again, if
\(\delta=1\), Equation (C1) must be valid. If \(0<\delta<1\), Equation
(C1) is valid when\[\tag{H3}
\delta^{-T}\geq (1-\delta)e^{v(x)}-1+\delta(1-\delta)
\]To make Equation (H3) holds, we should consider two cases. The first
case is the RHS is no greater than 1. This implies that
\(v(x)\leq v(\underline{x})\), where
\(v(\underline{x})=\ln(\frac{2}{1-\delta}-\delta)\). If the first case
does not hold, we should consider the second case: \(T\) is above a
threshold \(\underline{t}\), where
\(\underline{t}=\frac{\ln((1-\delta)e^{v(x)}-1+\delta(1-\delta))}{\ln(1/\delta)}\).
So, when the reward is large enough (greater than \(\underline{x}\)),
the discount function can be concave in the near future, which is the
implication of Proposition 3.
