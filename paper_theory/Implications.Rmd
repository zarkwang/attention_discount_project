---
bibliography: reference.bib
# biblio-style: apalike
# header-includes: 
#   \usepackage{setspace}
#   \usepackage{amsmath}
#   \usepackage{array}
#   \usepackage{caption}
#   \usepackage{longtable}
#   \usepackage{booktabs}
#   \usepackage{enumitem}
#   \renewcommand{\arraystretch}{1}
#   \captionsetup[table]{skip=5pt}
#   \setstretch{1.5} 
# fontsize: 12pt
# geometry: margin=1in
# editor_options: 
#   markdown: 
#     wrap: 72
# output:
#   #word_document:
#     #number_sections: true
#   pdf_document:
#     number_sections: true
#     citation_package: natbib
#     keep_tex: true
#   html_document:
#     toc: true
#     number_sections: true
---

# Implications for Decision Making

## Hidden Zero Effect \label{hidden_zero_effect}

Empirical research suggests time discounting is influenced by the framing of sequences. One prominent evidence in this field is the hidden zero effect [@magen2008hidden; @radu2011mechanism; @read2017value]. Studies about the hidden zero effect typically relate this effect to "temporal attention".

To illustrate the hidden zero effect, suppose the DM is indifferent between "receive £100 today" (SS) and "receive £120 in 25 weeks" (LL). The hidden zero effect suggests that people exhibit more patience when SS is framed as a sequence rather than as a single-period reward. In other words, if we frame SS as "receive £100 today and £0 in 25 weeks" (SS1), the DM would prefer LL to SS1. Similar to the violation of dominance, this phenomenon can be viewed as a justification for the Sequential Outcome-Betweenness axiom in Section \ref{optimal_discount}. Moreover, @read2017value find the effect is asymmetric, that is, framing LL as "receive £0 today and £120 in 25 weeks" (LL1) has no effect on preference.

The AMD model provides a formal account for the effect. When the DM evaluates a reward sequence $s_{0\rightarrow T}$, the AMD model assumes she would splits a fixed amount of attention over $T$ periods. In the given example, the DM may perceive the time length of SS as "today" and perceives the time length of SS1 as "25 weeks". For option SS, she can focus her attention on the current period in which she can get £100. For option SS1, she has to spend some of her attention to future periods in which no reward is delivered, and this also reduces the decision weight assigned to the current period. As a result, she values SS1 lower than SS. By contrast, the DM perceives the time length of both LL and LL1 as "25 weeks". Both LL and LL1 are sequences that deliver zero rewards from the current period to the period before "25 weeks". According to the AMD model, when she evaluates LL, she has already paid some attention to all periods earlier than "25 weeks". So, changing LL to LL1 does not affect her choice.

## Relation to Hyperbolic Discounting \label{hyperbolic}

Most intertemporal choice studies only involve comparisons between single-period rewards (SS and LL). Here, we derive the formula of the AMD factor for SS/LL and use that to illustrate how attention modulation can account for some anomalies in this decision setting. For simplicity, we assume the DM initial discount factor (before attention modulation) is exponential, that is, the default discount factor $d_t = \delta^t$, where $\delta\in(0,1]$.[^-1]

[^-1]: @strotz1955myopia shows that, for any reward delivered at period $t$, if the DM's discount factor can be written as $\delta^t$, then her preference will be stationary and consistent over time.

Consider a reward sequence $s_{0\rightarrow T}$ in which $u(s_t)=0$ for all $t\leq T$, and only $u(s_T)>0$. This implies the DM receives nothing until period $T$. In this case, the DM's valuation of $s_{0\rightarrow T}$ is $U(s_{0\rightarrow T})=w_Tu(s_T)$. Let $v(x)=u(x)/\lambda$. By Definition 1, we can derive that $w_T$ is a function of $s_T$:$$\tag{4}
 w_T = \frac{1}{1+G(T)e^{-v(s_T)}}
$$where$$ G(T) = \left\{ \begin{aligned} & \frac{1}{1-\delta}(\delta^{-T}-1) \; ,& 0<\delta<1\\ & T\; ,& \delta=1\ \end{aligned} \right. $$The $w_T$ in Equation (4) can represent the discount function for a single reward $s_T$, delivered at period $T$. Interestingly, when $\delta=1$, $w_T(s_T)$ takes a form similar to hyperbolic discounting. In recent years, several studies have attempted to provide a rational account for hyperbolic discounting. For instance, @gabaix2017myopia propose a model with similar assumptions to our information maximizing exploration approach to AMD: the perception of instantaneous utility is noisy and the DM use Bayes' rule on the perceived signals to update her belief about the utility. Nevertheless, @gabaix2017myopia account for hyperbolic discounting with an additional assumption that the variance of signals is proportional to the corresponding reward delay. The account we propose via AMD is that the variance is constant but DM seeks to maximize the information gain when learning from signals. Besides, @gershman2020rationally propose an alternative model based on the work of @gabaix2017myopia. We note under a certain specification of instantaneous utility, Equation (4) will generate a discount function similar to the function proposed by @gershman2020rationally.[^-2] In a nutshell, such accounts for hyperbolic discounting can somehow be generated by variants or special cases of the AMD model.

[^-2]: @gershman2020rationally propose that the discount function for a single reward $s_T$ is $1/[1+(\beta s_T)^{-1}T]$, where $\beta>0$. In Equation (4), if we set $\delta=1$ and $v(s_T)=\ln(\beta s_T+1)$, we will obtain $w_T = 1/[1+(\beta s_T+1)^{-1}T]$, which takes a similar form to @gershman2020rationally.

In the following three subsections, we use Equation (4) to explain three decision anomalies: the common difference effect (and its reverse), risk aversion over time lotteries, and S-shaped value function.

## Common Difference Effect

The common difference effect [@loewenstein1992anomalies] suggests that, when the DM faces a choice between LL and SS, adding a common delay to both options can increase her preference for LL. For example, suppose the DM is indifferent between "receive £120 in 25 weeks" (LL) and "receive £100 today" (LL). Then, she would prefer "receive £120 in 40 weeks" to "receive £100 in 15 weeks".

Let $(v_l,t_l)$ denote a reward of utility $v_l$ delivered at period $t_l$ and $(v_s,t_s)$ denote a reward of utility $v_s$ delivered at period $t_s$. We set $v_l>v_s>0$, $t_l>t_s>0$. So, $(v_l,t_l)$ can represent a LL and $(v_s,t_s)$ can represents a SS. We denote the discount factors for LL and SS by $w_{t_l}(v_l)$ and $w_{t_s}(v_s)$. Suppose $w_{t_l}(v_l)\cdot v_l = w_{t_s}(v_s)\cdot v_s$. The common difference effect implies that $w_{t_l+\Delta t}(v_l)\cdot v_l > w_{t_s+\Delta t}(v_s)\cdot v_s$, where $\Delta t >0$. We assume that the discount factors are derived from Equation (4), and describe the conditions that make the common difference effect hold in Proposition 2.

\noindent \textbf{Proposition 2}: *The following statements are true for AMD:*

(a) *If* $\delta=1$*, the common difference effect always holds.*

(b) *If* $0<\delta<1$*, i.e. the DM is initially impatient, the common difference effect holds when and only when* $v_l-v_s+\ln(v_l/v_s)>(t_l-t_s)\ln(1/\delta)$*.*

The proof of Proposition 2 is in Appendix B. The part (b) of Proposition 2 yields a novel prediction about the common difference effect. That is, for any impatient DM, to make the common difference effect hold, the relative and absolute differences in reward utility between LL and SS must be significantly larger than their absolute difference in time delay. In the opposite, if the difference in delay is significantly larger than the difference in reward utility, we may observe a reverse common difference effect.[^-3]

[^-3]: Also, it is easy to validate that if we make the "hidden zeros" explicit in LL and SS, adding a common delay under the AMD model would always yield a the common difference effect.

When the DM is impatient, adding a common delay would naturally make $v_l$ and $v_s$ more discounted; so, less attention is paid to the two corresponding rewards. Suppose the sum of decision weights is not changed, this implies that the DM can "free up" some attention that is originally captured by these rewards and reallocate it to other periods in each option. There are three mechanisms jointly determining whether we could observe the common difference effect under this circumstance.

First, in each option, the existing periods which have no reward delivered could grab some attention. That is, the DM would attend more to some rewards of zero utility, delivered in duration $[0,t_l)$ for LL and in duration $[0,t_s]$ for SS. Given $t_l >t_s$, the correponding duration in LL would naturally capture more attention than that in SS. In other words, adding the common delay makes the DM focus more on the original waiting time in LL than in SS, which decreases her preference for LL.

Second, the newly added time intervals could also grab some attention. That is, the DM needs to pay some attention to rewards (of zero utility as well) delivered in duration $(t_l,t_l+\Delta t)$ in LL and in duration $(t_s, t_s+\Delta t)$ in SS. For LL, there have been already a lot of periods to which DM has to attend before we add the delay $\Delta t$. So, the duration $(t_l,t_l+\Delta t)$ in LL can capture less attention than its counterpart in SS. This increases the DM's preference for LL.

Third, the only positive reward, delivered in $t_l$ for LL and in $t_s$ for SS, could draw some attention back. Given that the DM in general attends more to larger rewards, the positive reward in LL can capture more of the "freed-up" attention than that in SS. This also increases the preference for LL. If the latter two mechanisms override the first mechanism, we would observe a common difference effect in DM's choices.

```{=tex}
\begin{figure}[h]
  \centering   
  \includegraphics[width=0.65\textwidth]{figures/common_diff.png}   
  \caption{The common difference effect and its reverse}
  \vspace{8pt}
  \begin{minipage}{1.0\textwidth}
{\par\footnotesize Note: $x_l$ and $x_s$ are the positive reward levels for LL and SS. The values of LL and SS are calculated based on Equation (4). $d_t=0.75^t$, $u(x)=x^{0.6}$, $\lambda=2$. For each level of $t_s$, we identify the delay $t_l$ that makes the value of LL equivalent to SS. The blue line (above) demonstrates the common difference effect, and the red line (below) demonstrates the reverse common difference effect.}
\end{minipage}
  \label{fig:common_diff} 
\end{figure}
```
Figure \ref{fig:common_diff} demonstrates an example for the reverse common difference effect. In the figure, we set $v_s=x_s^{0.6}$, $v_l=x_l^{0.6}$, $\delta=0.75$, $\lambda=2$. For each level of the delay $t_s$, we identify the longer delay $t_l$ that makes the value of LL equivalent to SS. If the common different effect is valid, increasing $t_s$ and $t_l$ by the same level would make the DM prefer LL. Under this condition, for one unit increase in $t_s$, to make LL and SS valued equally, the identified $t_l$ should be increased by a time greater than one unit. On the contrary, if the reverse common difference effect is valid, for one unit increase in $t_s$, the identified $t_l$ should be increased by a time smaller. In Figure \ref{fig:common_diff}, the blue line (above) reflects the common different effect, while the red line (below), which has a lower $v_l-v_s$ and $v_l/v_s$, reflects the reverse of it.

## Concavity of Discount Function

Many time discounting models, such as exponential and hyperbolic discounting, assume the discount function is convex in time delay. This type of discount function predicts DM is *risk seeking over time lotteries*. To illustrate, suppose a reward of level $x$ is delivered at period $t_l$ with probability $\pi$ and is delivered at period $t_s$ with probability $1-\pi$, where $0<\pi<1$. Meanwhile, another reward of the same level is delivered at period $t_m$, where $t_m=\pi t_l +(1-\pi) t_s$. Under such discount functions, the DM should prefer the former reward to the latter reward. For instance, she may prefer receiving an amount of money today or in 20 weeks with equal chance, rather than receiving it in 10 weeks with certainty. However, experimental studies suggest that people are often *risk averse over time lotteries*, i.e. they prefer the reward to be delivered at a certain time [@onay2007intertemporal; @dejarnette2020time].

One way to accommodate risk aversion over time lotteries is to make the discount function concave in terms of delay. Notably, @onay2007intertemporal find that people are more likely to be risk averse over time lotteries when $\pi$ is small, and to be risk seeking when $\pi$ is large. Given that $t_m$ is increasing in $\pi$, we can claim that the discount function should be concave in delay for the near future but convex for the far future. @takeuchi2011non find the supportive evidence for this shape of discount function. In Proposition 3, we apply Equation (4) and show that the AMD model produces this shape of discount function when the DM is impatient and the reward level $x$ is large enough.

\noindent \textbf{Proposition 3}: *Suppose a single reward* $x$ is *delivered at period* $T$*. Let* $w_T$ *denote the AMD factor for this reward. If* $\delta =1$*, then* $w_T$ *is convex in* $T$*. If* $0<\delta<1$*, there exist a reward threshold* $\underline{x}>0$ *and a time threshold* $\underline{T}>0$ *such that:*

(a) *when* $x\leq \underline{x}$*,* $w_T$ i*s convex in* $T$;
(b) *when* $x > \underline{x}$*,* $w_T$ i*s convex in* $T$ *given* $T\geq \underline{T}$*, and it is concave in* $T$ *given* $0<T<\underline{T}$*.*

The proof of Proposition 3 is in Appendix C. Figure \ref{fig:discount_value_function}(a) demonstrates the convex discount function (blue line, below) and the inverse-S shaped discount function (red line, above) that could be yielded by Equation (4).

```{=tex}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/concave_discount.png} 
        \caption{discount function}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/s_shape_value.png}
        \caption{value function}
    \end{subfigure}
    \caption{Discount function and value function for a delayed reward}

  \vspace{8pt}
  \begin{minipage}{1.0\textwidth}
{\par\footnotesize Note: A reward of level $x_T$ is delivered at period $T$. The discount function and value function are calculated based on Equation (4). $d_t=0.75^t$, $u(x)=x^{0.6}$, $\lambda=2$.}
\end{minipage}
    
    \label{fig:discount_value_function}
\end{figure}
```
## S-Shaped Value Function \label{s_shape_value}

A common assumption in decision theories for the instantaneous utility function $u(.)$ is $u''<0$. Usually, this implies the value function of a reward is concave. However, empirical evidence suggests that the value functions are often S-shaped. Such S-shaped value functions can be generated by various sources, such as reference dependence [@kahneman1979prospect] and efficient coding of numbers [@louie2012efficient]. Through the AMD model, we provide a novel account for S-shaped value function based on the insight that larger rewards capture more attention.

Consider a reward of level $x$ delivered at period $T$. Its value function can be represented by $U(x,T)=w_T(x)u(x)$. We assume $u'>0$, $u''<0$, and $w_T$ is determined by Equation (4). $w_T$ is increasing with $x$ as the DM tends to pay more attention to larger rewards. Both functions $u(x)$ and $w_T(x)$ are concave in $x$; so when $x$ is small, they both grow fast. At some conditions, it is possible that the product of the two functions is convex in $x$ when $x$ is small enough. We derive the conditions for the S-shaped value function in Proposition 4.

\noindent \textbf{Proposition 4}: *Suppose* $T\geq1$*,* $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ *is continuous in* $(0,+\infty)$*, then:*

(a) *There exists a threshold* $\bar{x} \in\mathbb{R}_{\geq0}$ *such that* $U(x,T)$ *is strictly concave in* $x$ *when* $x\in [\bar{x},+\infty)$.

(b) *If* $\frac{d}{dx}\left(\frac{1}{v'(x)}\right)$ *is right-continuous at* $x=0$ *and* $\frac{d}{dx}\left(\frac{1}{v'(0)}\right)<1$*, there exists* $x^*\in(0, \bar{x})$ *such that, for any* $x\in (0,x^*)$*,* $U(x,T)$ *is strictly convex in* $x$.

(c) *There exists a threshold* $\lambda^*$ *and an interval* $(x_1,x_2)$ *such that, if* $\lambda<\lambda^*$*, for any* $x\in(x_1,x_2)$*,* $U(x,T)$ *is strictly convex in* $x$*, where* $\lambda^*>0$ *and* $(x_1,x_2)\subset(0,\bar{x})$*.*

The proof of Proposition 4 is in Appendix D. Proposition 4 implies, if the derivative of $\frac{1}{v'(x)}$ converges to a small number when $x\rightarrow 0^+$, or the unit cost of attention reallocation $\lambda$ is small enough, the value function $U(x,T)$ will be an S-shaped in some interval of $x$. Figure \ref{fig:discount_value_function}(b) demonstrates two examples of this S-shaped value function.

## Intertemporal Correlation Aversion

Consider a DM facing two lotteries, $L1$ and $L2$. For lottery $L1$, she can receive £100 today and £100 in 30 weeks with probability 1/2, and receive £3 today and £3 in 20 weeks with probability 1/2. For lottery $L2$, she can receive £3 today and £100 in 30 weeks with probability 1/2, and receive £100 today and £3 in 20 weeks with probability 1/2. In lottery $L1$, rewards delivered at the two different periods are positively correlated; in lottery $L2$, those rewards are negatively correlated. The expected discounted utility theory predicts the DM is indifferent between the two lotteries. However, recent studies find the evidence of *intertemporal correlation aversion* [@andersen2018multiattribute; @rohde2023intertemporal]. That is, people often prefer lottery $L2$ to $L1$.[^-4]

[^-4]: For theoretical analysis about intertemporal correlation aversion, please see @epstein1983stationary, @epstein1989substitution, @weil1990nonexpected, @bommier2005risk, and @bommier2017monotone. The AMD model takes a similar form to the class of models defined by @epstein1983stationary. A key feature of such models is that the discount factor for future utilities is dependent on the utility achieved in the current period.

For the above example, intertemporal correlation aversion can be explained by the AMD model as follows. The model assumes the allocation of decision weights is within each certain reward sequence, which implies the DM would first aggregate values over time in each state and then solve the certainty equivalence. For simplicity, suppose there are only two periods. In the state that the DM receives £3 in two periods, suppose she allocates decision weight $w$ to the first period and $1-w$ to the second period. Note when $u(s_0)=u(s_1)=...=u(s_T)$, the AMD factor for every period $t$ remains the same as its default discount factor $d_t$. So, in the state that the DM receives £100 in two periods, the decision weights is also the same as $w$ and $1-w$. In the state that the DM can receive £100 in the first period and £3 in the second period, the reward of £100 can capture more attention so that its decision weight, say $w'$, is greater than $w$. Similarly, in the state that the DM receives £3 earlier and then £100, the decision weight for the later reward £100, say $1-w''$, is greater than $1-w$. Therefore, the value of the lottery in which rewards are positively correlated, can be represented by $0.5\cdot u(3)+0.5\cdot u(100)$. Whereas, for the lottery in which rewards are negatively correlated, the value can be represented by $0.5(1-w'+w'')\cdot u(3)+0.5(1-w''+w')\cdot u(100)$. Given $(1-w'')+w'>1-w+w=1$, the decision weight assigned to $u(100)$, which is $0.5(1-w''+w')$, should be greater than 0.5. As a result, the DM prefer the latter lottery than the former lottery.

In a more general setting, whether the AMD model can robustly produce intertemporal correlation aversion is influenced by $\lambda$. To see this, we adopt the same definition of intertemporal correlation aversion as @bommier2005risk. Let $(s_1,s_2)$ denote the result of a lottery in which the DM can receive reward $s_1$ in period $t_1$ and then reward $s_2$ in period $t_2$, where $t_2>t_1\geq 0$. The results of each lottery is of the same length of sequence. $L1$ generates $(x_s,y_s)$ and $(x_l,y_l)$ with equal chance, $L_2$ generates $(x_s,y_l)$ and $(x_l,y_s)$ with equal chance, $x_l>x_s>0$, $y_l>y_s>0$. By Proposition 5, we show that in this setting, we can always find a $\lambda$ that makes the DM intertemporal correlation averse.

\noindent \textbf{Proposition 5}: *Suppose* $U(L1), U(L2)$ *are the values of lotteries* $L1$ *and* $L2$ *calculated based on the AAD model. For any* $x_l>x_s>0$*,* $y_l>y_s>0$*, any default discount factors, and any time length of lottery results, there exists* *a threshold* $\lambda^{**}$ such that for all unit co*st of attention reallocation* $\lambda\in(\lambda^{**},+\infty)$*, we have* $U(L1)<U(L2)$*, i.e. the DM performs intertemporal correlation aversion.*

The proof of Proposition 5 is in Appendix E. The threshold $\lambda^{**}$ is jointly determined by $x_l$, $y_l$, $y_s$, as well as the default discount factors for rewards delivered at $t_1$ and $t_2$. Notably, when $\lambda \leq \lambda^{**}$, the DM may be intertemporal correlation seeking under some conditions.[^-5] This suggests a potentially new mechanism for intertemporal correlation aversion, that is, DM performs intertemporal correlation aversion because she attends more to larger rewards while attention reallocation is very costly.

[^-5]: To validate, one can set $u(x_s)=5$, $u(x_l)=10$, $u(y_s)=1$, $u(y_l)=3$. Suppose the results of each lottery contain only two periods, $t_1$ and $t_2$, and the default discount factors are uniformly distributed, i.e. $d_{t_1}=d_{t_2}$. In this case, setting $\lambda=1$ would generate intertemporal correlation seeking, while setting $\lambda=100$ would generate intertemporal correlation aversion.

## Concentration Bias

In the existing literature, one approach to applying attention in explaining intertemporal chocies is the focus-weighted utility model, proposed by @kHoszegi2013model. In their model, \citeauthor{kHoszegi2013model} assume that within a reward sequence, the decision weight for a reward is increasing with the difference of that reward from its reference point. If we take zero reward as the reference point for every period, then this assumption is also true for the AMD model. The key difference between the AMD model and the focus-weighted utility model is that, in the latter, the decision weights are uncorrelated with each other and depend only on the choice set, whereas we assume increasing one decision weight can reduce some other decision weights within the sequence. The focus-weighted utility model predicts that people may perform a *concentration bias* and @dertwinkel2022concentration find supportive evidence for this prediction. In this subsection, we show that the AMD model provides an alternative way to generate the concentration bias. Furthermore, we identify the conditions in terms of impatience and attention reallocation cost (which are beyond the predictions of @kHoszegi2013model) for concentration bias under the AMD model.

To illustrate the concentration bias, consider a DM with a consumption budget £100 to spend over four days (from period 0 to period 3). Suppose the DM has two options: concentrating all consumption at period 0, or splitting the consumption evenly over four periods. The concentration bias implies that she would prefer the first option to the second. We denote the first option as sequence [100,0,0,0] and the second option as sequence [25,25,25,25]. For convenience, we assume the default discount factor for any period $t$ is $d_t = \delta^t$ and $0<\delta<1$. According to the AMD model, the DM prefers the first option if and only if$$
\frac{e^{u(100)/\lambda}}{e^{u(100)/\lambda}+\delta+\delta^2+\delta^3}\cdot u(100)>u(25)
$$Obviously, this inequality holds only when $\delta$ or $\lambda$ is small enough, which implies the DM should be very impatient or she can reallocate attention at a very low cost. Notably, under the AMD model, it is also possible that the DM prefers concentrating all consumption at the final period, i.e. the sequence [0,0,0,100], to the second option [25,25,25,25]. In this case, the decision weight multiplied by $u(100)$ in the inequality would become $\frac{\delta^3 \exp\{u(100)/\lambda\}}{1+\delta+\delta^2+\delta^3 \exp\{u(100)/\lambda\}}$. Then, the inequality holds only when both $\delta$ is large enough and $\lambda$ is small enough. Both cases are in line with the claim in @kHoszegi2013model and @dertwinkel2022concentration that the concentration bias can make people behave too impatiently or too patiently.

Next, we derive the conditions for concentration bias in a general optimal-decision setting. From the above example we can draw an intuition that, in a general case, to observe the concentration bias we require the unit cost of attention reallocation $\lambda$ to be small. We show in Proposition 6 that this intuition is true. Suppose the DM has a consumption budget $m$ ($m>0$) to spend over $T$ periods. Let reward sequence $s_{0\rightarrow T}$ represent her consumption plan at period 0, and let $A\subset \mathbb{R}_{\geq 0}^{T+1}$ denote her alternative space. In period 0, DM wants to find a $s_{0\rightarrow T}$ to solve the optimization problem:$$\tag{5}
\max_{s_{0\rightarrow T}\in A}\;\sum_{t=0}^T w_t u(s_t)\\ 
$$where$$
A=\left\{s_{0\rightarrow T} \bigg|\;\sum_{t=0}^T s_t = m,\; \forall t:s_t \geq 0\right\}
$$and $w_t$ is the AMD factor for consumption in period $t$, subject to default discount factor $d_t$. For $s\in[0,m]$, we have $0<u'(s)<\infty$, $-\infty<u''(s)<0$. Henceforth, we denote the optimization problem in Equation (5) by $\mathcal{O}(m,A,\{d_t\}_{t=0}^T)$. By Proposition 6, we know as long as the DM is impatient (for all period $t<T$, we have $d_t>d_{t+1}>0$) and $\lambda$ is small enough, her optimal consumption plan is to consume all of $m$ immediately. The proof of Proposition 6 is in Appendix F.

In Section \ref{model_setting}, we state that the unit cost of attention reallocation $\lambda$ has a potential link to cognitive uncertainty. If the DM is highly certain that the default discount factors $\{d_t\}_{t=0}^T$ truly capture her preference in the local context, she may inhibit the learning about value signals and thus $\lambda$ should be high. This link is also helpful for understanding the relationship between $\lambda$ and concentration bias: when allocating a budget over time, if the DM is totally uncertain about what to do (so $\lambda$ is very small), she may simply concentrate her budget into one period and consume it all.

\noindent \textbf{Proposition 6}: *Suppose the DM faces the planning problem* $\mathcal{O}(m,A,\{d_t\}_{t=0}^T)$, *and for all period* $t<T$, we have $d_t >d_{t+1}>0$*.* *There exists a threshold* $\underline{\lambda}>0$ *such that for any* $\lambda\leq\underline{\lambda}$*, her optimal consumption plan is to concentrate all consumption at period 0.*

## Inconsistent Planning and Learning \label{inconsistent}

An extensive amount of research evidence suggests that people often exhibit time-inconsistent behaviors in their daily lives [@ericson2019intertemporal]. For example, they often consume more than they originally planned, and procrastinate on effortful tasks. In a general sense, such behaviors can be termed present-biased behaviors. Several theories have been proposed for explaining the present-biased behaviors, such as dual-system preferences [@laibson1997golden], naivete [@o1999doing], reference dependence [@kHoszegi2009reference], and optimistic beliefs [@brunnermeier2017optimal]. Based on the AMD model, we can provide an alternative explanation for these behaviors: in dynamic decision-making, people update their default discount factors over time. In more intuitive terms, during each decision step, people will reference their past experiences when allocating attention. If, in the last step, they allocate too much attention to a particular period, they may then take this as a given or default status in the following step and continue to add attention to it. Compared with the existing theoretical explanations, our explanation is built on learning and memory. To perform present-biased behaviors, the DM should recall her mental status at the end of the last step and learn how to allocate attention accordingly. A memoryless DM may perform the reverse behavior. We analyse this with the consumption planning problem in the last subsection.

Again, we suppose the DM has a budget $m$ for consumption. She needs to allocate it over $T$ periods ($T\geq 2$) and the end of sequence is a fixed date. In period 0, her default discount factors $\{d_t\}_{t=0}^T$ satisfy $d_t>d_{t+1}>0$, where $t<T$. When making consumption plan, she would initially weight consumption of period 1 higher than consumption of any other future period. So, she may naturally plan to consume more in period 1 than in period $t=2,…,T$. This in turn, makes her relatively attend more to period 1. Her optimal consumption plan of period 0 should result in $w_1/w_t>d_1/d_t$ for each $t=2,…,T$. When arriving in period 1, we assume the DM will use the AMD factors determined in the last step as the new default discount factors. This will lead her to initially weight consumption of period 1 even higher, and therefore create a motive for over-consumption. In the end, her actual consumption in period 1 will be higher than what has been planned in the last step. Such a trend could continue until she reaches the final period or runs out of money.

```{=tex}
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/learning_c.png} 
        \caption{with learning}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/no_learning_c.png}
        \caption{without learning}
    \end{subfigure}
    \caption{Optimal Consumption Plan Over Time}

  \vspace{8pt}
  \begin{minipage}{1.0\textwidth}
{\par\footnotesize Note: In step 0, the DM allocates consumption budget $m=100$ over periods 0-4. In step 1, she allocates the remaining consumption over periods 1-4, and so on. "with learning" means the DM updates default discount factors per step; "without learning" means the default discount factors are constant over time. $d_t^0=0.9^t$, $u(x)=x^{0.6}$, $\lambda=70$. Each optimization problem is solved by projection gradient descent method.}
\end{minipage}
    
    \label{fig:inconsistency_learning}
\end{figure}
```
Figure \ref{fig:inconsistency_learning} illustrates the relationship between time inconsistency and learning. In the figure, we set $u(x)=x^{0.6}$, $T=4$, $\lambda=70$. At the first step of planning, the DM has a budget of $m=100$ for consumption and her default discount factors are exponential: $d_t^0=0.9^{t}$. Under the condition "with learning", which means the DM takes AMD factors of the optimal consumption plan as default discount factors for the next step, we observe a tendency for over-consumption. Under the condition "without learning", which means the default discount factors are constant over time, the DM's behavior is closer to being time-consistent, and in each step, the actual consumption is slightly lower than the consumption planned one step earlier.[^-6]

[^-6]: In one simulation under the condition "without learning", in step 0, the DM plans to consume 24.76 in period 1, and she ends up consuming 24.74. Also, in step 1, she plans to consume 17.32 in period 2, and she ends up consuming 17.31.

We state these results formally in Proposition 7. We focus on the transition from period 0 to period 1, but the conclusions can be expanded to other periods. When the DM plans her consumption in period $j$ ($j=0,1$), the default discount factor for period $t$ is termed $d_t^j$ and the corresponding AMD factor is termed $w^j_t$, where $t\geq j$. In the period 0's optimal consumption plan, the DM plans to consume $s_t^*$ in period $t$, and in the period 1's optimal plan, she plans to consume $s_t^{**}$. The alternative space is $A$ in period 0 and becomes $A'$ in period 1. Notably, to make the time-inconsistency results hold, the DM should not concentrate all consumption into period 0. From Proposition 6, we know that it requires $\lambda$ to be large enough. The proof of Proposition 7 is in Appendix G.

\noindent \textbf{Proposition 7}: *Suppose the DM faces the planning problem* $\mathcal{O}(m,A,\{d^0_t\}_{t=0}^T)$ *in period 0 and* $\mathcal{O}(m-s_0^*,A',\{d_t^1\}_{t=1}^T)$ *in period 1.* *There exists a threshold* $\bar{\lambda}>\underline{\lambda}$ *such that when* $\lambda\in[\bar{\lambda},+\infty)$*, the following is true:*

(a) *In period 0, for any sequence* $s_{0\rightarrow T}^*\in \{s_{0\rightarrow T}|s_{0\rightarrow T}\in A,\forall t<T:s_t>s_{t+1}>0\}$*, there exist a specification of default discount factors* $\{d_t^0\}_{t=0}^T$, where *for all* $t<T$ *we have* $d_t^0 >d_{t+1}^0>0$, *such that* $s_{0\rightarrow T}^*$ *is the optimal consumption plan.*

(b) *Given* $s_{0\rightarrow T}^*$ *as the period 0's optimal consumption plan, if in period 1, for all* $t\geq 1$ *we have* $d_t^1=d_t^0$*, then there must be* $s_1^{**}< s_1^*$*; if instead we have* $d_t^1=w_t^0$*, then* $s_1^{**}> s_1^*$*.*

In the part (a) of Proposition 7, we select an interior solution for the consumption planning problem in period 0. The part (b) suggests that if we do not take into account the updating of default discount factors, the DM would perform under-consumption behavior over time. In reverse, assuming the default discount factors are updated based on the AMD factors of the most recent consumption plan will result in over-consumption behavior. The former reflects the behavior of a DM "without learning", while the latter reflects how the DM would behave "with learning".

# Discussion

## Selection of Sequence Length

In our model, the attention a DM can allocate to each time period is affected by sequence length. When a sequence contains more periods, the average attention she can allocate to each period naturally decreases. In Section \ref{hidden_zero_effect}, we discuss how this can generate the hidden zero effect. Nevertheless, in reality, we usually cannot observe the actual sequence length perceived by the people. Researchers using our model need to make their own assumptions about sequence length. In this subsection, we discuss two issues related to setting these assumptions and provide recommendations.

The first issue is about the unit of time. A given duration can be represented as sequences of different lengths depending on the time unit used, such as months or days. For example, "receive £10 in 1 month" and "receive £10 in 30 days" essentially mean the same thing, but the latter seems to involve more units of time. When represented in sequence, the former can be represented by [0,10], and the latter can be [0,0,...,0,10], with 30 zeros before the 10. In "standard" discounting models, the number of zeros before the 10 has no effect on the valuation of the sequence. Whereas, the AMD model predicts that, when the reward sequence is described with more time units, the DM may perceive the waiting period for the £10 payment as longer, thereby discounting its value to a greater extent.

As an illustration, in the exponential discounting model, suppose each period represents a month and the discount factor for period $t$ is $\delta^t$. The value of sequence [0,10] could be written as $\delta \times u(10)$. For the sequence [0,0,...,0,10], which includes 30 zeros where each period represents a day, we can convert the monthly discount rate to a daily discount rate. Thus, the discount factor for period $t$ becomes $\delta^{\frac{1}{30}\cdot t}$, and the value of sequence is still the same. However, this approach does not apply to the AMD model. In the AMD model, suppose the default discount factors are exponential, as we have stated. According to Equation (4), the discount factor for the £10 payment is $1/(1+G(T)e^{-v(10)})$. And in the former case $G(T)=\delta^{-1}$, while in the latter case $G(T)=\delta^{-1}+\delta^{-\frac{29}{30}}+...+\delta^{-\frac{1}{30}}$. If we use the same parameterization in both cases, then in the latter case, the value of the £10 payment should be more discounted.[^-7]

[^-7]: If, in reverse, we elicit the DM's time preference from experiments and model it using the sequence with 30 zeros, then the estimate for $\delta$ should be greater compared to the sequence with only one zero.

The AMD model's prediction that people perceive the sequence with more time units as longer is also consistent with the numerosity effect. This effect refers to the tendency to overestimate quantity based on the number of units presented [@pelham1994easy] and has been extensively studied in various fields, particularly in consumer behavior [@zhang2012and; @monga2012years].[^-8] To the best of our knowledge, there is currently no clear evidence of the numerosity effect in intertemporal choice. Nonetheless, this can be a potential direction for future research. To capture such an effect, we recommend that researchers choose sequence lengths that match the time units presented to participants. For the given example, if a reward sequence is expressed as "receive £10 in 1 month", researchers would better represent it as [0,10] rather than a sequence with 30 zeros.

[^-8]: Some of such studies, such as @monga2012years, state that the unit itself can also influence decisions. For example, people may perceive "month" as longer than "day". So, the sequence that takes each month as a period may correspond to a smaller $\delta$.

The second issue is about the end of sequence. In our explanation of the model's implications, an implicit assumption is that each sequence terminates at the period when the final positive reward is delivered. For example, for a sequence described by "receive £10 in 1 month", we represent it as [0,10] rather than [0,10,0,0]. This assumption is sufficient for most of the well-established psychological effects in intertemporal choice. However, under this assumption the model is incapable of distinguishing between two constant sequences of different lengths. As an illustration, consider a choice between two options: (A) "receive £10 now"; and (B) "receive £10 now, plus £10 in 1 month, plus £10 in 2 months". According our implicit assumption, option (A) can be represented by a single-period sequence [10] and option (B) can be represented by [10,10,10]. In reality, people will definitely prefer option (B) to (A). But, as the AMD model assumes a constant sum of decision weights, such representations would result in both options being equally valued at $u(10)$.

We propose two remedies for this issue. The simplest approach is to assume that the DM always takes into account one additional period when processing each sequence. For example, she represents option (A) as [10,0] and option (B) as [10,10,10,0]. Then, in option (B), the attention jointly captured by the three 10s must be greater than the attention captured by the single 10 in option (A).[^-9] In Appendix H, we show that adding a period to the end of each sequence does not affect the implications of the model that we have discussed for choice between sequences. In addition, a more complex way to address the issue is to assume the sequences ends at a random position. Given that we do not know how people would actually perceive the sequence length, for option (B), we could assume the end of sequence follows a distribution, ranging from the third period to infinity. Researchers could set the class of distribution and then estimate its parameters from the data, which would allow them to accommodate a wider range of phenomena. We consider this as a potential direction for future research.

[^-9]: Set $u(x)=x^{0.6}$, $d_t=0.9^t$, $\lambda=2$. According to the AMD model, the value of option (A) is 3.55 and that of option (B) is 3.84.

To understand why including a zero (or zeros) at the end of a sequence is psychologically plausible, we can examine how people would perceive information that is not explicitly mentioned in each option. Adding a certain period to a sequence implies the DM must pay some attention to the reward delivered in that period, in order to process its value. For option (B), the DM knows there will be no reward delivered after period 3, although this is not explicitly described. On the one hand, this prohibits the DM from allocating more attention to perceive the value of rewards delivered in the far future (such rewards are not included in the sequence). On the other hand, "no reward will be delivered after period 3" could be seen as additional information beyond what is provided in option (B). In other words, the DM needs to at least allocate some attention to processing such an information, and adding some periods of no reward is a simple way to capture this.

## Relation to Other Intertemporal Choice Models

In the existing literature, the theory most similar to AMD is the salience theory, originally proposed by @bordalo2012salience. A recent review [@bordalo2022salience] summarizes the latest developments in the theory. According to that theory, the "salience" of an element in a sequence is increasing with its deviation from a reference point. Within the scope of this paper, we could set the reference point to zero. Thus, the salience theory also predicts that people pay more attention to large rewards and less attention to small rewards. Moreover, in Section \ref{inconsistent}, we consider the role of memory and learning in generating time-inconsistent behavior under the AMD model. This is also related to @bordalo2020memory, which incorporates a memory-based reference point in the salience theory.

Nevertheless, unlike the AMD model, the salience theory does not impose any restrictions on the sum of decision weights; it merely re-normalizes the decision weights. Through appying that theory to intertemporal choice, the value of a sequence could be represented by $U=\sum_{t=0}^T \pi_tw_tu(s_t)$, where $\pi_t$ is some "standard" discount factor (e.g. the exponential discount factor $\pi_t=\delta^t$), and the re-normalization weight $w_t$ satisfies $\sum_{t=0}^T w_t=1$. As a result, given a sequence [100, 4], additional two additional 4s to the end to make it [100, 4, 4, 4], would have different consequences under salience theory compared to the AMD model. In the salience theory, this may make the element 100 more "salient", as the total of decision weights increases from $\pi_0 + \pi_1$ to $\pi_0 + \pi_1 + \pi_2 + \pi_3$, and the 100 can capture the majority of this total. Whereas, in the AMD model, this operation always reduces the attention allocated to each element.

Some other models also claim to incorporate attentional mechanisms in intertemporal choice. For example, the focus-weighted utility theory [@kHoszegi2013model] also assumes that people discount the value of a large reward less because they focus on it more (if taking zero as the reference point). Despite that, the focus-weighted utility theory does not perform any normalization of decision weights. @steiner2017rational examine the role of rational inattention in dynamic risky decision-making, and use it to account for inertia and status quo bias. That theory is grounded in the instrumental utility of information while our model is grounded in its hedonic and cognitive utilities (see Section \ref{interpretation}). In psychology, some researchers use the attentional drift-diffusion model \citep[aDDM, see][]{krajbich2010visual} to analyse choices between SS and LL [@amasino2019amount]. In the aDDM, the value of an option is subject to an evidence accumulation process. If the DM pays more attention to an option (or attribute), she can accumulate more evidence about its value, leading her to value it more. So far, studies about aDDM have yet to examine how attention shifts within a sequence, but they may relate to the cost of attention reallocation, which controls the rate of information acquisition in the AMD model.

Our model extends the theories of endogenous time preference, which can be traced back to @uzawa1968time. Early theories, such as @uzawa1968time and @becker1997endogenous, assume that increasing a reward can lead to all sebsequent rewards in the same sequence being discounted more. However, they do not clearly address how changes in future rewards might affect the weighting of the rewards delivered earlier -- an issue crucial for explaining empirical findings like the violation of dominance and hidden-zero effect. Some theories \citep[e.g.][]{fudenberg2006dual,noor2022optimal} assume that time preferences are mediated by the cognitive cost of paying attention to the future. We borrowed this idea in the axiomatic characterization of the AMD model by using a modified version of the optimal discounting framework \citep{noor2022optimal,noor2024constrained}. We demonstrated that AMD is the only the model that satisfies both Sequential Outcome-Betweenness and Sequential Bracket-Independence within this general framework.

Moreover, the trade-off models for intertemporal choice provides an alternative approach to model choices between sequences [@read2012tradeoffs, @scholten2016cumulative, @scholten2024unified]. Such models assume that the DM trades off the average utility of receiving rewards against the average disutility of waiting. Yet, they can still be expressed in a form similar to endogenous time preference. For example, in @scholten2016cumulative, the value of a sequence can be represented by $U=\sum_{t=0}^T w_tu(s_t)$, where $w_t=1-\kappa\frac{T(T+1)(T-t+1)}{\sum_{\tau=0}^T (T-\tau+1)u(s_\tau)}$ and $\kappa$ is a positive parameter. Under this specification of $w_t$, increasing $s_t$ can increase the decision weights for all rewards in the sequence, while in the AMD model, this increases the deicison weight for $s_t$ but reduces the decision weights for all the other rewards.

## Possible Improvements

There are several ways to improve our model. First, the DM may not just allocate attention within a sequence, but also across sequences. As a result, within the same choice set, the sum of decision weights for one sequence may be smaller than that for another. Researchers interested in this direction might also refer to @manzini2014stochastic and @gossner2021attention. Second, as suggested by the aDMM, when the DM focuses more on a reward, she may not only assign it greater weight, but also accelerate the rate at which she learns about its value. In our model, the learning rate is controlled by a unified parameter $\lambda$. But in reality, this parameter might vary across different rewards as they capture different levels of attention. @leong2017dynamic provide an example of analyzing both consequences of attention simultaneously. Third, in the existing literature, the softmax function is usually used to model stochastic choices. Our model examines how this function can be used to study mental representations of reward sequences. Future research could explore its use in risk perception, and integrate these fields to offer a unified behavioral economic framework for analyzing dynamic risky decision-making problems.
